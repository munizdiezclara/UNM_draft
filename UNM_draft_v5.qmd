---
title: "Effects of expected and unexpected uncertainty on cue processing"
shorttitle: "Effects of uncertainty on cue processing"
author:
    # Roles are optional. 
    # conceptualization, data curation, formal Analysis, funding acquisition, investigation, 
    # methodology, project administration, resources, software, supervision, validation, 
    # visualization, writing, editing
    # List city and region/state for unaffiliated authors
  - name: Clara Muñiz-Diez
    corresponding: true
    orcid: 0000-0001-5192-0462
    email: c.muniz-diez@lancaster.ac.uk
    # Select from the CRediT: Contributor Roles Taxonomy https://credit.niso.org/
    affiliations:
      - id: id1
        name: "Lancaster University"
        department: Department of Psychology
        city: Lancaster
        region: UK
  - name: Sandra Lagator
    orcid: 0000-0001-6060-2941
    affiliations: 
      - id: id2
        name: "The University of Nottingham"
        department: School of Psychology
        city: Nottingham
        region: UK
  - name: Mark Haselgrove
    orcid: 0000-0001-8981-1181
    affiliations:
      - ref: id2
  - name: Tom Beesley
    orcid: 0000-0003-2836-2743
    affiliations:
      - ref: id1
author-note:
  status-changes: 
    # Example: [Author name] is now at [affiliation].
    affiliation-change: null
    # Example: [Author name] is deceased.
    deceased: null
  # Disclosures condensed to one paragraph, but you can start a field with two line breaks to break them up: \n\nNew Paragraph
  disclosures:
    # Acknowledge and cite data/materials to be shared.
    # Example. Because the authors are equal contributors, order of authorship was determined by a fair coin toss.
    # Example: This study was registered at X (Identifier Y).
    study-registration: null
    data-sharing: "The programs of the experiments presented here, the data and the full code for the writing this manuscript are freely available on www.github.com/munizdiezclara/UNM_draft."
    # Example: This article is based on data published in [Reference].
    # Example: This article is based on the dissertation completed by [citation].  
    related-report: null
    # Example: [Author name] has been a paid consultant for Corporation X, which funded this study.
    conflict-of-interest: null
    # Example: This study was supported by Grant [Grant Number] from [Funding Source].
    financial-support: "This study was supported by the ESRC grant Known unknowns and unknown unknowns (ES/W013215/1)." 
    # Example: The authors are grateful to [Person] for [Reason].
    gratitude: null
    authorship-agreements: null
abstract: "Learning influences the overt attention that is paid to stimuli in two main ways: first, stimuli which are reliable predictors of an outcome are paid more attention than unreliable stimuli; and second, stimuli associated with uncertain outcomes capture more attention than stimuli associated with certain outcomes. Past studies have shown that these two phenomena can be demonstrated within the same experiment, but strikingly, the increase in attention due to uncertainty does not necessarily translate into subsequent better learning. We investigate this paradox by examining stimulus processing in four experiments that included predictive and non-predictive cues, trained under different conditions of uncertainty. In Experiment 1, we tested memory for cues after training with either certain or uncertain contingencies, finding that overall levels of recognition memory were similar in the two conditions. In Experiment 2, uncertain contingencies were introduced after a period of learning with certain contingencies. During the subsequent memory test, this training resulted in better memory than training with certain contingencies throughout the learning phase. In Experiments 3 and 4 were a replication of the previous ones, including a group that was trained with certain contingencies, a group that was trained with uncertain contingencies, and a group in which the uncertain contingencies were experienced after a period of certain contingencies. These experiments failed to replicate the result of better memory in the group that experienced unexpected uncertain contingencies. Taken together, the results of the four experiments suggest that, despite the previous finding of higher attention under conditions of uncertainty, this does not translate to enhanced memory under uncertain conditions. The implications of these results for attentional models of learning are discussed."
keywords: [Associative Learning, Attention, Uncertainty, Predictiveness, Cue processing]
floatsintext: true
numbered-lines: false
bibliography: references.bib
suppress-title-page: false
mask: false
masked-citations:
  - schneider2012cattell
  - schneider2015intelligence
# Language options. See https://quarto.org/docs/authoring/language.html
lang: en
language:
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
format:
  apaquarto-docx: default
  apaquarto-html: default
  apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: man
editor: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(afex)
library(BayesFactor)
library(apa)
library(emmeans)
library(papaja)
library(rstatix)
library("writexl")
options(scipen=999)
bfit = 10000

# function to force scientific formatting of numbers (used for large BFs)
changeSciNot <- function(n) {
  output <- format(n, scientific = TRUE, digits = 2) #Transforms the number into scientific notation even if small
  output <- sub("e", "x10^", output) #Replace e with 10^
  output <- sub("\\+0?", "", output) #Remove + symbol and leading zeros on exponent, if > 1
  output <- sub("-0?", "-", output) #Leaves - symbol but removes leading zeros on exponent, if < 1
  output <- paste0(output,"^")
  # output = strsplit(output, "^", fixed = TRUE)
  # output = paste0(output[[1]][1],"^", output[[1]][2], "^")
  output
}
# function to extract and report BFs with error %s
report_BF_and_error <- function(BF_in, sci_not = FALSE, hyp = "alt"){
  
  if (hyp == "alt") {
    BF_notation = "BF~10~ = "
  } else if (hyp == "null") {
    BF_notation = "BF~01~ = "
  }
  
  if (sci_not == TRUE) {
    BF_value = changeSciNot(extractBF(BF_in)$bf) # change to sci notation
  } else {
    BF_value = round(extractBF(BF_in)$bf,2) # otherwise round
  }
  
  paste0(BF_notation, 
         BF_value, 
         " &plusmn; ", 
         round(100*extractBF(BF_in)$error,2), 
         "%")
}
```

A central feature of the cognition of humans and other animals is the ability to learn the predictive relationships between events in the world in order to anticipate future goals and modify behaviour accordingly. However, capacity limitations mean that processing all available stimulus pairings would neither be possible nor optimal. The cognitive system must therefore select events that are task relevant to focus resources and refine the allocation of attention. The attention that is paid to a stimulus plays an important role in models of associative learning. Take for example the influential Rescorla & Wagner [-@rescorlaTheoryPavlovianConditioning1972] model, in which, the parameter $\alpha$ refers to the salience of a cue. The inherent salience of a cue (e.g., how loud a sound is; how bright a light is) will determine how much it captures attention, and thus how successful learning will be with that stimulus. However, the attention paid to a stimulus is not just inherent – it can also be determined by its associative history [for a review, see @lepelleyAttentionAssociativeLearning2016]. For example, at busy pedestrian crossings/crosswalks it is common for a sound to be played when it is safe to cross. Through experience, this sound will readily come to capture attention due to its predictive nature, while other salient but non-predictive stimuli (e.g., an advertising billboard) will not.

There are two main ways in which learning can shape attention. The first is often referred to as the *predictiveness principle*, according to which attention increases to stimuli that reliably signal the occurrence of an outcome [e.g., @mackintoshTheoryAttentionVariations1975; see also: @kruschkeUnifiedModelAttention2001; -@kruschkeAttentionLearning2003; @lepelleyRoleAssociativeHistory2004]. It is thought that this mode of attention is advantageous as it allows for the *exploitation* of reliable knowledge and permits animals to be prepared to make responses to stimuli with known consequences. A common method for studying this process in the lab is with the “learned predictiveness” design [e.g., @lepelleyLearnedAssociabilityAssociative2003]. In this contingency learning procedure, cues are trained as differentially predictive of particular outcomes (e.g., in the allergist task, participants are instructed that various foods are differentially predictive of one kind of allergic reaction or another). Typically, compounds of cues are presented, where one cue is perfectly predictive of an outcome, while the other is non-predictive. Take for example, a training phase in which compounds AX and AY are followed by outcome 1, whereas compounds BX and BY are followed by outcome 2. Here, cues A and B are predictive of outcomes 1 and 2 respectively, but cues X and Y are non-predictive, being paired with outcomes 1 and 2 equally often. Once this training is complete and participants show good levels of learning, a second phase is introduced, in which the cues are trained with new outcomes, and both cues are now perfectly predictive of these outcomes (e.g., AX-O3; BY-O4). The central finding of this procedure is that, in the second phase, participants learn more about the cues that were predictive in the first phase than about the cues that were previously non-predictive. The interpretation of this result [e.g., @lepelleyLearnedAssociabilityAssociative2003] is that, in the first phase, the cues that were established as predictive undergo an increase in their ability to attract attention, which enhances learning in the second phase. Consistent with this interpretation is the observation that (a) cues which have been established as predictive attract more dwell time (as measured by eye-gaze recording) than non-predictive cues and (b) this overt measure of differential attention correlates with the size of the bias in learning [@lepelleyOvertAttentionPredictiveness2011]. The learned predictiveness effect has been extensively replicated and reproduced in studies of learning [for a review, see: @lepelleyAttentionAssociativeLearning2016].

The second way in which learning can modify the attention paid to stimuli is the *uncertainty principle* [e.g., @pearceModelPavlovianLearning1980; see also: @lepelleyModelingAttentionAssociative2012; @schmajukLatentInhibitionNeural1996] which states that more attention will be paid to cues which have an uncertain outcome. It is thought that this mode of attention is useful as it allows for the exploration of cues whose predictive validity is uncertain, in order to discover relationships between these events. Griffiths et al. [-@griffithsNegativeTransferHuman2011] showed an instance of this principle using the “negative transfer” procedure. In this study, participants experienced a stimulus followed by a small-magnitude outcome (a food predicted a minor allergic reaction) in the first phase, and the same stimulus followed by a larger-magnitude outcome (a food predicted a critical allergic reaction) in the second phase. The study had two groups, with the only difference between the groups being that one group received a small number of presentations of the stimulus in the absence of the outcome between the first and second phases of the experiment in order to introduce uncertainty into the learnt contingency. The critical result was that the group that received “no outcome” trials learned more quickly about the large outcome in the second phase, compared to the group who did not have these no outcome trials. This result suggests that this brief period of uncertainty enhanced the attention paid to the cues facilitating subsequent learning[^1].

[^1]: It should be noted that this result has proved difficult to reproduce (see Le Pelley et al., 2016).

At face value, these two principles seem to be incompatible or contradictory: the predictiveness principle states we focus resources on cues that we know about, whilst the uncertainty principle states that we focus resources on cues we are less sure about. However, it is quite possible that both principles operate and describe changes in the allocation of attention, depending on the experienced contingencies. In fact, a number of hybrid models of learning and attention have tried to reconcile the evidence in favour of both principles [e.g., @esberReconcilingInfluencePredictiveness2011; @lepelleyRoleAssociativeHistory2004; @pearceTwoTheoriesAttention2010], and some of them propose that the predictiveness and the uncertainty principles may have different functions [e.g., @kerstenTwoCompetingAttentional1998; @lepelleyRoleAssociativeHistory2004]. The predictiveness principle leads to a prioritization of information in situations in which outcome events are reliable. However, when outcome events are less stable, it is less advantageous to invest cognitive resources in exploiting what is known. Under these circumstances it might be more advantageous to explore other sources of information, in order to attempt to reduce the uncertainty in the environment. For example, a teacher can easily identify students that might need extra help, solely focusing on their grades on the exams. However, there might be students who pass those exams, but show other, less reliable signs of a need for extra help, such as poor class engagement or absenteeism. For this reason, if the teacher wants to better understand the needs of their students, they might explore new signals in order to reduce uncertainty.

There is a growing body of evidence that points towards both principles operating in parallel in human contingency learning tasks [@koenigRewardDrawsEye2017; @luquePredictionUncertaintyAssociative2017; @torrents-rodasEvidenceTwoAttentional2021]. Beesley et al. [-@beesleyUncertaintyPredictivenessDetermine2015], for example, adapted the learned predictiveness design of Le Pelley and McLaren [-@lepelleyLearnedAssociabilityAssociative2003] to manipulate both predictiveness and uncertainty within the same procedure. In this study, each compound of cues had either a certain contingency with the outcome (i.e., it was consistently followed by the same outcome), or an uncertain contingency (i.e., it was probabilistically related to the outcomes, with one outcome occurring on 70% of trials, and the other on 30% of trials). Measuring participants eye gaze, this study showed that on “uncertain trials”, all cues (both predictive and non-predictive) received more attention than the cues did on “certain trials”. However, a predictiveness effect was also evident, with higher attention to predictive than to non-predictive cues, although this effect was only evident for cues. Thus, this study showed that attention is both determined by the uncertainty principle, since there was higher attention when uncertainty was high, as well as by the predictiveness principle, since within each compound, attention was devoted more to predictive over non-predictive cues.

A study by Easdale et al. [-@easdaleOnsetUncertaintyFacilitates2019] examined how these differences in uncertainty affected the rate of learning about cues. Participants experienced the same certain and uncertain contingencies as in Beesley et al. [-@beesleyUncertaintyPredictivenessDetermine2015] in a first phase, before receiving a second phase in which there were new contingencies to learn that would resolve the uncertainty entirely. Contrary to the expectations of models of the uncertainty principle [e.g., @pearceModelPavlovianLearning1980] it was found that participants who initially experienced the uncertain contingencies learnt about these new contingencies more *slowly* than those participants who first learnt about certain contingencies. Easdale et al. argued that this result provides evidence to support a distinction between “expected uncertainty” and “unexpected uncertainty”: in the former, participants may learn to anticipate variation in the outcome, which then leads to slower acquisition of the new contingencies [see also, @behrensLearningValueInformation2007].

Torrent-Rodas et al. [-@torrents-rodasEffectPredictionError2023] also examined the impact of uncertainty on overt attention and new learning using a within-subjects design. In a first phase they found that non-predictive cues that were associated with maximal prediction error (e.g., cue X during XZ-O1 and XZ-O2 training) received higher levels of attention compared to predictive cues (e.g., cues A and B during AZ-O1 and BZ-O2 training). However, when the participants were given new contingencies to learn in a second stage, discriminations between compounds that relied on previously non-predictive cues were not learnt at a faster rate than those that relied on previously predictive cues. Thus, like the results of Easdale et al., the data from Torrents-Rodas et al. suggest that expected uncertainty drives higher levels of overt attention to cues, but this does not translate into more rapid learning. One of the reasons why this finding of slower learning under conditions of expected uncertainty is surprising, is that participants in this condition showed higher attention to the cues in the first stage. Thus, the data from Easdale et al. and Torrents-Rodas et al. represent a paradoxical set of results for theories of associative learning to explain, since participants were overtly attending to cues more in the uncertain condition, yet this did not translate into faster learning about these cues. All attentional theories of associative learning predict that the attention paid to a stimulus is directly related to the rate at which learning occurs for that stimulus. This raises the question of what the high levels of overt attention to uncertain cues represent in the results of Beesley et al. [-@beesleyUncertaintyPredictivenessDetermine2015], Easdale et al. [-@easdaleOnsetUncertaintyFacilitates2019], and Torrents-Rodas et al. [-@torrents-rodasEffectPredictionError2023]. It is this process that is currently poorly understood and is the focus of the current study.

A critical question arises from this paradox: do uncertain conditions result in an increase in cognitive processing? Pearce and Hall [-@pearceModelPavlovianLearning1980] suggested that stimuli which are part of an unfamiliar (i.e. unlearned) task undergo more controlled processing (Pearce and Hall, 1980, p 549) and only transition to more automatic processing once the task is familiar. Consequently, we might expect that uncertainty should not only increase just the associative learning pertaining to these stimuli (i.e., “learning rate”), but also the memory of the stimulus representation itself [@chunInteractionsAttentionMemory2007]. For example, Otten et al. [-@ottenBrainActivityEvent2006] have shown that neural activity associated with a cue to semantically process an upcoming stimulus predicts the successful later retrieval of that stimulus at test; deeper processing tasks encourage richer encoding.

In the current study, two experiments were conducted following the design of Beesley et al. [-@beesleyUncertaintyPredictivenessDetermine2015], in which the predictiveness and the uncertainty of the cues were manipulated. Based on the notion that greater stimulus processing is associated with superior memory recall [@craikLevelsProcessingFramework1972; @craikDepthProcessingRetention1975; @fletcherFunctionalRolesPrefrontal1998], stimulus processing was measured by means of a recognition memory test for the cues at the end of the task. Experiment 1 assessed the memory for predictive and non-predictive cues trained either under certain or expected uncertain contingencies, and Experiment 2 contrasted the effect of expected and unexpected uncertainty.

# Experiment 1

The purpose of Experiment 1 was to examine differences in recognition memory in a learned predictiveness procedure under certain and uncertain cue-outcome contingency conditions. Two groups were trained, one with a perfect contingency between the predictive cues and their paired outcome (Group Certain) and one with a contingency of 0.8 between the predictive cues and their paired outcome (Group Uncertain). After this training, we tested the memory for cues in both groups using a two-alternative forced-choice memory test. On each test trial participants viewed a cue they had seen in the training phase (the target), and a foil, which was very similar to one of the cues seen in training.

The design of Experiment 1 is shown in @tbl-exp1. Previous experiments have established that for uncertain contingencies, participants spend longer attending to (looking at) all cues compared to attention to cues in certain contingencies [@beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019; @walkerProtectionUncertaintyExploration2022]. Experiment 1 therefore aimed to test whether uncertain contingencies, where it is well established that there is a high level of attention to cues, result in an improvement in the processing of these stimuli. As such, we predicted that memory would be better, overall, for the cues in group Uncertain compared to group Certain. Also, on the basis of the better attention found to predictive than non-predictive cues in the learned predictiveness effect, we anticipated seeing superior memory scores for the predictive than the non-predictive cues in group Certain.

::: {#tbl-exp1 apa-note="Uppercase letters A, B, X, and Y represent the cues presented during training. O1 and O2 represent the outcomes presented in training. Lowercase letters a, b, x, and y represent the foils that are similar to the (corresponding upper-case letter) cues presented in the training phase. The numbers before the trials in the uncertain condition define the proportion of trials of that type that were presented." apa-twocolumn="true"}
+----------------+-------------------------+----------------+
| Group          | Training                | Test           |
+================+:=======================:+:==============:+
| Certain        | AX - O1                 | A vs           |
|                |                         | *b*/*x*/*y*    |
|                | AY - O1                 |                |
|                |                         | B vs           |
|                | BX - O2                 | *a*/*x*/*y*    |
|                |                         |                |
|                | BY - O2                 | X vs           |
|                |                         | *a*/*b*/*y*    |
|                |                         |                |
|                |                         | Y vs           |
|                |                         | *a*/*b*/*x*    |
+----------------+-------------------------+----------------+
| Uncertain      | 0.8 AX - O1 / 0.2 AX -  | A vs           |
|                | O2                      | *b*/*x*/*y*    |
|                |                         |                |
|                | 0.8 AY - O1 / 0.2 AY -  | B vs           |
|                | O2                      | *a*/*x*/*y*    |
|                |                         |                |
|                | 0.8 BX - O2 / 0.2 BX -  | X vs           |
|                | O1                      | *a*/*b*/*y*    |
|                |                         |                |
|                | 0.8 BY - O2 / 0.2 BY -  | Y vs           |
|                | O1                      | *a*/*b*/*x*    |
+----------------+-------------------------+----------------+

Design of Experiment 1
:::

## Methods

### Transparency and openness statement

In this study we detail the processes for identifying any data to be excluded, any data exclusions and all measures in the study. Statistical analyses were conducted using RStudio [-@positteamRStudio2024], with R version 4.4.1 [@rcoreteamLanguageEnvironmentStatistical2024]. All experiments were built with the open-source software PsychoPy [v. 2022.2.4, @peircePsychoPy2ExperimentsBehavior2019], and all experiments were run on Pavlovia. Participants were recruited through Prolific. The design and analysis of the experiments were based on previously published manuscripts but were not preregistered. Some of the pilotwork, as well as the materials and data of the two experiments reported in this paper are freely available at: www.github.com/munizdiezclara/UNM_draft. All the experiments reported in this paper received ethical approval by the Ethics Committee at the
School of Psychology, Lancaster University, UK.

### Participants

```{r, include=FALSE}
#load the data
load("UNM07_proc_data.RData")
UNM07_demographics <- demographics
UNM07_training <- training
UNM07_test <- test
UNM07_not_passed <- not_passed_pNum

#create the PPR measure
UNM07_training <- UNM07_training %>%
  mutate(prob_response = case_when((cue1 == 1 | cue1 == 3) & response == "o1_image" ~ 1,
                                   (cue1 == 1 | cue1 == 3) & response == "o2_image" ~ 0, 
                                   (cue1 == 2 | cue1 == 4) & response == "o1_image" ~ 0,
                                   (cue1 == 2 | cue1 == 4) & response == "o2_image" ~ 1))

#detect and clean participants that not passed the test comprehension check
UNM07_training <- filter(UNM07_training, !pNum %in% UNM07_not_passed$pNum)
UNM07_test <- filter(UNM07_test, !pNum %in% UNM07_not_passed$pNum)
UNM07_demographics <- filter(UNM07_demographics, !pNum %in% UNM07_not_passed$pNum)
```

`r nrow(demographics)` participants were recruited through Prolific, from which four where excluded due to failing the comprehension check before the test. The sample consisted of `r length(which(UNM07_demographics$gender == "female"))` women, `r length(which(UNM07_demographics$gender == "male"))` men and one non-binary person, with `r n_distinct(UNM07_demographics$Nationality)` different nationalities. The mean age was `r format(mean(UNM07_demographics$age, na.rm = TRUE), digits = 3)` calculated for the `r nrow(UNM07_demographics) - sum(is.na(UNM07_demographics$age))` participants that reported their age (range `r min(UNM07_demographics$age, na.rm = TRUE)` - `r max(UNM07_demographics$age, na.rm = TRUE)`). Pre-screening of participants in Prolific ensured that they had normal or corrected to normal vision, fluency in English language, and had not participated in previous studies from our lab. Participants were rewarded with £2.70 for their participation in the study. Participants were randomly allocated to either the Certain or Uncertain condition. Four participants were excluded due to failing the comprehension check before the test (three in group Certain and one in group Uncertain). Post-hoc calculations using G\*Power 3.1 [@faulStatisticalPowerAnalyses2007] revealed that this sample size had a power of .99 to detect an effect size of *η~p~^2^* = .08 that was observed for the *group x predictiveness* interaction reported in @fig-testExp1.

### Apparatus and stimuli

Participants were presented with a task built in PsychoPy [v. 2022.2.4, @peircePsychoPy2ExperimentsBehavior2019] and hosted in Pavlovia. The task was designed so it could only be run on a computer, but not on mobile devices. The screen background colour was grey (RGB: 128, 128, 128) and all stimuli and instructions were presented against this background. The four cues presented to each participant (A, B, X and Y) were randomly selected from a set of eight images, representing imaginary chemical compounds made of three red circles and three blue circles connected with black lines. Each cue was 945 x 945 pixels, automatically re-scaled to 0.4 x 0.4 of the window height. The foils (*a*, *b*, *x* and *y*) used in the tests were colour-modifications of the original cues: the colours of one red and one blue circle were switched (four remained unchanged). The outcomes (O1 and O2) were two images displaying a mutant creature, black with yellow details. Each was 332 x 664 pixels, automatically re-scaled to 0.16 x 0.2 of the window height. All of the images used in the experiment are presented in Appendix I.

### Design

The design of Experiment 1 is shown in @tbl-exp1. The experiment used a mixed design, with cue-predictiveness (P and NP) manipulated within-subjects and the contingency between predictive cues and outcomes manipulated between-subjects (group Certain and Uncertain). The training phase consisted of eight blocks, each containing 20 trials. There were four trial types (compound cues), each presented five times per block, with cues A and B predictive of outcome 1 and 2, respectively. In the Certain group, cues A and B were perfectly predictive of the outcomes they were paired with, while cues X and Y were non-predictive. For the Uncertain group, cues A and B were the best available predictors on each trial but had a 0.8 contingency with the predicted outcome. To implement this contingency, in each block, for four of the five trials one outcome was “correct” (e.g., AX-O1) and for the remaining one the alternative outcome was “correct” (e.g., AX-O2). Cues X and Y were paired equally often with outcomes 1 and 2 and were therefore non-predictive. The position of the cues and the outcomes (right-left), as well as the order of presentation of the trials, was fully randomized within each block. Note that during the training stage cues A, B, X and Y were all presented equally frequently and were therefore (on average) equivalent in their recency to the recognition memory tests. Other things being equal, then, their memory traces should be equivalent. Thus, if recognition scores differ between these stimuli, it would indicate a difference in the level of processing that the cues underwent during training.

Training was followed by a memory test, in which two images were presented side-by-side, with one image being a cue presented in the training phase, the target, and the other image being a variation of one of the other training cues, the foil. The left-right display of the target and the foil was counterbalanced, in such a way that each target appeared once on the left and once on the right. Each of the four targets (A, B, X and Y) was presented six times (24 trials in total), with target presented twice with each of the three foils shown in @tbl-exp1. For example, cue A was presented with the foil corresponding to cue B, the foil corresponding to cue X, and the foil corresponding to cue Y. It is worth noting that this distribution of the target vs foil trials produces two types of trials: congruent trials, in which the target and the foil were of the same predictiveness, and incongruent trials, in which the target and foil differed in their predictiveness. However, in order to make the presentation of the results clearer, the following analyses do not include this factor, given that it had no main effect in either experiment, and did not interact with any of the critical significant results. Analyses that include this factor for both Experiment 1 and 2 are reported in Appendix II.

### Procedure

Participants were presented with the study information and responded to a series of questions to give informed consent. If participants gave consent, they proceeded to the training phase instructions. These described the initial learning task, in which they would see two images of fictitious chemicals that would be mixed to produce a mutant creature. Participants were told that their task was to predict which mutant will result from each combination of chemicals. They were also instructed to use the feedback provided after their decision to make their future choices more accurate.

After reading the instructions, participants were presented with a comprehension check, in which the instructions were summarised, and they were asked to select the answer that best described what they had to do in the task. The experiment ended if participants failed this comprehension check twice. If participants passed the comprehension check, they proceeded to the training phase.

All the trials in the training phase started with a 0.5 second blank screen. After that, two cues were presented in the top part and the two outcomes in the bottom part. The coordinates (x/y PsychoPy height units) of the centre of the four stimuli were: left cue, -0.3 x 0.2; right cue, 0.3 x 0.2 left outcome, -0.125 x -0.2; right outcome, 0.125 x -0.2. All stimuli were rescaled according to the height of the monitor. The participants had to select one of the outcomes by clicking on them, which was indicated by a yellow frame surrounding the selected outcome. Feedback was provided after 0.5 seconds: the correct outcome was surrounded by a green frame. If the correct outcome was selected by participants, the message “CORRECT!” was displayed in the centre of the screen in green (RGB: 0, 255, 0), otherwise the message “INCORRECT!” appeared in red (RGB: 255, 0, 0). After two seconds, the next trial started. If participants failed to select an outcome within 10 seconds of the trial starting, all images disappeared from the screen and the message “TIMEOUT - TOO SLOW” was presented in the centre of the screen in red, and the next trial started after one second. An example of a training trial can be seen in @fig-trainexample.

```{r @fig-trainexample, echo=FALSE}
#| label: fig-trainexample
#| fig-cap: An example of a Training Trial in Experiment 1.
#| apa-twocolumn: true
#| apa-note: "The timings represent the duration of each display. "
#| out-width: 100%
#| fig-pos: h
knitr::include_graphics("images/trainexample.png")
```

Once participants had completed the training phase, the instructions for the memory test were displayed, telling participants that they were about to see two similar chemicals on each trial, and only one of them had appeared in the previous task. They had to select that chemical and then rate their confidence on a scale from 1 to 10 (see below). No feedback on response accuracy was provided. After the instructions, a comprehension check was included, presented only once and participants continued to the test, irrespective of the response given.

All test trials started with a 0.5 second blank screen. After that, the images of a target (a cue presented in the training phase), and a foil were presented in the top half of the screen. These images had the same size and position as in the training phase. Participants had to click on the image they thought they had seen in the previous phase, after which a rating scale was displayed for them to give a confidence rating. Above this rating scale, the question *How confident are you of your response?* was displayed. The rating scale had 10 points, with the labels *I am guessing* on the left end (point 1), and *I am certain* on the right end (point 10), and a red dot in the middle. Participants had to click on the rating scale to move the red dot to give their confidence rating. After this, a button with the word *CONTINUE* appeared. All responses in the test phase had no time limit and participants could advance to the next test trial at their own pace. The test trials procedure is shown in @fig-testexample.

```{r @fig-testexample, echo=FALSE}
#| label: fig-testexample
#| fig-cap: An example of a test Trial in Experiment 1.
#| apa-twocolumn: true
#| apa-note: "The timings represent the duration of each display. "
#| out-width: 100%
#| fig-pos: h
knitr::include_graphics("images/testexample.png")
```

## Results

Since participants in the uncertain condition received trials in which the alternative outcome was presented on 20% of the trials, even if participants in group Uncertain were to always select the most probable outcome (O1 when A is present and O2 when B is present), it would result in an accuracy score of 80%. Thus, we calculated the proportion of probable responses (PPR): for the Uncertain group, on each trial, the score was 0 when participants chose the less probable outcome (i.e., O2 for A and O1 for B) and 1 when they chose the most probable outcome (i.e., O1 for A and O2 for B). For the certain condition, this equates to a standard accuracy score.

```{r, include = FALSE}
#Calculate mean PPR by participant, block and group
UNM07_acc <- UNM07_training %>%
  group_by (pNum, block, condition) %>%
  summarise(mean_response = mean(prob_response, na.rm = TRUE))
#Calculate the mean PPR and standard error for each block, including the groups
UNM07_MA_training <- UNM07_acc %>%
  group_by(block, condition) %>%
  summarise(mean_accuracy = mean(mean_response, na.rm = TRUE), 
            se_accuracy = sd(mean_response, na.rm = TRUE)/sqrt(length(mean_response)))
```

@fig-trainingExp1 shows the mean PPR across blocks for each group. Participants in the Certain group showed a higher PPR through training than the Uncertain group, reaching a PPR of about 0.85 on block 8. The Uncertain group showed consistently lower PPR, that reached approximately 0.7 in block 8.

```{r, echo = FALSE, message=FALSE}
#| label: fig-trainingExp1
#| fig-cap: PPR on the training phase of Experiment 1.
#| apa-note: "Mean proportion of probable responses (±SEM) during the training phase of Experiment 1, for groups trained with certain and uncertain contingencies."
#| fig-height: 4
ggplot(UNM07_MA_training, mapping = aes(x = block, y = mean_accuracy, group = condition, color = condition)) +
  geom_point(mapping = aes(shape = condition), size = 2.5) +
  geom_line() +
  geom_errorbar(aes(x= block, y = mean_accuracy, ymin = mean_accuracy-se_accuracy, ymax = mean_accuracy+se_accuracy), colour = "black", width=.1)+
  scale_x_continuous(name = "Block") + 
  labs(shape = "Group", colour = "Group") +
  scale_color_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6, 7, 8))+
  scale_y_continuous(name = "PPR", limits = c(NA, 1))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA
UNM07_acc$block <- factor(UNM07_acc$block)
UNM07_acc$pNum <- factor(UNM07_acc$pNum)
UNM07_acc$condition <- factor(UNM07_acc$condition)
ANOVA_UNM07_acc <- aov_car(formula = mean_response ~ condition + Error(pNum/block), data = UNM07_acc)
print(ANOVA_UNM07_acc)
#Bayesian Anova
bay_ANOVA_UNM07_acc <- anovaBF(formula = mean_response ~ condition + block + pNum,
        data = data.frame(UNM07_acc),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM07_acc)
bay_ANOVA_UNM07_acc_int <- bay_ANOVA_UNM07_acc[4]/bay_ANOVA_UNM07_acc[3]
print(bay_ANOVA_UNM07_acc_int)
```

These data were analysed with a mixed model ANOVA including the between-subjects factor *group* and the within-subjects factor *predictiveness*. This ANOVA (and all the following ones in the paper) included the Greenhouse-Geisser correction of the degrees of freedom when the sphericity assumption was not fulfilled. The ANOVA found significant both the main effect of *group*, `r apa(ANOVA_UNM07_acc, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM07_acc[2])`, and of *block*, `r apa(ANOVA_UNM07_acc, effect = "block")`, `r report_BF_and_error(bay_ANOVA_UNM07_acc[1], sci_not = TRUE)`. There was no interaction effect between these factors, `r apa(ANOVA_UNM07_acc, effect = "condition:block")`, `r report_BF_and_error(bay_ANOVA_UNM07_acc_int[1])`. These results indicate that PPR increased across training and that the Certain group showed a consistently higher PPR than the Uncertain group.

```{r, include=FALSE}
#calculate mean accuracy by participant, block and group
acc_UNM07_test <- UNM07_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
#Calculate the mean accuracy and standard error for each block, including the groups
MA_test <- acc_UNM07_test %>%
  group_by(condition, predictiveness) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

@fig-acctestExp1 shows the accuracy results from the recognition memory test. Accuracy for non-predictive cues was lower than for the predictive cues in the Certain group, but this difference was not present in the
Uncertain group.

```{r, echo = FALSE, message=FALSE}
#| label: fig-acctestExp1
#| fig-cap: Accuracy on the test phase of Experiment 1.
#| apa-note: "Mean accuracy (±SEM) during the test phase of Experiment 1, for groups trained with certain and uncertain contingencies."
#| fig-height: 4
ggplot(data = MA_test, mapping = aes(x = factor(condition, level=c('Certain', 'Uncertain')), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA accuracy
acc_UNM07_test$predictiveness <- factor(acc_UNM07_test$predictiveness)
acc_UNM07_test$condition <- factor(acc_UNM07_test$condition)
acc_UNM07_test$pNum <- factor(acc_UNM07_test$pNum)
ANOVA_acc_UNM07_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness), data = acc_UNM07_test)
print(ANOVA_acc_UNM07_test)

bay_ANOVA_acc_UNM07_test <- anovaBF(formula = acc ~ condition*predictiveness + pNum,
        data = data.frame(acc_UNM07_test),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_acc_UNM07_test)

bay_ANOVA_acc_UNM07_test_gxp <- bay_ANOVA_acc_UNM07_test[4]/bay_ANOVA_acc_UNM07_test[3]
print(bay_ANOVA_acc_UNM07_test_gxp)
```

```{r, include = FALSE}
# SME of the condition:predictiveness interaction
SME_acc_UNM07_test <- UNM07_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
#calculate the simple main effect of predictiveness
sme_acc_UNM07_test_pred <- SME_acc_UNM07_test %>%
  group_by(condition) %>%
  anova_test(acc ~ predictiveness + Error(pNum/predictiveness), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_acc_UNM07_test_pred #Call the output table

# Conditions you want to test
conditions <- c("Certain", "Uncertain")
# Run the Bayesian ANOVA for each condition
bay_SME_acc_UNM07_test <- lapply(conditions, function(cond) {
  df_cond <- UNM07_test %>%
    filter(condition == cond) %>%
    group_by(pNum, predictiveness) %>%
    summarise(acc = mean(acc, na.rm = TRUE),
              .groups = "drop")
  df_cond$predictiveness <- factor(df_cond$predictiveness)
  df_cond$pNum <- factor(df_cond$pNum)
  res <- anovaBF(
    formula = acc ~ predictiveness + pNum,
    data = data.frame(df_cond),
    whichRandom = "pNum",
    iterations = bfit
  )
  return(res)
})
names(bay_SME_acc_UNM07_test) <- conditions
# Create summary table
bay_summary_SME_acc_UNM07_test <- lapply(names(bay_SME_acc_UNM07_test), function(nm) {
  df <- as.data.frame(bay_SME_acc_UNM07_test[[nm]])
  df$condition <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_acc_UNM07_test <- bay_summary_SME_acc_UNM07_test %>%
  select(condition, everything())
# Show table
print(bay_summary_SME_acc_UNM07_test)
```

A mixed model ANOVA with the between subjects-factor *group* (Uncertain vs Certain) and the within-subjects factor *predictiveness*, found non significant the main effects of *group*, *F* $\le$ 1, `r report_BF_and_error(bay_ANOVA_acc_UNM07_test[1])`, or *predictiveness*, `r apa(ANOVA_acc_UNM07_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM07_test[2])`, but there was a significant *group x predictiveness* interaction, `r apa(ANOVA_acc_UNM07_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM07_test_gxp[1])`. Simple main effects analysis showed a significant effect of predictiveness for group Certain, *F* (`r sme_acc_UNM07_test_pred[1, 3]`, `r sme_acc_UNM07_test_pred[1, 4]`) = `r sme_acc_UNM07_test_pred[1, 5]`, *p* = `r sme_acc_UNM07_test_pred[1, 9]`, *η~p~^2^* = `r sme_acc_UNM07_test_pred[1, 8]`, `r report_BF_and_error(bay_SME_acc_UNM07_test[["Certain"]])`, but not for group Uncertain, *F* $\le$ 1, `r report_BF_and_error(bay_SME_acc_UNM07_test[["Uncertain"]])`. These analyses suggest that memory representations were stronger for predictive than non-predictive cues in group Certain, but that overall memory performance was equivalent in the two groups.

```{r, include = FALSE}
#Calculate the mean memory score by participant, group and predictiveness
UNM07_memscore_test <- UNM07_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
#Calculate the mean memory score and standard error for each group and predictiveness of the cues
UNM07_MS_test <- UNM07_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_mem_score = mean(c_mem_score, na.rm = TRUE), 
            se_mem_score = sd(c_mem_score, na.rm = TRUE)/sqrt(length(c_mem_score)))
```

Memory scores, calculated as the product of the accuracy score (1 or 0) with the confidence rating given, can be seen in @fig-testExp1. The memory scores showed a very similar pattern to the accuracy data: memory scores for non-predictive cues were lower than for the predictive cues in the Certain group, but not for the Uncertain group, and overall memory scores were a similar level in the two groups.

```{r, echo = FALSE, message=FALSE}
#| label: fig-testExp1
#| fig-cap: Memory scores during the Test of Experiment 1.
#| apa-note: "Mean memory scores (±SEM) during the Test phase of Experiment 1 for predictive and non-predictive trials in the Certain and Uncertain groups."
#| fig-height: 4
ggplot(UNM07_MS_test, mapping = aes(x = factor(condition, level=c('Certain', 'Uncertain')), y = mean_mem_score, fill = predictiveness)) + #display groups in axis x, memory score in axis y, and fill the bars in different colours depending on predictiveness
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  scale_y_continuous(name = "Memory score")+
  #scale_fill_grey(start = 0.33) +
  theme_apa()
```

```{r, include=FALSE}
#ANOVA mem_score
UNM07_memscore_test$predictiveness <- factor(UNM07_memscore_test$predictiveness)
UNM07_memscore_test$condition <- factor(UNM07_memscore_test$condition)
UNM07_memscore_test$pNum <- factor(UNM07_memscore_test$pNum)
ANOVA_UNM07_test <- aov_car(formula = mem_score ~ condition + Error(pNum*predictiveness), data = UNM07_memscore_test)
print(ANOVA_UNM07_test)

bay_ANOVA_UNM07_test <- anovaBF(formula = mem_score ~ condition*predictiveness + pNum,
        data = data.frame(UNM07_memscore_test),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM07_test)
bay_ANOVA_UNM07_test_gxp <- bay_ANOVA_UNM07_test[4]/bay_ANOVA_UNM07_test[3]
print(bay_ANOVA_UNM07_test_gxp)
```

```{r, include = FALSE}
# SME of the condition:predictiveness interaction
SME_mem_UNM07_test <- UNM07_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(mem = mean(c_mem_score, na.rm = TRUE))
#calculate the simple main effect of predictiveness
sme_mem_UNM07_test_pred <- SME_mem_UNM07_test %>%
  group_by(condition) %>%
  anova_test(mem ~ predictiveness + Error(pNum/predictiveness), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_mem_UNM07_test_pred #Call the output table

# Conditions you want to test
conditions <- c("Certain", "Uncertain")
# Run the Bayesian ANOVA for each condition
bay_SME_mem_UNM07_test <- lapply(conditions, function(cond) {
  df_cond <- UNM07_test %>%
    filter(condition == cond) %>%
    group_by(pNum, predictiveness) %>%
    summarise(mem = mean(c_mem_score, na.rm = TRUE),
              .groups = "drop")
  df_cond$predictiveness <- factor(df_cond$predictiveness)
  df_cond$pNum <- factor(df_cond$pNum)
  res <- anovaBF(
    formula = mem ~ predictiveness + pNum,
    data = data.frame(df_cond),
    whichRandom = "pNum",
    iterations = bfit
  )
  return(res)
})
names(bay_SME_mem_UNM07_test) <- conditions
# Create summary table
bay_summary_SME_mem_UNM07_test <- lapply(names(bay_SME_mem_UNM07_test), function(nm) {
  df <- as.data.frame(bay_SME_mem_UNM07_test[[nm]])
  df$condition <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_mem_UNM07_test <- bay_summary_SME_mem_UNM07_test %>%
  select(condition, everything())
# Show table
print(bay_summary_SME_mem_UNM07_test)
```

The mixed model ANOVA mirrored the findings from the accuracy analysis: there was no main effects of *group*, *F* $\le$ 1, `r report_BF_and_error(bay_ANOVA_UNM07_test[1])`, nor of *predictiveness*, `r apa(ANOVA_UNM07_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM07_test[2])`, but there was a significant *group x predictiveness* interaction, `r apa(ANOVA_UNM07_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM07_test_gxp[1])`. Simple main effects showed a significant effect of *predictiveness* for group Certain, *F*(`r sme_mem_UNM07_test_pred[1,3]`, `r sme_mem_UNM07_test_pred[1,4]`) = `r sme_mem_UNM07_test_pred[1,5]`, *p* = `r sme_mem_UNM07_test_pred[1,9]`, *η~p~^2^* = `r sme_mem_UNM07_test_pred[1,8]`, `r report_BF_and_error(bay_SME_mem_UNM07_test[["Certain"]])`, but not for group Uncertain, *F* $\le$ 1,`r report_BF_and_error(bay_SME_mem_UNM07_test[["Uncertain"]])`. Again, memory score analysis suggests there was better memory for predictive cues than for non-predictive cues in group Certain, whereas this difference was not present in group Uncertain. The Bayesian analysis suggests that, overall, memory was equivalent in the two groups.

## Discussion

Experiment 1 aimed to examine the effect of uncertainty on recognition memory for predictive and non-predictive cues. The participants in group Uncertain were exposed to a probabilistic relationship between the predictive cues and their respective outcomes, while those in group Certain received deterministic relationships. There was an effect of cue-predictiveness in group Certain, with better recognition memory for the predictive than the non-predictive cues, but this effect was not present in the Uncertain group, with evidence to suggest memory for predictive and non-predictive cues was equivalent. This is consistent with previous studies [@beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019] that have shown that attention (in those cases measured by eye-gaze dwell times) decreased for non-predictive cues but not for predictive cues, only under certain training. That decrease in attention could be responsible for the worse memory performance for the non-predictive cues, compared with the predictive cues, in the certain group. However, we hypothesised that the previously observed effect of uncertainty on increased overt attention (e.g., Beesley et al., 2015) would lead to better memory for cues in the Uncertain condition. This was not the case: in a final recognition memory test, the two groups showed a similar overall level of recognition memory for the cues.

A central distinction made in Easdale et al. [-@easdaleOnsetUncertaintyFacilitates2019] was that between *expected-* and *unexpected-uncertainty*. In those experiments, participants who experienced a sustained period of training with uncertain compounds (as is the case in group “Uncertain” in Experiment 1) learnt more slowly about new contingencies, compared to a group that received a sudden and unexpected change in the contingencies. Thus, it may be the case that the current uncertain condition does not promote higher recognition memory overall, because participants have come to expect a certain level of uncertainty and are no longer engaging in an exploratory mode of cue-processing. Of course, the expected levels of high attention to cues under uncertain conditions presents a paradox for learning and attention research: why does a high level of attention not translate to better learning and memory for those cues? We return to this point in the general discussion. Nevertheless, this analysis of the findings in terms of expected and unexpected uncertainty suggests that a more acute period of uncertainty may (re)engage a mode of exploratory attentional processing for the cues, which would result in better memory of those cues. Experiment 2 tested this hypothesis.

# Experiment 2

Experiment 2 aimed to examine whether the introduction of uncertainty, following a period of certain training (i.e., unexpected uncertainty), would lead to an increase in cue-processing, as determined by better recognition memory for the cues. The design of Experiment 2 can be seen in @tbl-exp2. The experiment consisted of three groups. Groups Certain Long and Certain Short received training that was similar to the Certain condition from Experiment 1, experiencing certain contingencies between the cue compounds and the outcomes throughout the training phase, differing only in the amount of training they experienced. Group Unexpected Uncertain first experienced the certain contingencies before the contingencies became uncertain for a short period before the recognition memory test. Our prediction was that, if the introduction of unexpected uncertainty promotes greater levels of exploratory attention, then we should see better recognition memory performance in this uncertain condition, compared to the certain condition.

::: {#tbl-exp2 apa-note="Uppercase letters A, B, X, and Y represent the cues presented during training. O1 and O2 represent the outcomes presented in training. Lowercase letters a, b, x, and y represent the foils that are similar to the (corresponding upper-case letter) cues presented in the training phase. The numbers before the trials define the proportion of trials of that type that were presented in the uncertain condition." apa-twocolumn="true"}
+----------------+----------------+-------------------------+----------------+
| Group          | Stage 1        | Stage 2                 | Test           |
+================+:==============:+:=======================:+:==============:+
| Certain Long   | AX - O1        | AX - O1                 |A vs *b*/*x*/*y*|
|                | AY - O1        | AY - O1                 |B vs *a*/*x*/*y*|
|                | BX - O2        | BX - O2                 |X vs *a*/*b*/*y*|
|                | BY - O2        | BY - O2                 |Y vs *a*/*b*/*x*|
+----------------+----------------+-------------------------+----------------+
| Certain Short  | AX - O1        |                         |A vs *b*/*x*/*y*|
|                | AY - O1        |                         |B vs *a*/*x*/*y*|
|                | BX - O2        |                         |X vs *a*/*b*/*y*|
|                | BY - O2        |                         |Y vs *a*/*b*/*x*|
+----------------+----------------+-------------------------+----------------+
| Unexpected     | AX - O1        |0.8 AX - O1 / 0.2 AX - O2|A vs *b*/*x*/*y*|
| Uncertainty    | AY - O1        |0.8 AY - O1 / 0.2 AY - O2|B vs *a*/*x*/*y*|
|                | BX - O2        |0.8 BX - O2 / 0.2 BX - O1|X vs *a*/*b*/*y*|
|                | BY - O2        |0.8 BY - O2 / 0.2 BY - O1|Y vs *a*/*b*/*x*|
+----------------+----------------+-------------------------+----------------+

Design of Experiment 2
:::

Group Certain Short received the same certain contingencies as the other two conditions in Stage 1 but did not experience Stage 2; they received a shorter training phase than the other two conditions. If the onset of the uncertainty leads to greater cue-processing, then we should also see better cue-memory in the Uncertain condition compared to the Certain Short condition. The inclusion of this condition is important because longer training with the certain contingencies in the Certain Long condition could *decrease* cue processing, which would be an alternative explanation of any difference in cue processing we observe between Group Unexpected Uncertain and Group Certain Long. If this is the case, we should see equivalent recognition memory in the Certain Short and Uncertain conditions, and poorer recognition memory in the Certain Long condition. Therefore, the addition of this third condition allowed us to make stronger inferences about the causal relationship between the onset of uncertainty and cue-processing.

## Methods

### Participants

```{r, include=FALSE}
#load the data
load("UNM08_proc_data.RData")
UNM08_demographics <- demographics
UNM08_training <- rbind(stage1, stage2)
UNM08_test <- test
UNM08_not_passed <- not_passed_pNum
UNM08_training <- filter(UNM08_training, !pNum %in% UNM08_not_passed$pNum)
UNM08_test <- filter(UNM08_test, !pNum %in% UNM08_not_passed$pNum)
UNM08_demographics <- filter(UNM08_demographics, !pNum %in% UNM08_not_passed$pNum)
#rename the uncertain condition as  Unexpected Uncertain
UNM08_training <- UNM08_training %>%
  mutate(condition = case_when(condition == "Certain Long" ~ "Certain Long",
                               condition == "Certain Short" ~ "Certain Short",
                               condition == "Uncertain" ~ "Unexpected Uncertain"))
UNM08_test <- UNM08_test %>%
  mutate(condition = case_when(condition == "Certain Long" ~ "Certain Long",
                               condition == "Certain Short" ~ "Certain Short",
                               condition == "Uncertain" ~ "Unexpected Uncertain"))
```

```{r, include = FALSE}
#create the PPR measure
UNM08_training <- UNM08_training %>%
  mutate(prob_response = case_when(cue1 == 1 & response == "o1_image" ~ 1,
                                   cue1 == 1 & response == "o2_image" ~ 0,
                                   cue1 == 2 & response == "o1_image" ~ 0,
                                   cue1 == 2 & response == "o2_image" ~ 1))

#detect and clean participants that had an PPR lower than 0.6 in the final block or not passed the test comprehension check
UNM08_block6 <- filter(UNM08_training, block == 6) %>%
  group_by(pNum, condition) %>%
 summarise (mean_response = mean(prob_response, na.rm = TRUE))
UNM08_low_acc_total <- filter(UNM08_block6, mean_response < 0.75) 
UNM08_low_acc <- UNM08_low_acc_total$pNum
UNM08_training <- filter(UNM08_training, !pNum %in% UNM08_low_acc_total$pNum)
UNM08_test <- filter(UNM08_test, !pNum %in% UNM08_low_acc_total$pNum)
UNM08_demographics <- filter(UNM08_demographics, !pNum %in% UNM08_low_acc_total$pNum)
```

`r nrow(demographics)` participants were recruited through Prolific. Eight participants were excluded on the basis of failing the comprehension check before the test, six in group Uncertain, one in group Certain Short, and one in group Certain Long. Since all three conditions experienced the same training in Stage 1, we imposed a performance criterion of 75% PPR (i.e., accuracy) in the last block of Stage 1, on the basis that the effect of "unexpected uncertainty" would be minimal if the contingencies had not been learned to a reasonable level at the point of this manipulation. Thirty-six participants were excluded due to a low PPR ($\le$ 0.75) on the last block of Stage 1, eight in group Certain Long, `r length(which(UNM08_low_acc_total$condition == "Certain Short"))` in group Certain Short and `r length(which(UNM08_low_acc_total$condition == "Uncertain"))` in group Uncertain, leaving `r nrow(UNM08_test)/24` participants overall in the final analyses. The mean age of the `r nrow(UNM08_demographics) - sum(is.na(UNM08_demographics$age))` participants that reported their age was `r format(mean(UNM08_demographics$age, na.rm = TRUE), digits = 3)` (range `r min(UNM08_demographics$age, na.rm = TRUE)` - `r max(UNM08_demographics$age, na.rm = TRUE)`), with `r length(which(UNM08_demographics$gender == "female"))` women, `r length(which(UNM08_demographics$gender == "male"))` men, and one non-binary person, and `r n_distinct(UNM08_demographics$Nationality)` different nationalities. Participants were randomly allocated to each condition. Post-hoc calculations using G\*Power 3.1 [@faulStatisticalPowerAnalyses2007] revealed that this sample size had a power of .79 to detect an effect size of *η~p~^2^* = .05 that was observed for the group main effect reported in @fig-testExp2.

### Apparatus and stimuli

The materials were the same as in Experiment 1.

### Design

The experiment used a mixed design (as seen in @tbl-exp2), with three groups: Certain Long, Certain Short, and Unexpected Uncertain. All groups received six blocks of certain training. Group Certain Long then received a further 4 blocks of certain training; group Unexpected Uncertain, received a further four blocks of uncertain training (with contingencies of 0.8); and group Certain Short received no further training (they completed six training blocks only). When training was completed, all groups progressed to the memory test, which was identical to that used in Experiment 1.

### Procedure

The procedure was identical to Experiment 1.

## Results

```{r, include = FALSE}
#calculate the mean PPR by participant, stage, block and group
UNM08_train <- UNM08_training %>%
  group_by (pNum, stage, block, condition) %>%
  summarise(mean_response = mean(prob_response, na.rm = TRUE))
#Calculate the mean PPR and standard error for each block, including the groups and stages
UNM08_MA_training <- UNM08_train %>%
  group_by(block, stage, condition) %>%
  summarise(mean_accuracy = mean(mean_response, na.rm = TRUE), 
            se_accuracy = sd(mean_response, na.rm = TRUE)/sqrt(length(mean_response)))

#add a dummy to display stage 2 for Certain Short
MA_stage2_dummy <- data.frame(stage = c('stage 2', 'stage 2', 'stage 2', 'stage 2'),
                              block = c(7:10),
                              condition = c('Certain Short', 'Certain Short', 'Certain Short', 'Certain Short'),
                              mean_accuracy = c(0.001, 0.002, 0.003, 0.004),
                              se_accuracy = c(0.0001, 0.00020, 0.0003, 0.00004))
UNM08_MA_training <- rbind(UNM08_MA_training, MA_stage2_dummy)
#change stage 1 and stage 2 to Stage1 and Stage 2, and Certain_short to Certain Short
UNM08_MA_training <- UNM08_MA_training %>%
  mutate(stage = case_when(stage == "stage 1" ~ "Stage 1",
                           stage == "stage 2" ~ "Stage 2"))
```

@fig-trainingExp2 shows the mean PPR for each group across the ten blocks of training. All participants showed a similar increase in PPR in stage 1, reaching a PPR of around 0.93 on block 6. In Stage 2, group Certain Long showed a maintenance of the rate of PPR across blocks 7-10, but the Unexpected Uncertain group showed a decrease in PPR to a level of around 0.85.

```{r, echo = FALSE, message=FALSE, warning=FALSE}
#| label: fig-trainingExp2
#| fig-cap: PPR on the training phase of Experiment 2.
#| apa-note: "Mean proportion of probable responses (±SEM) during the training phase of Experiment 2, plotted against the ten blocks of trials, for each Group."
#| fig-height: 4
ggplot(UNM08_MA_training, mapping = aes(x = block, y = mean_accuracy, group = factor(condition, level=c('Certain Long','Certain Short', 'Unexpected Uncertain')))) +
  geom_point(mapping = aes(shape =  factor(condition, level=c('Certain Long','Certain Short', 'Unexpected Uncertain')), color =  factor(condition, level=c('Certain Long','Certain Short', 'Unexpected Uncertain'))), size = 2.5) +
  geom_line(mapping = aes(color =  factor(condition, level=c('Certain Long','Certain Short', 'Unexpected Uncertain')))) +
  geom_errorbar(aes(x= block, y = mean_accuracy, ymin = mean_accuracy-se_accuracy, ymax = mean_accuracy+se_accuracy), colour = "black", width=.1)+
  facet_grid(cols = vars(stage), space = "free_x", scales = "free_x") + 
  scale_x_continuous(name = "Block", breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) + 
  scale_color_discrete(type = c("#AF8DC3", "#FEB24C", "#7FBF7B"))+
  labs(shape = "Group", color = "Group") +
  scale_y_continuous(name = "PPR", limits = c(0.5, 1))+
  theme_apa()
```
```{r, include=FALSE}
#ANOVA
UNM08_stage1 <- filter(UNM08_training, stage == "stage 1") %>%
  group_by (pNum, block, condition) %>%
  summarise(mean_response = mean(prob_response, na.rm = TRUE))
UNM08_stage1$block <- factor(UNM08_stage1$block)
UNM08_stage1$pNum <- factor(UNM08_stage1$pNum)
UNM08_stage1$condition <- factor(UNM08_stage1$condition)
ANOVA_UNM08_stage1 <- aov_car(formula = mean_response ~ condition + Error(pNum/block), data = UNM08_stage1)
print(ANOVA_UNM08_stage1)
#Bayesian Anova
bay_ANOVA_UNM08_stage1 <- anovaBF(formula = mean_response ~ condition + block + pNum,
        data = data.frame(UNM08_stage1),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM08_stage1)
bay_ANOVA_UNM08_stage1_int <- bay_ANOVA_UNM08_stage1[4]/bay_ANOVA_UNM08_stage1[3]
print(bay_ANOVA_UNM08_stage1_int)
```

The Stage 1 data were analysed with a mixed-model ANOVA, with the between-subjects factor of *group* (Certain Long, Certain Short, and Unexpected Uncertain), and the within-subjects factor of *block* (1-6). This revealed a significant effect of *block*, `r apa(ANOVA_UNM08_stage1, effect = "block")`, `r report_BF_and_error(bay_ANOVA_UNM08_stage1[1], sci_not = TRUE)`. There was no effect of *group*, *F* $\le$ 1, `r report_BF_and_error(bay_ANOVA_UNM08_stage1[2])`, and no interaction effect, `r apa(ANOVA_UNM08_stage1, effect = "condition:block")`, `r report_BF_and_error(bay_ANOVA_UNM08_stage1_int[1])`.

```{r, include=FALSE}
#ANOVA
UNM08_acc <- filter(UNM08_training, condition == "Certain Long" | condition == "Unexpected Uncertain") %>%
  group_by (pNum, block, condition) %>%
  summarise(mean_response = mean(prob_response, na.rm = TRUE))
UNM08_acc$block <- factor(UNM08_acc$block)
UNM08_acc$pNum <- factor(UNM08_acc$pNum)
UNM08_acc$condition <- factor(UNM08_acc$condition)
ANOVA_UNM08_acc <- aov_car(formula = mean_response ~ condition + Error(pNum/block), data = UNM08_acc)
print(ANOVA_UNM08_acc)
#Bayesian Anova
bay_ANOVA_UNM08_acc <- anovaBF(formula = mean_response ~ condition + block + pNum,
        data = data.frame(UNM08_acc),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM08_acc) 
bay_ANOVA_UNM08_acc_int <- bay_ANOVA_UNM08_acc[4]/bay_ANOVA_UNM08_acc[3]
print(bay_ANOVA_UNM08_acc_int)
```
```{r, include = FALSE}
# SME of the condition:block interaction
SME_acc_UNM08_training <- filter(UNM08_training, condition == "Certain Long" | condition == "Unexpected Uncertain") %>%
  group_by(pNum, condition, block) %>%
  summarise(mean_response = mean(prob_response, na.rm = TRUE))
#calculate the simple main effect of condition
sme_acc_UNM08_training_condition <- SME_acc_UNM08_training %>%
  group_by(block) %>%
  anova_test(mean_response ~ condition, effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_acc_UNM08_training_condition #Call the output table
#calculate the simple main effect of block
sme_acc_UNM08_training_pred <- SME_acc_UNM08_training %>%
  group_by(condition) %>%
  anova_test(mean_response ~ block + Error(pNum/block), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_acc_UNM08_training_pred #Call the output table

# Conditions you want to test
blocks <- 1:10
# Store results in a list
bay_SME_acc_UNM08_training <- lapply(blocks, function(b) {
    df_block <- UNM08_training %>%
    filter(block == b) %>%
    group_by(pNum, condition) %>%
    summarise(mean_response = mean(prob_response, na.rm = TRUE),
              .groups = "drop")
    df_block$condition <- factor(df_block$condition)
    res <- anovaBF(
    formula = mean_response ~ condition,
    data = data.frame(df_block),
    iterations = bfit
  )
    return(res)
})
names(bay_SME_acc_UNM08_training) <- paste0("block_", blocks)
# Create summary table with Bayes Factors
bay_summary_SME_acc_UNM08_training <- lapply(names(bay_SME_acc_UNM08_training), function(nm) {
  df <- as.data.frame(bay_SME_acc_UNM08_training[[nm]])
  df$block <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_acc_UNM08_training <- bay_summary_SME_acc_UNM08_training %>%
  select(block, everything())
# Show table
print(bay_summary_SME_acc_UNM08_training)
```

The data from Stage 1 and 2 were analysed with a mixed model ANOVA (using the Greenhouse-Geisser correction when needed), with the between-subjects factor of *group* (Certain Long vs Unexpected Uncertain) and the within-subjects factor of *block* (1-10). There was no effect of *group*, `r apa(ANOVA_UNM08_acc, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM08_acc[2])`, but there was a significant effect of *block*, `r apa(ANOVA_UNM07_acc, effect = "block")`, `r report_BF_and_error(bay_ANOVA_UNM08_acc[1], sci_not = TRUE)`, and a significant *group x block* interaction, `r apa(ANOVA_UNM08_acc, effect = "condition:block")`, `r report_BF_and_error(bay_ANOVA_UNM08_acc_int[1])`. Simple main effects showed a significant effect of condition for blocks 7 to 10, *F*(`r sme_acc_UNM08_training_condition[7, 3]`, `r sme_acc_UNM08_training_condition[7, 4]`) $\ge$ `r sme_acc_UNM08_training_condition[7, 5]`, *p* $\le$ .001, `r report_BF_and_error(bay_summary_SME_acc_UNM08_training[["block_7"]])`, but not for blocks 1 to 6, *F*(`r sme_acc_UNM08_training_condition[4, 3]`, `r sme_acc_UNM08_training_condition[4, 4]`) $\le$ `r sme_acc_UNM08_training_condition[4, 5]`, *p* $\ge$ `r sme_acc_UNM08_training_condition[4, 9]`, `r report_BF_and_error(bay_summary_SME_acc_UNM08_training[["block_3"]])`.

Taken together, these results indicate that PPR increased in the same manner for all groups in Stage 1 , while in Stage 2, the Certain Long group showed a consistently higher PPR than the Unexpected Uncertain group.

```{r, include=FALSE}
#calculate the mean accuracy by participant, group and predictiveness
acc_UNM08_test <- UNM08_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
#Calculate the mean accuracy and standard error for each block, including the groups
MA_UNM08_test <- acc_UNM08_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

@fig-acctestExp2 shows the accuracy results from the recognition memory test. Overall, accuracy was higher in group Unexpected Uncertain compared with the two Certain groups. Accuracy for non-predictive cues was lower than for the predictive cues in both Certain Long and Certain Short groups, but this difference was attenuated in the Uncertain group.

```{r, echo=FALSE, message=FALSE}
#| label: fig-acctestExp2
#| fig-cap: Accuracy on the test phase of Experiment 2.
#| apa-note: "Mean accuracy (±SEM) during the test phase of Experiment 2, across the three groups."
#| fig-height: 4
ggplot(data = MA_UNM08_test, mapping = aes(x =  factor(condition, level=c('Certain Long','Certain Short', 'Unexpected Uncertain')), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA accuracy
acc_UNM08_test$predictiveness <- factor(acc_UNM08_test$predictiveness)
acc_UNM08_test$condition <- factor(acc_UNM08_test$condition)
acc_UNM08_test$pNum <- factor(acc_UNM08_test$pNum)
ANOVA_acc_UNM08_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness), data = acc_UNM08_test)
print(ANOVA_acc_UNM08_test)

bay_ANOVA_acc_UNM08_test <- anovaBF(formula = acc ~ condition*predictiveness + pNum,
        data = data.frame(acc_UNM08_test),
        whichRandom = "pNum", 
        iterations = bfit)
print(bay_ANOVA_acc_UNM08_test)

bay_ANOVA_acc_UNM08_test_gxp <- bay_ANOVA_acc_UNM08_test[4]/bay_ANOVA_acc_UNM08_test[3]
print(bay_ANOVA_acc_UNM08_test_gxp)
```

```{r, include = FALSE}
# Pairwise comparisons for the main effect of condition
acc_UNM08_test_interaction <- emmeans(ANOVA_acc_UNM08_test, ~condition)
contrast(acc_UNM08_test_interaction, adjust = "bon", "trt.vs.ctrl", ref = c(1,2))

acc_UNM08_test_certs <- subset(acc_UNM08_test, (condition == "Certain Long") | (condition == "Certain Short"), acc, drop = TRUE)
acc_UNM08_test_uncert <- subset(acc_UNM08_test, condition == "Unexpected Uncertain", acc, drop = TRUE)
bay_t.test_acc_UNM08_int_uncer_vs_certs <-  ttestBF(acc_UNM08_test_certs, acc_UNM08_test_uncert)
print(bay_t.test_acc_UNM08_int_uncer_vs_certs)

pairs(acc_UNM08_test_interaction, adjust = "bon")

acc_UNM08_test_cert_l <- subset(acc_UNM08_test, condition == "Certain Long", acc, drop = TRUE)
acc_UNM08_test_cert_s <- subset(acc_UNM08_test, condition == "Certain Short", acc, drop = TRUE)
bay_t.test_acc_UNM08_test_certs <-  ttestBF(acc_UNM08_test_cert_l, acc_UNM08_test_cert_s)
print(bay_t.test_acc_UNM08_test_certs)
```

A mixed model ANOVA with the between-subjects factor Group (Certain Long, Certain Short, and Unexpected Uncertain) showed a significant main effect of *group*, `r apa(ANOVA_acc_UNM08_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_acc_UNM08_test[1])`, and a main effect of *predictiveness*: `r apa(ANOVA_acc_UNM08_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM08_test[2])`. The *group x predictiveness* interaction was not significant, `r apa(ANOVA_acc_UNM08_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM08_test_gxp[1])`. These results indicate that recognition accuracy was higher for predictive cues compared to non-predictive cues and that overall accuracy was higher in group Unexpected Uncertain compared to the other two groups. This interpretation of the main effect of group was confirmed by Bonferroni corrected pairwise comparisons, which revealed a significant difference between the overall accuracy (average of P and NP cues) in group Unexpected Uncertain compared to the overall accuracy in groups Certain Long and Certain Short, *t*(133) = 3.449, *p* $\le$ .001, `r report_BF_and_error(bay_t.test_acc_UNM08_int_uncer_vs_certs[1])`. There was evidence to suggest that overall accuracy for group Certain Long and group Certain Short was equivalent, *t* $\le$ 1, `r report_BF_and_error(bay_t.test_acc_UNM08_test_certs[1])`.

```{r, include = FALSE}
#create the memory_score
UNM08_test <- UNM08_test %>%
  mutate (c_mem_score = case_when(acc == 0 ~ 0, acc == 1 ~ mem_score))
#calculate the mean memory score by participant, group and predictiveness
UNM08_memscore_test <- UNM08_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
#Calculate the mean memory score and standard error for each block, including the groups
UNM08_MS_test <- UNM08_memscore_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

@fig-testExp2 shows the recognition memory scores for the three conditions. Memory for non-predictive cues was lower than for predictive cues in all groups, but this difference was notably attenuated in the Unexpected Uncertain group. Mirroring the accuracy data, the memory scores for the cues in group Unexpected Uncertain were on average higher, than those for groups Certain Long and Certain Short.

```{r, echo = FALSE, message=FALSE}
#| label: fig-testExp2
#| fig-cap: Memory scores on the Test of Experiment 2.
#| apa-note: "Mean memory scores (±SEM) during the Test of Experiment 2 for predictive and non-predictive trials across the three groups."
#| fig-height: 4
ggplot(UNM08_MS_test, mapping = aes(x = factor(condition, level=c('Certain Short', 'Certain Long', "Unexpected Uncertain")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  scale_y_continuous(name = "Memory score")+
  #scale_fill_grey(start = 0.33) +
  theme_apa()
```

```{r, include=FALSE}
#ANOVA mem_score
UNM08_memscore_test$predictiveness <- factor(UNM08_memscore_test$predictiveness)
UNM08_memscore_test$condition <- factor(UNM08_memscore_test$condition)
UNM08_memscore_test$pNum <- factor(UNM08_memscore_test$pNum)
ANOVA_UNM08_test <- aov_car(formula = mem_score ~ condition + Error(pNum*predictiveness), data = UNM08_memscore_test)
print(ANOVA_UNM08_test)
bay_ANOVA_UNM08_test <- anovaBF(formula = mem_score ~ condition + predictiveness ,
        data = data.frame(UNM08_memscore_test),
        whichRandom = "pNum", 
        iterations = bfit)
print(bay_ANOVA_UNM08_test)
bay_ANOVA_UNM08_test_gxp <- bay_ANOVA_UNM08_test[4]/bay_ANOVA_UNM08_test[3]
print(bay_ANOVA_UNM08_test_gxp)
```
```{r, include = FALSE}
# Pairwise comparisons for the main effect of condition
UNM08_test_interaction <- emmeans(ANOVA_UNM08_test, ~condition)
contrast(UNM08_test_interaction, adjust = "bon", "trt.vs.ctrl", ref = c(1,2))

UNM08_test_certs <- subset(UNM08_memscore_test, (condition == "Certain Long") | (condition == "Certain Short"), mem_score, drop = TRUE)
UNM08_test_uncert <- subset(UNM08_memscore_test, condition == "Unexpected Uncertain", mem_score, drop = TRUE)
bay_t.test_UNM08_int_uncer_vs_certs <-  ttestBF(UNM08_test_certs, UNM08_test_uncert)
print(bay_t.test_UNM08_int_uncer_vs_certs)

pairs(UNM08_test_interaction, adjust = "bon")

UNM08_test_cert <- subset(UNM08_memscore_test, condition == "Certain Long", mem_score, drop = TRUE)
UNM08_test_cert_s <- subset(UNM08_memscore_test, condition == "Certain Short", mem_score, drop = TRUE)
bay_t.test_UNM08_test_certs <-  ttestBF(UNM08_test_cert, UNM08_test_cert_s)
print(bay_t.test_UNM08_test_certs)
```
```{r, include = FALSE}
# SME of the condition:predictiveness interaction
SME_UNM08_test <- UNM08_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
#calculate the simple main effect of predictiveness
sme_UNM08_test_pred <- SME_UNM08_test %>%
  group_by(condition) %>%
  anova_test(mem_score ~ predictiveness + Error(pNum/predictiveness), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_UNM08_test_pred #Call the output table

# Conditions you want to test
conditions <- c("Certain Long", "Certain Short", "Unexpected Uncertain")
# Run the Bayesian ANOVA for each condition
bay_SME_UNM08_test <- lapply(conditions, function(cond) {
    df_cond <- UNM08_test %>%
    filter(condition == cond) %>%
    group_by(pNum, predictiveness) %>%
    summarise(mem_score = mean(c_mem_score, na.rm = TRUE),
              .groups = "drop")
    df_cond$predictiveness <- factor(df_cond$predictiveness)
  df_cond$pNum <- factor(df_cond$pNum)
    res <- anovaBF(
    formula = mem_score ~ predictiveness + pNum,
    data = data.frame(df_cond),
    whichRandom = "pNum",
    iterations = bfit
  )
    return(res)
})
names(bay_SME_UNM08_test) <- conditions
# Create summary table
bay_summary_SME_UNM08_test <- lapply(names(bay_SME_UNM08_test), function(nm) {
  df <- as.data.frame(bay_SME_UNM08_test[[nm]])
  df$condition <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_UNM08_test <- bay_summary_SME_UNM08_test %>%
  select(condition, everything())
# Show table
print(bay_summary_SME_UNM08_test)
```
```{r, include = FALSE}
#calculate the simple main effect of condition
sme_UNM08_test_condition <- SME_UNM08_test %>%
  group_by(predictiveness) %>%
  anova_test(mem_score ~ condition, effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_UNM08_test_condition #Call the output table

# Conditions you want to test
preds <- c("predictive", "non-predictive")
# Run the Bayesian ANOVA for each condition
bay_SME_UNM08_test_pred <- lapply(preds, function(p) {
    df_pred <- UNM08_test %>%
    filter(predictiveness == p) %>%
    group_by(pNum, condition) %>%
    summarise(mem_score = mean(c_mem_score, na.rm = TRUE),
              .groups = "drop")
    df_pred$condition <- factor(df_pred$condition)
  df_pred$pNum <- factor(df_pred$pNum)
    res <- anovaBF(
    formula = mem_score ~ condition,
    data = data.frame(df_pred),
    whichRandom = "pNum",
    iterations = bfit
  )
    return(res)
})
names(bay_SME_UNM08_test_pred) <- preds
# Create summary table
bay_summary_SME_UNM08_test_pred <- lapply(names(bay_SME_UNM08_test_pred), function(nm) {
  df <- as.data.frame(bay_SME_UNM08_test_pred[[nm]])
  df$predictiveness <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_UNM08_test_pred <- bay_summary_SME_UNM08_test_pred %>%
  select(predictiveness, everything())
# Show table
print(bay_summary_SME_UNM08_test_pred)
```

A mixed model ANOVA, including the between-subjects factor *group* (Certain Long, Certain Short, Unexpected Uncertain), and the within-subjects factor *predictiveness* (predictive vs non-predictive) showed significant main effects of *group*, `r apa(ANOVA_UNM08_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM08_test[1])`, and *predictiveness*, `r apa(ANOVA_UNM08_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM08_test[2])`, and a significant *group x predictiveness* interaction, `r apa(ANOVA_UNM08_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM08_test_gxp[1])`. However, is worth noting that the Bayesian analysis indicated moderate evidence in favour of the null hypothesis for this interaction. Bonferroni corrected pairwise comparisons on the main effect of *group* showed that group Unexpected Uncertain differed significantly from the average of the Certain groups, *t*(133) = 2.624, *p* = .01, `r report_BF_and_error(bay_t.test_UNM08_int_uncer_vs_certs[1])`, but that overall memory scores for the two Certain groups were equivalent, *t* $\le$ 1, `r report_BF_and_error(bay_t.test_UNM08_test_certs[1])`. Furthermore, simple main effects showed a significant effect of *predictiveness* for group Certain Long, *F* (`r sme_UNM08_test_pred[1, 3]`, `r sme_UNM08_test_pred[1, 4]`) = `r sme_UNM08_test_pred[1, 5]`, *p* $\le$ 0.001, *η~p~^2^* = `r sme_UNM08_test_pred[1, 8]`, `r report_BF_and_error(bay_SME_UNM08_test[["Certain Long"]])`, and for group Certain Short, *F* (`r sme_UNM08_test_pred[2, 3]`, `r sme_UNM08_test_pred[2, 4]`) = `r sme_UNM08_test_pred[2, 5]`, *p* $\le$ 0.001, *η~p~^2^* = `r sme_UNM08_test_pred[2, 8]`, `r report_BF_and_error(bay_SME_UNM08_test[["Certain Short"]])`, but not for group Unexpected Uncertain, *F* (`r sme_UNM08_test_pred[3, 3]`, `r sme_UNM08_test_pred[3, 4]`) = `r sme_UNM08_test_pred[3, 5]`, *p* = `r sme_UNM08_test_pred[3, 9]`, *η~p~^2^* = `r sme_UNM08_test_pred[3, 8]`, `r report_BF_and_error(bay_SME_UNM08_test[["Unexpected Uncertain"]])`. It is also important to note that when the simple main effect of *group* was analysed, this was significant for the non-predictive cues, *F* (`r sme_UNM08_test_condition[1, 3]`, `r sme_UNM08_test_condition[1, 4]`) = `r sme_UNM08_test_condition[1, 5]`, *p* = `r sme_UNM08_test_condition[1, 9]`, *η~p~^2^* = `r sme_UNM08_test_condition[1, 8]`, `r report_BF_and_error(bay_SME_UNM08_test_pred[["non-predictive"]])`, but not for the predictive cues, *F* $\le$ 1, `r report_BF_and_error(bay_SME_UNM08_test_pred[["predictive"]])`.

## Discussion

Experiment 2 examined the effect of unexpected uncertainty on recognition memory. The Unexpected Uncertain group of participants first experienced a period of training with certain contingencies, before receiving a second period with uncertain contingencies. Participants that were exposed to this unexpected uncertainty showed a higher level of recognition memory for the cues than participants that received only certain training. Furthermore, group Unexpected Uncertain did not show an effect of predictiveness, and the better overall memory seems to be driven by better memory for the non-predictive cues. An important difference between Experiment 1 and 2 is that in Experiment 1, the Certain and Uncertain groups had a similar recognition memory for the cues, whereas in the current experiment, the Unexpected Uncertain group showed better cue-recognition. We interpret this difference to be a consequence of the expectancy of uncertainty: in Experiment 2, but not Experiment 1, uncertainty is suddenly introduced after a sustained period of certain
training.

These results suggest that introducing a period of unexpected uncertainty results in enhanced cue processing, consistent with previous results [@easdaleOnsetUncertaintyFacilitates2019] that showed that unexpected uncertainty enhances learning. Easdale et al. used a training phase in which participants learnt about either certain or uncertain contingencies. Participants showed better attention to cues under uncertain conditions. However, when those cues were subsequently trained under new contingencies, it was participants in the certain condition that learnt about these more rapidly, compared to those participants in the uncertain condition. Easdale et al. suggested that the transition from certain to uncertain contingencies brought about a state of “unexpected uncertainty” which promoted new learning. Experiment 2 shows more directly that a period of unexpected uncertainty leads to superior cue processing and stronger memory representations.Yet, Easdale et al. results are based on the comparison of a group that transitioned from certain to uncertain contingencies, and a group that experienced uncertain contingencies throughout the experiment. Experiment 3 served as a replication of the present results, but included an Uncertain group that received uncertain contingencies throughout the training phase, allowing for a more direct comparison with the results reported by Easdale et al.

# Experiment 3

Experiment 3 aimed to directly compare the effects of expected and unexpected uncertainty. The experiment consistent on three groups. Group Certain was identical to group Certain Long from Experiment 2, being trained with certain cue-outcome contingencies throughout the ten blocks of the training phase. Group Uncertain was similar to group Uncertain in Experiment 1, experiencing uncertain contingencies throughout the training phase. Group Unexpected Uncertain was identical to group Uncertain in Experiment 2, experiencing six blocks of certain training, followed by four block of uncertain training. The design can be seen in @tbl-exp3. Based on the results of previous experiments, we expect that participants in the Unexpected Uncertain group will show a better overall memory for the cues than participants in the Certain and Uncertain groups. We also expect that group Certain will show a predictiveness effect, showing better memory for predictive than non-predictive cues.

::: {#tbl-exp3 apa-note="Uppercase letters A, B, X, and Y represent the cues presented during training. O1 and O2 represent the outcomes presented in training. Lowercase letters a, b, x, and y represent the foils that are similar to the (corresponding upper-case letter) cues presented in the training phase. The numbers before the trials define the proportion of trials of that type that were presented in the uncertain condition." apa-twocolumn="true"}
+---------------+---------------------------+---------------------------+------------------+
| Group         | Stage 1                   | Stage 2                   | Test             |
+===============+===========================+:=========================:+:================:+
| Certain       | AX - O1                   | AX - O1                   | A vs *b*/*x*/*y* |
|               |                           |                           |                  |
|               | AY - O1                   | AY - O1                   | B vs *a*/*x*/*y* |
|               |                           |                           |                  |
|               | BX - O2                   | BX - O2                   | X vs *a*/*b*/*y* |
|               |                           |                           |                  |
|               | BY - O2                   | BY - O2                   | Y vs *a*/*b*/*x* |
+---------------+---------------------------+---------------------------+------------------+
| Unexpected    | AX - O1                   | 0.8 AX - O1 / 0.2 AX - O2 | A vs *b*/*x*/*y* |
|               |                           |                           |                  |
| Uncertain     | AY - O1                   | 0.8 AY - O1 / 0.2 AY - O2 | B vs *a*/*x*/*y* |
|               |                           |                           |                  |
|               | BX - O2                   | 0.8 BX - O2 / 0.2 BX - O1 | X vs *a*/*b*/*y* |
|               |                           |                           |                  |
|               | BY - O2                   | 0.8 BY - O2 / 0.2 BY - O1 | Y vs *a*/*b*/*x* |
+---------------+---------------------------+---------------------------+------------------+
| Uncertain     | 0.8 AX - O1 / 0.2 AX - O2 | 0.8 AX - O1 / 0.2 AX - O2 | A vs *b*/*x*/*y* |
|               |                           |                           |                  |
|               | 0.8 AY - O1 / 0.2 AY - O2 | 0.8 AY - O1 / 0.2 AY - O2 | B vs *a*/*x*/*y* |
|               |                           |                           |                  |
|               | 0.8 BX - O1 / 0.2 BX - O2 | 0.8 BX - O1 / 0.2 BX - O2 | X vs *a*/*b*/*y* |
|               |                           |                           |                  |
|               | 0.8 BY - O1 / 0.2 BY - O2 | 0.8 BY - O1 / 0.2 BY - O2 | Y vs *a*/*b*/*x* |
+---------------+---------------------------+---------------------------+------------------+

Design of Experiment 3
:::

## Methods

### Transparency and openness statement

Experiment 3 was registered prior to the creation of the data. This registration can be found at https://doi.org/10.17605/OSF.IO/MF52K.


### Participants

```{r, include=FALSE}
#load the data
load("UNM10_proc_data.RData")
UNM10_demographics <- demographics
UNM10_training <- training
UNM10_test <- test
UNM10_not_passed <- notpassed_pNum
UNM10_training <- filter(UNM10_training, !pNum %in% UNM10_not_passed$pNum)
UNM10_test <- filter(UNM10_test, !pNum %in% UNM10_not_passed$pNum)
UNM10_all_p <- nrow(UNM10_demographics)
UNM10_demographics <- filter(UNM10_demographics, !pNum %in% UNM10_not_passed$pNum)
```
```{r, include = FALSE}
#create the PPR measure
UNM10_training <- UNM10_training %>%
  mutate(prob_response = case_when(cue1 == "A" & response == "o1_image" ~ 1,
                                   cue1 == "A" & response == "o2_image" ~ 0,
                                   cue1 == "B" & response == "o1_image" ~ 0,
                                   cue1 == "B" & response == "o2_image" ~ 1))
```

`r UNM10_all_p` participants were recruited through Prolific, but `r nrow(UNM10_not_passed)` where excluded due to failing the comprehension check prior to the test phase. Of the remaining `r nrow(UNM10_demographics)`, three participants did not report their age or gender, and six participants did not report their nationality. From the data available, the mean age was `r format(mean(UNM10_demographics$age, na.rm = TRUE), digits = 3)` (range `r min(UNM10_demographics$age, na.rm = TRUE)` - `r max(UNM10_demographics$age, na.rm = TRUE)`), with `r length(which(UNM10_demographics$gender == "female"))` women, `r length(which(UNM10_demographics$gender == "male"))` men, and one non-binary person, and `r n_distinct(UNM10_demographics$Nationality)` different nationalities. Participants were randomly allocated to each condition. The final sample was determined a priori based on the previous experiments. As our main interest was to have enough sample to detect the effect of group on the mixed model ANOVA with the within factor predictiveness and the between factor group, and previous effect size was ηp2 = .05. A priori calculations for this analysis showed that, to achieve a power of β = .85 with an effect size of ηp2 = .05 and a significance level of α = .05 for the between-subjects factor group, a sample of 159 participants was needed. It is worth noting that we expected a directionality of the effect here, as we expected group Unexpected Uncertain to have higher mean than the Certain and Uncertain groups, which led to the use of a one-tailed test and thus adopting a significance level of α = .1. With this significance level, as well as a sample size of 159 and an effect size of *η~p~^2^* = .05, the estimated power that would be achieved was of β = .913.

### Apparatus and stimuli

The materials were the same as in Experiments 1 and 2.

### Design

The experiment used a mixed design (as seen in @tbl-exp3), with three groups: Certain, Unexpected Uncertain, and Uncertain. All groups received 10 blocks of training. This training was divided in two stages: Stage 1 comprised the first 6 blocks and Stage 2, the last 4 blocks. For groups Certain and Uncertain, training in Stages 1 and 2 was identical: group Certain received training with certain contingencies throughout the training phase, whereas group Uncertain received uncertain contingencies. For group Unexpected Uncertain, the training in Stage 1 was with certain contingencies, whereas the training in Stage 2 was with uncertain contingencies. When training was completed, all groups progressed to the memory test, which was identical to that used in Experiments 1 and 2.

### Procedure

The procedure was identical to Experiments 1 and 2.

## Results

```{r, include = FALSE}
#Calculate the mean PPR and standard error for each block and each participant, including the groups and stages
UNM10_acc_training <- UNM10_training %>%
  group_by(pNum, block, stage, condition) %>%
  summarise(PPR = mean(prob_response, na.rm = TRUE))

#calcualte mean and se per groups
UNM10_MA_training <- UNM10_acc_training %>%
  group_by(block, stage, condition) %>%
  summarise(mean_PPR = mean(PPR, na.rm = TRUE), 
            se_PPR = sd(PPR, na.rm = TRUE)/sqrt(length(PPR)))
```

@fig-trainingExp3 shows the mean PPR for each group across the ten blocks of training. Participants in group Certain and Unexpected Uncertain showed a similar increase in PPR, that was consistently higher than the one of group Uncertain throughout Stage 1. Groups Certain and Uncertain maintained a similar level of PPR in Stage 2 to the one they showed, respectively, in Stage 1. However, group Unexpected Uncertain showed a drop in PPR in Stage 2, with levels between the PPR in Certain and Uncertain groups.

```{r, echo = FALSE, message=FALSE, warning=FALSE}
#| label: fig-trainingExp3
#| fig-cap: PPR on the training phase of Experiment 3.
#| apa-note: "Mean proportion of probable responses (±SEM) during the training phase of Experiment 3, plotted against the ten blocks of trials, for each Group."
#| fig-height: 4
ggplot(UNM10_MA_training, mapping = aes(x = block, y = mean_PPR, group = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")))) +
  geom_point(mapping = aes(shape = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")), color = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain"))), size = 2.5) +
  geom_line(mapping = aes(color = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")))) +
  geom_errorbar(aes(x= block, y = mean_PPR, ymin = mean_PPR-se_PPR, ymax = mean_PPR+se_PPR), colour = "black", width=.1)+
  facet_grid(cols = vars(stage), space = "free_x", scales = "free_x") + 
  #scale_x_continuous(name = "Block", breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) + 
  scale_color_discrete(type = c("#AF8DC3", "#FEB24C", "#7FBF7B"))+
  labs(shape = "Group", color = "Group") +
  scale_y_continuous(name = "PPR", limits = c(0.5, 1))+
  theme_apa()
```
```{r, include=FALSE}
#ANOVA
UNM10_acc_training$stage <- factor(UNM10_acc_training$stage)
UNM10_acc_training$block <- factor(UNM10_acc_training$block)
UNM10_acc_training$pNum <- factor(UNM10_acc_training$pNum)
UNM10_acc_training$condition <- factor(UNM10_acc_training$condition)
ANOVA_UNM10_train <- aov_car(formula = PPR ~ condition + Error(pNum/block), data = UNM10_acc_training)
print(ANOVA_UNM10_train)
#Bayesian Anova
bay_ANOVA_UNM10_train <- anovaBF(formula = PPR ~ condition + block + pNum,
        data = data.frame(UNM10_acc_training),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM10_train)
bay_ANOVA_UNM10_train_int <- bay_ANOVA_UNM10_train[4]/bay_ANOVA_UNM10_train[3]
print(bay_ANOVA_UNM10_train_int)
```
```{r, include = FALSE}
# SME of the condition:block interaction
SME_UNM10_train <- UNM10_training %>%
  group_by(pNum, condition, block) %>%
  summarise(mean_response = mean(prob_response, na.rm = TRUE))
#calculate the simple main effect of condition
sme_UNM10_training_condition <- SME_UNM10_train %>%
  group_by(block) %>%
  anova_test(mean_response ~ condition, effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_UNM10_training_condition #Call the output table
#calculate the simple main effect of block
sme_UNM10_training_block <- SME_UNM10_train %>%
  group_by(condition) %>%
  anova_test(mean_response ~ block + Error(pNum/block), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_UNM10_training_block #Call the output table

blocks <- 1:10
# Store results in a list
bay_SME_UNM10_train <- lapply(blocks, function(b) {
  df_block <- UNM10_training %>%
    filter(block == b) %>%
    group_by(pNum, condition) %>%
    summarise(mean_response = mean(prob_response, na.rm = TRUE),
              .groups = "drop")
  df_block$condition <- factor(df_block$condition)
  res <- anovaBF(
    formula = mean_response ~ condition,
    data = data.frame(df_block),
    iterations = bfit
  )
  return(res)
})
names(bay_SME_UNM10_train) <- paste0("block_", blocks)
# Create summary table with Bayes Factors
bay_summary_SME_UNM10_train <- lapply(names(bay_SME_UNM10_train), function(nm) {
  df <- as.data.frame(bay_SME_UNM10_train[[nm]])
  df$block <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_UNM10_train <- bay_summary_SME_UNM10_train %>%
  select(block, everything())
# Show table
print(bay_summary_SME_UNM10_train)
```
```{r, include=FALSE}
#Simple comparisons for block
sc_UNM10_train <- SME_UNM10_train %>%
  group_by(block) %>%
  pairwise_t_test(mean_response ~ condition,
                  paired = FALSE,
                  p.adjust.method = "bonferroni")
#Call the summary
sc_UNM10_train

# Define blocks and conditions
blocks <- 1:10
conditions <- c("Certain", "Unexpected Uncertain", "Uncertain")
# Store raw ttestBF objects in a nested list: block -> comparison
bay_sc_UNM10_train <- lapply(blocks, function(b) {
  df_block <- SME_UNM10_train %>%
    filter(block == b) %>%
    group_by(pNum, condition) %>%
    summarise(mean_response = mean(mean_response, na.rm = TRUE), .groups = "drop")
  df_block$condition <- factor(df_block$condition, levels = conditions)
# Generate all pairwise comparisons
  cond_pairs <- combn(conditions, 2, simplify = FALSE)
# Compute ttestBF for each pair and store the raw object
  pair_results <- lapply(cond_pairs, function(pair) {
    data1 <- df_block %>% filter(condition == pair[1]) %>% pull(mean_response)
    data2 <- df_block %>% filter(condition == pair[2]) %>% pull(mean_response)
    
    ttestBF(data1, data2)  # return the raw BayesFactor object
  })
# Name the list elements with descriptive pair names
  names(pair_results) <- sapply(cond_pairs, function(x) paste(x, collapse = "_vs_"))
  return(pair_results)
})
# Name outer list by block
names(bay_sc_UNM10_train) <- paste0("block_", blocks)
# Example: access the raw ttestBF object for block 1, Certain vs Unexpected Uncertain
bay_sc_UNM10_train$block_1$Certain_vs_Unexpected_Uncertain
# Later, if you want a summary table of BF10 and error, you can do:
bay_summary_sc_UNM10_train <- lapply(names(bay_sc_UNM10_train), function(b) {
  lapply(names(bay_sc_UNM10_train[[b]]), function(pair) {
    bf_obj <- bay_sc_UNM10_train[[b]][[pair]]
    bf_info <- extractBF(bf_obj)
    tibble(
      block = b,
      comparison = pair,
      BF10 = as.numeric(bf_info$bf),
      error = as.numeric(bf_info$error)
    )
  }) %>% bind_rows()
}) %>% bind_rows()
bay_summary_sc_UNM10_train
```

Training phase data were analysed with a mixed-model ANOVA, with the between-subjects factor of *group* (Certain, Unexpected Uncertain, and Uncertain), and the within-subjects factor of *block* (1-10). This revealed a significant effect of *block*, `r apa(ANOVA_UNM10_train, effect = "block")`, `r report_BF_and_error(bay_ANOVA_UNM10_train[1], sci_not = TRUE)`, of *group*, `r apa(ANOVA_UNM10_train, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM10_train[2])`, and of the *group x block* interaction, `r apa(ANOVA_UNM10_train, effect = "condition:block")`, `r report_BF_and_error(bay_ANOVA_UNM10_train_int[1])`. Simple main effects showed a significant effect of condition in all blocks, *F*(`r sme_acc_UNM10_training_condition[7, 3]`, `r sme_UNM10_training_condition[8, 4]`) > `r sme_UNM10_training_condition[8, 5]`, *p* < .001, `r report_BF_and_error(bay_SME_UNM10_train[["block_2]])`. Bonferroni corrected pairwise comparisons showed that there was a significant difference between groups Certain and Uncertain in all blocks, *p* < `r sc_UNM10_train[4, 9]`, `r report_BF_and_error(bay_sc_UNM10_train[[1]][[2]])`, between groups Unexpected Uncertain and Uncertain in blocks 1 to 7, *p* < `r sc_UNM10_train[21, 9]`, `r report_BF_and_error(bay_sc_UNM10_train[[3]][[3]])`, and between Certain and Unexpected Uncertain in block 9, *p* = `r sc_UNM10_train[26, 9]`, `r report_BF_and_error(bay_sc_UNM10_train[[9]][[1]])`. The rest of the comparisons were not significant in any of the blocks, *p* > `r sc_UNM10_train[23, 9]`, `r report_BF_and_error(bay_sc_UNM10_train[[8]][[1]])`.

```{r, include=FALSE}
#calculate the mean accuracy for each participant in each group and predictiveness
acc_UNM10_test <- UNM10_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
#Calculate the mean accuracy and standard error in each group and predictiveness
MA_UNM10_test <- acc_UNM10_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

@fig-acctestExp3 shows the accuracy results from the recognition memory test. Groups Certain and Unexpected Uncertain both showed higher responding for the predictive than the non-predictive cues, whereas this difference was not evident in group Uncertain. Uncertain group also showed a lower responding for the non-predictive cues than the other two groups. 

```{r, echo=FALSE, message=FALSE}
#| label: fig-acctestExp3
#| fig-cap: Accuracy on the test phase of Experiment 3.
#| apa-note: "Mean accuracy (±SEM) during the test phase of Experiment 3, across the three groups."
#| fig-height: 4
ggplot(data = MA_UNM10_test, mapping = aes(x = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  theme_apa()
```
```{r, include=FALSE}
#ANOVA accuracy
acc_UNM10_test$predictiveness <- factor(acc_UNM10_test$predictiveness)
acc_UNM10_test$condition <- factor(acc_UNM10_test$condition)
acc_UNM10_test$pNum <- factor(acc_UNM10_test$pNum)
ANOVA_acc_UNM10_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness), data = acc_UNM10_test)
print(ANOVA_acc_UNM10_test)

bay_ANOVA_acc_UNM10_test <- anovaBF(formula = acc ~ condition*predictiveness + pNum,
        data = data.frame(acc_UNM10_test),
        whichRandom = "pNum", 
        iterations = bfit)
print(bay_ANOVA_acc_UNM10_test)

bay_ANOVA_acc_UNM10_test_gxp <- bay_ANOVA_acc_UNM10_test[4]/bay_ANOVA_acc_UNM10_test[3]
print(bay_ANOVA_acc_UNM10_test_gxp)
```

A mixed model ANOVA with the between subjects factor *group* (Certain, Unexpected Uncertain, Uncertain) and the within subjects factor *predictiveness* (predictive, non-predictive) found significant only the effect of *predictiveness*, `r apa(ANOVA_acc_UNM10_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM10_test[2])`. he main effect of *group*, `r apa(ANOVA_acc_UNM10_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_acc_UNM10_test[1])`, and the *group x predictiveness* interaction were not significant, `r apa(ANOVA_acc_UNM10_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM10_test_gxp[1])`.

```{r, include = FALSE}
#create the memory_score
UNM10_test <- UNM10_test %>%
  mutate (c_mem_score = case_when(acc == 0 ~ 0, acc == 1 ~ mem_score))
#calculate mean memory score for each participant including group and predictiveness
UNM10_memscore_test <- UNM10_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
#Calculate the mean memory score and standard error for each group and predictivenes
UNM10_MS_test <- UNM10_memscore_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

@fig-testExp3 shows the recognition memory scores for the three conditions. Memory for non-predictive cues was lower than for predictive cues in all groups, but this difference was notably attenuated in the Uncertain group. Overall, the group with that displayed better memory was the Unexpected Uncertain group, followed by the Certain group, and last by the Uncertain group.

```{r, echo = FALSE, message=FALSE}
#| label: fig-testExp3
#| fig-cap: Memory scores on the Test of Experiment 3.
#| apa-note: "Mean memory scores (±SEM) during the Test of Experiment 3 for predictive and non-predictive trials across the three groups."
#| fig-height: 4
ggplot(UNM10_MS_test, mapping = aes(x = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  scale_y_continuous(name = "Memory score")+
  #scale_fill_grey(start = 0.33) +
  theme_apa()
```
```{r, include=FALSE}
#ANOVA mem_score
UNM10_memscore_test$predictiveness <- factor(UNM10_memscore_test$predictiveness)
UNM10_memscore_test$condition <- factor(UNM10_memscore_test$condition)
UNM10_memscore_test$pNum <- factor(UNM10_memscore_test$pNum)
ANOVA_UNM10_test <- aov_car(formula = mem_score ~ condition + Error(pNum*predictiveness), data = UNM10_memscore_test)
print(ANOVA_UNM10_test)
bay_ANOVA_UNM10_test <- anovaBF(formula = mem_score ~ condition + predictiveness ,
        data = data.frame(UNM10_memscore_test),
        whichRandom = "pNum", 
        iterations = bfit)
print(bay_ANOVA_UNM10_test)
bay_ANOVA_UNM10_test_gxp <- bay_ANOVA_UNM10_test[4]/bay_ANOVA_UNM10_test[3]
print(bay_ANOVA_UNM10_test_gxp)
```

A mixed model ANOVA, including the between-subjects factor *group* (Certain, Unexpected Uncertain, Uncertain), and the within-subjects factor *predictiveness* (predictive vs non-predictive) showed a significant main effect of *predictiveness*, `r apa(ANOVA_UNM10_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM10_test[2])`, but not of the *group*, `r apa(ANOVA_UNM10_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM10_test_gxp[1])`, nor the *group x predictiveness* interaction, `r apa(ANOVA_UNM10_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM10_test_gxp[1])`. 

## Discussion

Experiment 3 compared the recognition memory for cues trained under certain, unexpected uncertain and uncertain contingencies. Group Certain experienced a perfect relationship between predictive cues and their respective outcomes throughout training whereas group Uncertain experienced a imperfect predictive relationships. Group Unexpected Uncertain experienced training with perfectly predictive relationship followed by a period of imperfect predictive relationships. Contrary to the results of Experiment 2, the groups did not show any differences on the memory for the cues. It is worth noting that there was a significant effect of predictiveness, indicating that the memory for predictive cues was overall better than for non-predictive cues. Although this effect was present in Experiment 2, the analysis of the interaction showed that this predictiveness effect was modulated by the condition, being present on the Certain group but not on the Unexpected Uncertain group. Similarly, on Experiment 1, there was a significant difference in the memory between predictive and non-predictive cues in group Certain but not in group Uncertain. Thus, the results of Experiment 3 are inconsistent with both the results of Experiment 1 and 2. It's worth noting that the Bayesian evidence for the significant effect of predictiveness was anecdotal (BF~10~ < 3), as the evidence for the null effect of condition (BF~10~ > 0.33). 

Given these inconsistencies and the lack of robust Bayesian evidence, we chose to replicate the groups of Experiment 3 on Experiment 4. In the recognition memory test used in Experiments 1 to 3, participants were presented with foils that were similar to both predictive and non-predictive cues, independently of the predictiveness status of the target. This might be confounding the results: if a participant is faced with a non-predictive target and a predictive foil, the participant might be able to accurately respond not based on their memory of the non-predictive target but on their memory of the predictive cue in which the target is based, rejecting the foil based on this memory. Experiment 4 used six cues, three of them trained as predictive cues (A, B, C) and three of them as non-predictive (X, Y, Z). Crucially, the recognition memory test compared predictive target with foils based on predictive cues (e.g., A vs *b*/*c*) and non-predictive targets with foils based on non-predictive cues (e.g., X vs *y*/*z*). 

# Experiment 4

Experiment 4 aimed to replicate and extend the previous experiments. As can be seen in @tbl-exp4, experiment 4 included three groups: group Certain, in which participants experienced certain contingencies throughout training, group Uncertain, in which participants experienced uncertain contingencies throughout training, and group Unexpected Uncertain, in which uncertain contingencies were introduced after a period of training with certain contingencies. Also, the number of cues trained was increased from previous experiments, from four to six cues. The number of cues was increased so, in the memory test, predictive targets where presented with foils based on predictive cues and non-predictive targets were presented with foils based on non-predictive cues, and the number of trials on the memory test was the same as in previous experiments. 

::: {#tbl-exp4 apa-note="Uppercase letters A, B, X, and Y represent the cues presented during training. O1 and O2 represent the outcomes presented in training. Lowercase letters a, b, x, and y represent the foils that are similar to the (corresponding upper-case letter) cues presented in the training phase. The numbers before the trials define the proportion of trials of that type that were presented in the uncertain condition." apa-twocolumn="true"}
+------------+-----------------------------------------+-----------------------------------------+--------------+
| Group      | Stage 1                                 | Stage 2                                 | Test         |
+============+:=======================================:+:=======================================:+:============:+
| Certain    | AX - O1                                 | AX - O1                                 | A vs *b/c*   |
|            |                                         |                                         |              |
|            | AY - O1                                 | AY - O1                                 | B vs *a/c*   |
|            |                                         |                                         |              |
|            | AZ - O1                                 | AZ - O1                                 | C vs *a/b*   |
|            |                                         |                                         |              |
|            | BX - O2                                 | BX - O2                                 | X vs *y/z*   |
|            |                                         |                                         |              |
|            | BY - O2                                 | BY - O2                                 | Y vs *x/z*   |
|            |                                         |                                         |              |
|            | BZ - O2                                 | BZ - O2                                 | Z vs *x/y*   |
|            |                                         |                                         |              |
|            | CX -O3                                  | CX -O3                                  |              |
|            |                                         |                                         |              |
|            | CY - O3                                 | CY - O3                                 |              |
|            |                                         |                                         |              |
|            | CZ - O3                                 | CZ - O3                                 |              |
+------------+-----------------------------------------+-----------------------------------------+--------------+
| Unexpected | AX - O1                                 | 0.8 AX - O1 / 0.1 AX - O2 / 0.1 AX - O3 | A vs *b/c*   |
|            |                                         |                                         |              |
| Uncertain  | AY - O1                                 | 0.8 AY - O1 / 0.1 AY - O2 / 0.1 AY - O3 | B vs *a/c*   |
|            |                                         |                                         |              |
|            | AZ - O1                                 | 0.8 AZ - O1 / 0.1 AZ - O2 / 0.1 AZ - O3 | C vs *a/b*   |
|            |                                         |                                         |              |
|            | BX - O2                                 | 0.8 BX - O2 / 0.1 BX - O1 / 0.1 BX - O3 | X vs *y/z*   |
|            |                                         |                                         |              |
|            | BY - O2                                 | 0.8 BY - O2 / 0.1 BY - O1 / 0.1 BY - O3 | Y vs *x/z*   |
|            |                                         |                                         |              |
|            | BZ - O2                                 | 0.8 BZ - O2 / 0.1 BZ - O1 / 0.1 BZ - O3 | Z vs *x/y*   |
|            |                                         |                                         |              |
|            | CX -O3                                  | 0.8 CX - O3 / 0.1 CX - O1 / 0.1 CX - O2 |              |
|            |                                         |                                         |              |
|            | CY - O3                                 | 0.8 CY - O3 / 0.1 CY - O1 / 0.1 CY - O2 |              |
|            |                                         |                                         |              |
|            | CZ - O3                                 | 0.8 CZ - O3 / 0.1 CZ - O1 / 0.1 CZ - O2 |              |
+------------+-----------------------------------------+-----------------------------------------+--------------+
| Uncertain  | 0.8 AX - O1 / 0.1 AX - O2 / 0.1 AX - O3 | 0.8 AX - O1 / 0.1 AX - O2 / 0.1 AX - O3 | A vs *b/c*   |
|            |                                         |                                         |              |
|            | 0.8 AY - O1 / 0.1 AY - O2 / 0.1 AY - O3 | 0.8 AY - O1 / 0.1 AY - O2 / 0.1 AY - O3 | B vs *a/c*   |
|            |                                         |                                         |              |
|            | 0.8 AZ - O1 / 0.1 AZ - O2 / 0.1 AZ - O3 | 0.8 AZ - O1 / 0.1 AZ - O2 / 0.1 AZ - O3 | C vs *a/b*   |
|            |                                         |                                         |              |
|            | 0.8 BX - O2 / 0.1 BX - O1 / 0.1 BX - O3 | 0.8 BX - O2 / 0.1 BX - O1 / 0.1 BX - O3 | X vs *y/z*   |
|            |                                         |                                         |              |
|            | 0.8 BY - O2 / 0.1 BY - O1 / 0.1 BY - O3 | 0.8 BY - O2 / 0.1 BY - O1 / 0.1 BY - O3 | Y vs *x/z*   |
|            |                                         |                                         |              |
|            | 0.8 BZ - O2 / 0.1 BZ - O1 / 0.1 BZ - O3 | 0.8 BZ - O2 / 0.1 BZ - O1 / 0.1 BZ - O3 | Z vs *x/y*   |
|            |                                         |                                         |              |
|            | 0.8 CX - O3 / 0.1 CX - O1 / 0.1 CX - O2 | 0.8 CX - O3 / 0.1 CX - O1 / 0.1 CX - O2 |              |
|            |                                         |                                         |              |
|            | 0.8 CY - O3 / 0.1 CY - O1 / 0.1 CY - O2 | 0.8 CY - O3 / 0.1 CY - O1 / 0.1 CY - O2 |              |
|            |                                         |                                         |              |
|            | 0.8 CZ - O3 / 0.1 CZ - O1 / 0.1 CZ - O2 | 0.8 CZ - O3 / 0.1 CZ - O1 / 0.1 CZ - O2 |              |
+------------+-----------------------------------------+-----------------------------------------+--------------+

Design of Experiment 4
:::

## Methods

### Transparency and openness statement

Experiment 4 was registered prior to the creation of the data. This registration can be found at 
https://doi.org/10.17605/OSF.IO/VCZ4X.

### Participants

```{r, include=FALSE}
#load the data
load("UNM11_proc_data.RData")
UNM11_demographics <- demographics
UNM11_training <- training
UNM11_test <- test
UNM11_not_passed <- notpassed_pNum
UNM11_training <- filter(UNM11_training, !pNum %in% UNM11_not_passed$pNum)
UNM11_test <- filter(UNM11_test, !pNum %in% UNM11_not_passed$pNum)
UNM11_all_p <- nrow(UNM11_demographics)
UNM11_demographics <- filter(UNM11_demographics, !pNum %in% UNM11_not_passed$pNum)
```

```{r, include = FALSE}
#create the PPR measure and a epoch variable
UNM11_training <- UNM11_training %>%
  mutate(prob_response = case_when(cue1 == "A" & response == "o1_image" ~ 1,
                                   cue1 == "A" & response != "o1_image" ~ 0,
                                   cue1 == "B" & response == "o2_image" ~ 1,
                                   cue1 == "B" & response != "o2_image" ~ 0,
                                   cue1 == "C" & response == "o3_image" ~ 1,
                                   cue1 == "C" & response != "o3_image" ~ 0),
         epoch = case_when(block == 1 & trial <= 30 ~ 1,
                           block == 1 & (trial > 30 & trial <= 60) ~ 2,
                           (block == 1 & trial >= 60) ~ 3,
                           block == 2 & trial <= 30 ~ 4,
                           block == 2 & (trial > 30 & trial <= 60) ~ 5,
                           (block == 2 & trial >= 60) ~ 6,
                           block == 3 & trial <= 30 ~ 7,
                           block == 3 & (trial > 30 & trial <= 60) ~ 8,
                           (block == 3 & trial >= 60) ~ 9))
```

`r UNM11_all_p` participants were recruited through Prolific, but `r nrow(UNM11_not_passed)` where excluded due to failing the comprehension check prior to the test phase. Of the remaining `r nrow(UNM11_demographics)`, five participants did not report their age, nine did not report their gender, and one did not report their nationality. From the data available, the mean age was `r format(mean(UNM11_demographics$age, na.rm = TRUE), digits = 3)` (range `r min(UNM11_demographics$age, na.rm = TRUE)` - `r max(UNM11_demographics$age, na.rm = TRUE)`), with `r length(which(UNM11_demographics$gender == "female"))` women, `r length(which(UNM11_demographics$gender == "male"))` men, and one non-binary person, and `r n_distinct(UNM11_demographics$Nationality)` different nationalities. Participants were randomly allocated to each condition. The final sample was determined a priori based on the previous experiments. As in Experiment 3, the main analysis was a mixed model ANOVA with the within factor predictiveness and the between factor group, and our main interest was to have enough sample to detect the effect of group, sample size will be identical to Experiment 3, for which the a priori calculations showed that, to achieve a power of β = .85 with an effect size of ηp2 = .05 and a significance level of α = .05 for the between-subjects factor group, a sample of 159 participants is needed. It is worth noting that we expect a directionality of the effect here, as we expect group Unexpected Uncertain to have higher mean than the Certain and Uncertain groups, which will lead to the use of a one-tailed test and thus adopting a significance level of α = .1. With this significance level, as well as a sample size of 159 and an effect size of ηp2 = .05, the estimated power that would be achieved will be of β = .913. 

### Apparatus and stimuli

The materials were the same as in Experiments 1, 2 and 3.

### Design

The design of Experiment 4 can be seen in @tbl-exp4. The experiment used a mixed design, with the cue predictiveness factor (predictive vs non-predictive) manipulated within-subject, and the group factor (Certain, Unexpected Uncertain, Uncertain) manipulated between-subject. The training phase consisted of three blocks, each containing 90 trials. There were nine trial types, each presented 10 times per block. This training was divided in two stages: Stage 1 comprised the first two blocks and Stage 2, the last one block. For groups Certain and Uncertain, training in Stages 1 and 2 was identical, but for group Unexpected Uncertain, the training in Stage 1 will differ from the training in Stage 2. In the Certain group, cues A, B and C were perfectly predictive of the outcomes they were paired with. In the Uncertain group, cues A, B and C were the best available predictors on each trial but had a 0.8 contingency with the predicted outcome. To implement this contingency, in each block, for eight of the ten trials one outcome was “correct” (e.g., AX-O1), and for the remaining two trials, each of the other two outcomes were "correct" once (e.g., AX-O2 / AX-O3). Cues X, Y, and Z were paired equally often with outcomes 1, 2 and 3 and were therefore non-predictive. Group Unexpected Uncertain received the same training as group Certain in Stage 1, and the same training as group Uncertain in Stage 2. In the memory test, the predictive targets were always presented with a foil based on another predictive cue, and non-predictive targets were presented with a foil based on another non-predictive cue. For example, target A was presented with the foil corresponding to cue B and the foil corresponding to cue C, whereas X was presented with the foil corresponding with cue Y and the foil corresponding with cue Z. The rest of the details of the design are identical to Experiments 1, 2 and 3.


### Procedure

The procedure was identical to Experiments 1, 2 and 3.

## Results

```{r, include = FALSE}
#Calculate the mean PPR and standard error for each block and each participant, including the groups and stages
UNM11_acc_training <- UNM11_training %>%
  group_by(pNum, epoch, stage, condition) %>%
  summarise(PPR = mean(prob_response, na.rm = TRUE))

#calcualte mean and se per groups
UNM11_MA_training <- UNM11_acc_training %>%
  group_by(epoch, stage, condition) %>%
  summarise(mean_PPR = mean(PPR, na.rm = TRUE), 
            se_PPR = sd(PPR, na.rm = TRUE)/sqrt(length(PPR)))
```

Given that in this experiment there were only three blocks of training, the results for the training phase are displayed and analysed in epochs. Each epoch consisted of 30 trials, dividing each block in 3 epochs. The mean PPR per group and epoch is shown in @fig-trainingExp4. Group Certain showed an increase in their PPR throughout training, reaching a PPR of about 0.8 in epoch 9. Group Unexpected Uncertain an increase in PPR similar to group Certain in Stage 1, with a small decrease in Stage 2. Group Uncertain also showed an increase in PPR, but the level was consistently lower than in the other two groups. 

```{r, echo = FALSE, message=FALSE, warning=FALSE}
#| label: fig-trainingExp4
#| fig-cap: PPR on the training phase of Experiment 4.
#| apa-note: "Mean proportion of probable responses (±SEM) during the training phase of Experiment 4, plotted against the nine epochs of 30 trials each, for each Group."
#| fig-height: 4
ggplot(UNM11_MA_training, mapping = aes(x = epoch, y = mean_PPR, group = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")))) +
  geom_point(mapping = aes(shape = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")), color = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain"))), size = 2.5) +
  geom_line(mapping = aes(color = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")))) +
  geom_errorbar(aes(x= epoch, y = mean_PPR, ymin = mean_PPR-se_PPR, ymax = mean_PPR+se_PPR), colour = "black", width=.1)+
  facet_grid(cols = vars(stage), space = "free_x", scales = "free_x") + 
  scale_x_continuous(name = "Epoch", breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9)) + 
  scale_color_discrete(type = c("#AF8DC3", "#FEB24C", "#7FBF7B"))+
  labs(shape = "Group", color = "Group") +
  scale_y_continuous(name = "PPR", limits = c(NA, 1))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA
UNM11_acc_training$stage <- factor(UNM11_acc_training$stage)
UNM11_acc_training$epoch <- factor(UNM11_acc_training$epoch)
UNM11_acc_training$pNum <- factor(UNM11_acc_training$pNum)
UNM11_acc_training$condition <- factor(UNM11_acc_training$condition)
ANOVA_UNM11_train <- aov_car(formula = PPR ~ condition + Error(pNum/epoch), data = UNM11_acc_training)
print(ANOVA_UNM11_train)
#Bayesian Anova
bay_ANOVA_UNM11_train <- anovaBF(formula = PPR ~ condition + epoch + pNum,
        data = data.frame(UNM11_acc_training),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM11_train)
bay_ANOVA_UNM11_train_int <- bay_ANOVA_UNM11_train[4]/bay_ANOVA_UNM11_train[3]
print(bay_ANOVA_UNM11_train_int)
```
```{r, include = FALSE}
# SME of the condition:block interaction
SME_UNM11_train <- UNM11_training %>%
  group_by(pNum, condition, epoch) %>%
  summarise(mean_response = mean(prob_response, na.rm = TRUE))
#calculate the simple main effect of condition
sme_UNM11_training_condition <- SME_UNM11_train %>%
  group_by(epoch) %>%
  anova_test(mean_response ~ condition, effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_UNM11_training_condition #Call the output table
#calculate the simple main effect of block
sme_UNM11_training_epoch <- SME_UNM11_train %>%
  group_by(condition) %>%
  anova_test(mean_response ~ epoch + Error(pNum/epoch), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_UNM11_training_epoch #Call the output table

epochs <- 1:9
# Store results in a list
bay_SME_UNM11_train <- lapply(epochs, function(e) {
  df_epoch <- UNM11_training %>%
    filter(epoch == e) %>%
    group_by(pNum, condition) %>%
    summarise(mean_response = mean(prob_response, na.rm = TRUE),
              .groups = "drop")
  df_epoch$condition <- factor(df_epoch$condition)
  res <- anovaBF(
    formula = mean_response ~ condition,
    data = data.frame(df_epoch),
    iterations = bfit
  )
  return(res)
})
names(bay_SME_UNM11_train) <- paste0("epoch_", epochs)
# Create summary table with Bayes Factors
bay_summary_SME_UNM11_train <- lapply(names(bay_SME_UNM11_train), function(nm) {
  df <- as.data.frame(bay_SME_UNM11_train[[nm]])
  df$epoch <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_UNM11_train <- bay_summary_SME_UNM11_train %>%
  select(epoch, everything())
# Show table
print(bay_summary_SME_UNM11_train)
```
```{r, include=FALSE}
#Simple comparisons for block
sc_UNM11_train <- SME_UNM11_train %>%
  group_by(epoch) %>%
  pairwise_t_test(mean_response ~ condition,
                  paired = FALSE,
                  p.adjust.method = "bonferroni")
#Call the summary
sc_UNM11_train

# Define blocks and conditions
epochs <- 1:9
conditions <- c("Certain", "Unexpected Uncertain", "Uncertain")
# Store raw ttestBF objects in a nested list: epoch -> comparison
bay_sc_UNM11_train <- lapply(epochs, function(b) {
  df_epoch <- SME_UNM11_train %>%
    filter(epoch == b) %>%
    group_by(pNum, condition) %>%
    summarise(mean_response = mean(mean_response, na.rm = TRUE), .groups = "drop")
  df_epoch$condition <- factor(df_epoch$condition, levels = conditions)
# Generate all pairwise comparisons
  cond_pairs <- combn(conditions, 2, simplify = FALSE)
# Compute ttestBF for each pair and store the raw object
  pair_results <- lapply(cond_pairs, function(pair) {
    data1 <- df_epoch %>% filter(condition == pair[1]) %>% pull(mean_response)
    data2 <- df_epoch %>% filter(condition == pair[2]) %>% pull(mean_response)
    
    ttestBF(data1, data2)  # return the raw BayesFactor object
  })
# Name the list elements with descriptive pair names
  names(pair_results) <- sapply(cond_pairs, function(x) paste(x, collapse = "_vs_"))
  return(pair_results)
})
# Name outer list by epoch
names(bay_sc_UNM11_train) <- paste0("epoch_", epochs)
# Later, if you want a summary table of BF10 and error, you can do:
bay_summary_sc_UNM11_train <- lapply(names(bay_sc_UNM11_train), function(b) {
  lapply(names(bay_sc_UNM11_train[[b]]), function(pair) {
    bf_obj <- bay_sc_UNM11_train[[b]][[pair]]
    bf_info <- extractBF(bf_obj)
    tibble(
      block = b,
      comparison = pair,
      BF10 = as.numeric(bf_info$bf),
      error = as.numeric(bf_info$error)
    )
  }) %>% bind_rows()
}) %>% bind_rows()
bay_summary_sc_UNM11_train
```

A mixed model ANOVA for the between subjects factor *group* (Certain, Unexpected Uncertain, Uncertain) and the within subjects factor *epoch* (1 to 9), showed a significant effect of *group*, `r apa(ANOVA_UNM11_train, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM11_train[2])`, of *epoch*, `r apa(ANOVA_UNM11_train, effect = "block")`, `r report_BF_and_error(bay_ANOVA_UNM11_train[1], sci_not = TRUE)`, and of the *group x block* interaction, `r apa(ANOVA_UNM11_train, effect = "condition:block")`, `r report_BF_and_error(bay_ANOVA_UNM10_train_int[1])`. Simple main effects showed a significant effect of condition in all epochs, *F*(`r sme_acc_UNM11_training_condition[3, 3]`, `r sme_UNM11_training_condition[8, 4]`) > `r sme_UNM11_training_condition[3, 5]`, *p* < `r sme_UNM11_training_condition[3, 9]`, `r report_BF_and_error(bay_SME_UNM11_train[["epoch_3"]])`. Bonferroni corrected pairwise comparisons showed that there was a significant difference between groups Certain and Uncertain in all epochs, *p* < `r sc_UNM10_train[4, 9]`, `r report_BF_and_error(bay_sc_UNM11_train[[2]][[2]])`, between groups Unexpected Uncertain and Uncertain in all epochs, *p* < `r sc_UNM11_train[3, 9]`, `r report_BF_and_error(bay_sc_UNM11_train[[1]][[3]])`, and between Certain and Unexpected Uncertain in epoch 8, *p* = `r sc_UNM11_train[23, 9]`, `r report_BF_and_error(bay_sc_UNM11_train[[8]][[1]])`. The rest of the comparisons were not significant in any of the blocks, *p* > `r sc_UNM11_train[26, 9]`, `r report_BF_and_error(bay_sc_UNM11_train[[9]][[1]])`.

```{r, include=FALSE}
#calculate the mean accuracy for each participant in each group and predictiveness
acc_UNM11_test <- UNM11_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
#Calculate the mean accuracy and standard error in each group and predictiveness
MA_UNM11_test <- acc_UNM11_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

@fig-acctestExp4 shows the accuracy results from the recognition memory test. Overall, accuracy was slightly higher in group Certain compared with the two other groups. The accuracy for predictive cues is modestly higher than for non-predictive cues in groups Certain and Unexpected Uncertain, but this is inverted for group Uncertain.

```{r, echo=FALSE, message=FALSE}
#| label: fig-acctestExp4
#| fig-cap: Accuracy on the test phase of Experiment 4.
#| apa-note: "Mean accuracy (±SEM) during the test phase of Experiment 4, across the three groups."
#| fig-height: 4
ggplot(data = MA_UNM11_test, mapping = aes(x = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  theme_apa()
```
```{r, include=FALSE}
#ANOVA accuracy
acc_UNM11_test$predictiveness <- factor(acc_UNM11_test$predictiveness)
acc_UNM11_test$condition <- factor(acc_UNM11_test$condition)
acc_UNM11_test$pNum <- factor(acc_UNM11_test$pNum)
ANOVA_acc_UNM11_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness), data = acc_UNM11_test)
print(ANOVA_acc_UNM11_test)

bay_ANOVA_acc_UNM11_test <- anovaBF(formula = acc ~ condition*predictiveness + pNum,
        data = data.frame(acc_UNM11_test),
        whichRandom = "pNum", 
        iterations = bfit)
print(bay_ANOVA_acc_UNM11_test)

bay_ANOVA_acc_UNM11_test_gxp <- bay_ANOVA_acc_UNM11_test[4]/bay_ANOVA_acc_UNM11_test[3]
print(bay_ANOVA_acc_UNM11_test_gxp)
```

A mixed model ANOVA found no significant differences due to to the between-subjects factor *group*, `r apa(ANOVA_acc_UNM11_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_acc_UNM11_test[1])`, nor for the within-subjects factor *predictiveness*, `r apa(ANOVA_acc_UNM11_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM11_test[2])`, nor the *group x predictiveness* interaction, `r apa(ANOVA_acc_UNM11_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM11_test_gxp[1])`. 

```{r, include = FALSE}
#create the memory_score
UNM11_test <- UNM11_test %>%
  mutate (c_mem_score = case_when(acc == 0 ~ 0, acc == 1 ~ mem_score))
#calculate mean memory score for each participant including group and predictiveness
UNM11_memscore_test <- UNM11_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
#Calculate the mean memory score and standard error for each group and predictivenes
UNM11_MS_test <- UNM11_memscore_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

@fig-testExp3 shows the recognition memory scores for the three conditions. Groups Certain and Unexpected uncertain both showed higher memory score for predictive than non-predictive cues, whereas in group Uncertain, there was slightly higher memory score for non-predictive cues. Also, group Unexpected Uncertain displayed a moderately higher memory score than the other two groups.

```{r, echo = FALSE, message=FALSE}
#| label: fig-testExp4
#| fig-cap: Memory scores on the Test of Experiment 4.
#| apa-note: "Mean memory scores (±SEM) during the Test of Experiment 4 for predictive and non-predictive cues across the three groups."
#| fig-height: 4
ggplot(UNM11_MS_test, mapping = aes(x = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  scale_y_continuous(name = "Memory score")+
  #scale_fill_grey(start = 0.33) +
  theme_apa()
```
```{r, include=FALSE}
#ANOVA mem_score
UNM11_memscore_test$predictiveness <- factor(UNM11_memscore_test$predictiveness)
UNM11_memscore_test$condition <- factor(UNM11_memscore_test$condition)
UNM11_memscore_test$pNum <- factor(UNM11_memscore_test$pNum)
ANOVA_UNM11_test <- aov_car(formula = mem_score ~ condition + Error(pNum*predictiveness), data = UNM11_memscore_test)
print(ANOVA_UNM11_test)
bay_ANOVA_UNM11_test <- anovaBF(formula = mem_score ~ condition + predictiveness ,
        data = data.frame(UNM11_memscore_test),
        whichRandom = "pNum", 
        iterations = bfit)
print(bay_ANOVA_UNM11_test)
bay_ANOVA_UNM11_test_gxp <- bay_ANOVA_UNM11_test[4]/bay_ANOVA_UNM11_test[3]
print(bay_ANOVA_UNM11_test_gxp)
```
```{r, include = FALSE}
# SME of the condition:block interaction
SME_UNM11_memscore_test <- UNM11_memscore_test %>%
  group_by(pNum, condition, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
#calculate the simple main effect of condition
sme_UNM11_memscore_test_condition <- SME_UNM11_memscore_test %>%
  group_by(predictiveness) %>%
  anova_test(mem_score ~ condition, effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_UNM11_memscore_test_condition #Call the output table
#calculate the simple main effect of block
sme_UNM11_memscore_test_pred <- SME_UNM11_memscore_test %>%
  group_by(condition) %>%
  anova_test(mem_score ~ predictiveness + Error(pNum/predictiveness), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_UNM11_memscore_test_pred #Call the output table

# Conditions you want to test
conditions <- c("Certain", "Unexpected Uncertain", "Uncertain")
# Run the Bayesian ANOVA for each condition
bay_sme_UNM11_memscore_test_pred <- lapply(conditions, function(cond) {
  df_cond <- UNM11_memscore_test %>%
    filter(condition == cond) %>%
    group_by(pNum, predictiveness) %>%
    summarise(mem_score = mean(mem_score, na.rm = TRUE),
              .groups = "drop")
  df_cond$predictiveness <- factor(df_cond$predictiveness)
  df_cond$pNum <- factor(df_cond$pNum)
  res <- anovaBF(
    formula = mem_score ~ predictiveness + pNum,
    data = data.frame(df_cond),
    whichRandom = "pNum",
    iterations = bfit
  )
  return(res)
})
names(bay_sme_UNM11_memscore_test_pred) <- conditions
# Create summary table
bay_summary_sme_UNM11_memscore_test_pred <- lapply(names(bay_sme_UNM11_memscore_test_pred), function(nm) {
  df <- as.data.frame(bay_sme_UNM11_memscore_test_pred[[nm]])
  df$condition <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_sme_UNM11_memscore_test_pred <- bay_summary_sme_UNM11_memscore_test_pred %>%
  select(condition, everything())
# Show table
print(bay_summary_sme_UNM11_memscore_test_pred)
```
A mixed model ANOVA, including the between-subjects factor *group* (Certain, Unexpected Uncertain, Uncertain), and the within-subjects factor *predictiveness* (predictive vs non-predictive) showed a significant main effect of *predictiveness*, `r apa(ANOVA_UNM11_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM11_test[2])`, and of the or the *group x predictiveness* interaction, `r apa(ANOVA_UNM11_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM11_test_gxp[1])`, but not of the *group* main effect, `r apa(ANOVA_UNM11_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM11_test_gxp[1])`. Simple main effects of the interaction showed that there was a significant effect of *predictiveness* in the Certain group, *F*(`r sme_UNM11_memscore_test_pred[1, 3]`, `r sme_UNM11_memscore_test_pred[1, 4]`) = `r sme_UNM11_memscore_test_pred[1, 5]`, *p* = `r sme_UNM11_memscore_test_pred[1, 9]`, *η~p~^2^* = `r sme_UNM11_memscore_test_pred[1, 8]`, `r report_BF_and_error(bay_sme_UNM11_memscore_test_pred[['Certain']])`, but not for the other two groups,  *F* <= `r `r sme_UNM11_memscore_test_pred[1, 8]`. It is also important to note that when the simple main effect of *group* was analysed, this was not significant for the predictive nor non-predictive cues, *F* < `r sme_UNM08_test_condition[1, 5]`. The predictiveness effect showed moderate Bayesian evidence in favor of the null hypothesis, probably driven by the lack of effect in the Unexpected Uncertain and Uncertain group.

## Dicussion

Experiment 4 examined the effect of certainty and predictiveness on recognition memory, replicating the previous three experiments, and extending them by including two new cues (one predictive and one non-predicitve). Three groups of participants received three blocks of training. Participants in group Certain were trained with certain contingencies in all three blocks, whereas participants in group Uncertain were trained with uncertain contingencies. A third group, Unexpected Uncertain, experienced two block of certain training followed by a block of uncertain training. However, on the subsequent memory test, there were no evident differences between these groups in their memory of the cues. There was a clear predictiveness in group Certain, with better memory for predictive cues, but this effect was not evident in the other two groups. Thus, the effect of a higher memory in group Unexpected Uncertain that was observed in Experiment 2 was not replicates, but the results of Experiment 1 were replicated. Experiment 1 showed no differences between the overall memory for groups Certain and Uncertain, and the predictiveness effect was only significant in group Certain. However, the results of this experiment, and Experiments 1 and 2, diverge from the results of Experiment 3, specially the lack of an interaction that point towards an effect of predictiveness that only occurs for group Certain. In the next section, we present a pooled analysis of the four experiments, in an attempt to reconcile the disparity of results found in this experimental series. 


# Experiments 1-4 pooled analysis

In this section, we present an overall analysis of the data from the four experiments. Although there were small differences in the training received for each group in each experiment, any participant that experienced certain contingencies throughout training was assigned to group Certain, any participant that received uncertain contingencies throughout training to group Uncertain, and any participant that experienced certain contingencies followed by uncertain contingencies to the Unexpected Uncertain group.

```{r, include=FALSE}
#load  and pool the data
UNM07_test <- UNM07_test %>%
  mutate(experiment = "UNM07", .before = pNum) %>%
  select(-choice)
UNM08_test <- UNM08_test %>%
  mutate(experiment = "UNM08", .before = pNum,
         condition = case_when(condition == "Certain Long" ~ "Certain",
                               condition == "Certain Short" ~ "Certain",
                               condition == "Unexpected Uncertain" ~ "Unexpected Uncertain"),
         pNum = pNum + max(UNM07_test$pNum)) %>%
  select(-choice)
UNM10_test <- UNM10_test %>%
  mutate(experiment = "UNM10", .before = pNum,
          pNum = pNum + max(UNM08_test$pNum))
UNM11_test <- UNM11_test %>%
  mutate(experiment = "UNM11", .before = pNum,
          pNum = pNum + max(UNM10_test$pNum))
UNMpooled <- rbind(UNM07_test, UNM08_test, UNM10_test, UNM11_test)
```
## Results
```{r, include=FALSE}
#calculate the mean accuracy for each participant in each group and predictiveness
acc_test <- UNMpooled %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
#Calculate the mean accuracy and standard error in each group and predictiveness
MA_test <- acc_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

@fig-acctestPool shows the accuracy results from the recognition memory test. Group Certain showed better performance for predictive than non-predictive cues, a difference that was attenuated in group Unexpected Uncertain, and that disappeared in group Uncertain. Overall accuracy was similar in all three groups.

```{r, echo=FALSE, message=FALSE}
#| label: fig-acctestPool
#| fig-cap: Accuracy on the test phase for the pooled data.
#| apa-note: "Mean accuracy (±SEM) during the test phase of the pooled data of Experiments 1 to 4, across groups Certain, Unexpected Uncertain, and Uncertain."
#| fig-height: 4
ggplot(data = MA_test, mapping = aes(x = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  theme_apa()
```
```{r, include=FALSE}
#ANOVA accuracy
acc_test$predictiveness <- factor(acc_test$predictiveness)
acc_test$condition <- factor(acc_test$condition)
acc_test$pNum <- factor(acc_test$pNum)
ANOVA_acc_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness), data = acc_test)
print(ANOVA_acc_test)

bay_ANOVA_acc_test <- anovaBF(formula = acc ~ condition*predictiveness + pNum,
        data = data.frame(acc_test),
        whichRandom = "pNum", 
        iterations = bfit
        )
print(bay_ANOVA_acc_test)

bay_ANOVA_acc_test_gxp <- bay_ANOVA_acc_test[4]/bay_ANOVA_acc_test[3]
print(bay_ANOVA_acc_test_gxp)
```
```{r, include = FALSE}
# SME of the condition:block interaction
SME_acc_test <- acc_test %>%
  group_by(pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
#calculate the simple main effect of condition
sme_acc_test_condition <- SME_acc_test %>%
  group_by(predictiveness) %>%
  anova_test(acc ~ condition, effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_acc_test_condition #Call the output table
#calculate the simple main effect of block
sme_acc_test_pred <- SME_acc_test %>%
  group_by(condition) %>%
  anova_test(acc ~ predictiveness + Error(pNum/predictiveness), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_acc_test_pred #Call the output table

# Conditions you want to test
preds <- c("non-predicitive", "predictive")
# Run the Bayesian ANOVA for each condition
bay_SME_acc_test_cond <- lapply(preds, function(p) {
  df_pred <- acc_test %>%
    filter(predictiveness == p) %>%
    group_by(pNum, condition) %>%
    summarise(acc = mean(acc, na.rm = TRUE),
              .groups = "drop")
  df_pred$condition <- factor(df_pred$condition)
  df_pred$pNum <- factor(df_pred$pNum)
  res <- anovaBF(
    formula = acc ~ condition,
    data = data.frame(df_pred),
    whichRandom = "pNum",
    iterations = bfit
  )
  return(res)
})
names(bay_SME_acc_test_cond) <- preds
# Create summary table
bay_summary_SME_acc_test_cond <- lapply(names(bay_SME_acc_test_cond), function(nm) {
  df <- as.data.frame(bay_SME_acc_test_cond[[nm]])
  df$predictiveness <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_acc_test_cond <- bay_summary_SME_acc_test_cond %>%
  select(predictiveness, everything())
# Show table
print(bay_summary_SME_acc_test_cond)

# Conditions you want to test
conditions <- c("Certain", "Unexpected Uncertain", "Uncertain")
# Run the Bayesian ANOVA for each condition
bay_SME_acc_test_pred <- lapply(conditions, function(cond) {
  df_cond <- acc_test %>%
    filter(condition == cond) %>%
    group_by(pNum, predictiveness) %>%
    summarise(acc = mean(acc, na.rm = TRUE),
              .groups = "drop")
  df_cond$predictiveness <- factor(df_cond$predictiveness)
  df_cond$pNum <- factor(df_cond$pNum)
  res <- anovaBF(
    formula = acc ~ predictiveness + pNum,
    data = data.frame(df_cond),
    whichRandom = "pNum",
    iterations = bfit
  )
  return(res)
})
names(bay_SME_acc_test_pred) <- conditions
# Create summary table
bay_summary_SME_acc_test_pred <- lapply(names(bay_SME_acc_test_pred), function(nm) {
  df <- as.data.frame(bay_SME_acc_test_pred[[nm]])
  df$condition <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_acc_test_pred <- bay_summary_SME_acc_test_pred %>%
  select(condition, everything())
# Show table
print(bay_summary_SME_acc_test_pred)
```
```{r, include=FALSE}
#Simple comparisons for time
sc_acc_test <- SME_acc_test %>%
  group_by(predictiveness) %>%
  pairwise_t_test(acc ~ condition,
                  paired = FALSE,
                  p.adjust.method = "bonferroni")

#Call the summary
sc_acc_test

# Define blocks and conditions
preds <- c("predictive", "non-predictive")
conditions <- c("Certain", "Unexpected Uncertain", "Uncertain")
# Store raw ttestBF objects in a nested list: pred -> comparison
bay_sc_acc_test <- lapply(preds, function(b) {
  df_pred <- acc_test %>%
    filter(predictiveness == b) %>%
    group_by(pNum, condition) %>%
    summarise(acc = mean(acc, na.rm = TRUE), .groups = "drop")
  df_pred$condition <- factor(df_pred$condition, levels = conditions)
# Generate all pairwise comparisons
  cond_pairs <- combn(conditions, 2, simplify = FALSE)
# Compute ttestBF for each pair and store the raw object
  pair_results <- lapply(cond_pairs, function(pair) {
    data1 <- df_pred %>% filter(condition == pair[1]) %>% pull(acc)
    data2 <- df_pred %>% filter(condition == pair[2]) %>% pull(acc)
    
    ttestBF(data1, data2)  # return the raw BayesFactor object
  })
# Name the list elements with descriptive pair names
  names(pair_results) <- sapply(cond_pairs, function(x) paste(x, collapse = "_vs_"))
  return(pair_results)
})
# Name outer list by epoch
names(bay_sc_acc_test) <- paste0("pred_", preds)
# Later, if you want a summary table of BF10 and error, you can do:
bay_summary_sc_acc_test <- lapply(names(bay_sc_acc_test), function(b) {
  lapply(names(bay_sc_acc_test[[b]]), function(pair) {
    bf_obj <- bay_sc_acc_test[[b]][[pair]]
    bf_info <- extractBF(bf_obj)
    tibble(
      block = b,
      comparison = pair,
      BF10 = as.numeric(bf_info$bf),
      error = as.numeric(bf_info$error)
    )
  }) %>% bind_rows()
}) %>% bind_rows()
bay_summary_sc_acc_test
```

A mixed model ANOVA for the within-subject factor *predictiveness* (predictive, non-predictive), and for the between-subjects *group* showed a significant main effect of *predictiveness*, `r apa(ANOVA_acc_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_test[2])`, but not a significant main effect of the *group*, `r apa(ANOVA_acc_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_acc_test[1])`. Furthermore, there was a significant *group x predictiveness* interaction `r apa(ANOVA_acc_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_test_gxp[1])`. Simple main effects showed an effect of  predictiveness in group Certain, *F* (`r sme_acc_test_pred[1,3]`, `r sme_acc_test_pred[1,4]`) = `r sme_acc_test_pred[1,5]`, *p* < 0.001, *η~p~^2^* = `r sme_acc_test_pred[1, 8]`, `r report_BF_and_error(bay_SME_acc_test_pred[['Certain']])`, but not on the other two groups, *F* < `r sme_acc_test_pred[3,5]`, *p* > 0.057. It is worth noting that there was a simple main effect of *group* for predictive cues, *F* (`r sme_acc_test_condition[2,3]`, `r sme_acc_test_condition[2,4]`) = `r sme_acc_test_condition[2,5]`, *p* = `r sme_acc_test_condition[2, 8]`, *η~p~^2^* = `r sme_acc_test_condition[2, 8]`, `r report_BF_and_error(bay_SME_acc_test_cond[['predictive']])`, but not for the non-predictive cues, *F* (`r sme_acc_test_condition[1,3]`, `r sme_acc_test_condition[1,4]`) = `r sme_acc_test_condition[1,5]`, *p* = `r sme_acc_test_condition[1, 8]`, *η~p~^2^* = `r sme_acc_test_condition[1, 8]`, `r report_BF_and_error(bay_SME_acc_test_cond[['non-predictive']])`. Bonferroni corrected comparisons showed that, for predictive cues, there was significant differences between Certain and Uncertain groups,  *p* = `r sc_acc_test[3, 9]`, `r report_BF_and_error(bay_sc_acc_test$pred_predictive$Certain_vs_Uncertain)`, but not for the other groups, *p* $\ge$ `r sc_acc_test[6, 9]`, `r report_BF_and_error(bay_sc_acc_test[[2]][[3]])`. 

```{r, include = FALSE}
#calculate mean memory score for each participant including group and predictiveness
memscore_test <- UNMpooled %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
#Calculate the mean memory score and standard error for each group and predictivenes
MS_test <- memscore_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

@fig-testPool shows the recognition memory scores for the three conditions. Group Certain showed better performance for predictive than non-predictive cues, a difference that was attenuated in group Unexpected Uncertain, and that disappeared in group Uncertain. Overall memory score was slightly higher in group Unexpected Uncertain, followed by group Certain, and the lowest in group Uncertain.

```{r, echo = FALSE, message=FALSE}
#| label: fig-testPool
#| fig-cap: Memory scores on the test phase for the pooled data.
#| apa-note: "Mean memory scores (±SEM) during the test phase of the pooled data of Experiments 1 to 4, across groups Certain, Unexpected Uncertain, and Uncertain."
#| fig-height: 4
ggplot(MS_test, mapping = aes(x = factor(condition, c("Certain", "Unexpected Uncertain", "Uncertain")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  scale_y_continuous(name = "Memory score")+
  #scale_fill_grey(start = 0.33) +
  theme_apa()
```


```{r, include=FALSE}
#ANOVA accuracy
memscore_test$predictiveness <- factor(memscore_test$predictiveness)
memscore_test$condition <- factor(memscore_test$condition)
memscore_test$pNum <- factor(memscore_test$pNum)
ANOVA_memscore_test <- aov_car(formula = mem_score ~ condition + Error(pNum*predictiveness), data = memscore_test)
print(ANOVA_memscore_test )

bay_ANOVA_memscore_test  <- anovaBF(formula = mem_score ~ condition*predictiveness + pNum,
        data = data.frame(memscore_test ),
        whichRandom = "pNum", 
        iterations = bfit
        )
print(bay_ANOVA_memscore_test)

bay_ANOVA_memscore_test_gxp <- bay_ANOVA_memscore_test[4]/bay_ANOVA_memscore_test[3]
print(bay_ANOVA_memscore_test_gxp)
```
```{r, include = FALSE}
# SME of the condition:block interaction
SME_memscore_test <- memscore_test %>%
  group_by(pNum, condition, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE))
#calculate the simple main effect of condition
sme_memscore_test_condition <- SME_memscore_test  %>%
  group_by(predictiveness) %>%
  anova_test(mem_score ~ condition, effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_memscore_test_condition #Call the output table
#calculate the simple main effect of block
sme_memscore_test_pred <- SME_memscore_test  %>%
  group_by(condition) %>%
  anova_test(mem_score ~ predictiveness + Error(pNum/predictiveness), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_memscore_test_pred #Call the output table

# Conditions you want to test
conditions <- c("Certain", "Unexpected Uncertain", "Uncertain")
# Run the Bayesian ANOVA for each condition
bay_SME_memscore_test_pred <- lapply(conditions, function(cond) {
  df_cond <- memscore_test  %>%
    filter(condition == cond) %>%
    group_by(pNum, predictiveness) %>%
    summarise(mem_score = mean(mem_score, na.rm = TRUE),
              .groups = "drop")
  df_cond$predictiveness <- factor(df_cond$predictiveness)
  df_cond$pNum <- factor(df_cond$pNum)
  res <- anovaBF(
    formula = mem_score ~ predictiveness + pNum,
    data = data.frame(df_cond),
    whichRandom = "pNum",
    iterations = bfit
  )
  return(res)
})
names(bay_SME_memscore_test_pred) <- conditions
# Create summary table
bay_summary_SME_memscore_test_pred <- lapply(names(bay_SME_memscore_test_pred), function(nm) {
  df <- as.data.frame(bay_SME_memscore_test_pred[[nm]])
  df$condition <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_memscore_test_pred <- bay_summary_SME_memscore_test_pred %>%
  select(condition, everything())
# Show table
print(bay_summary_SME_memscore_test_pred)

# Conditions you want to test
preds <- c("non-predictive", "predictive")
# Run the Bayesian ANOVA for each condition
bay_SME_memscore_test_cond <- lapply(preds, function(p) {
  df_pred <- memscore_test  %>%
    filter(predictiveness == p) %>%
    group_by(pNum, condition) %>%
    summarise(mem_score = mean(mem_score, na.rm = TRUE),
              .groups = "drop")
  df_pred$condition <- factor(df_pred$condition)
  df_pred$pNum <- factor(df_pred$pNum)
  res <- anovaBF(
    formula = mem_score ~ condition,
    data = data.frame(df_pred),
    whichRandom = "pNum",
    iterations = bfit
  )
  return(res)
})
names(bay_SME_memscore_test_cond) <- preds
# Create summary table
bay_summary_SME_memscore_test_cond <- lapply(names(bay_SME_memscore_test_cond), function(nm) {
  df <- as.data.frame(bay_SME_memscore_test_cond[[nm]])
  df$predictiveness <- nm
  df
}) %>%
  bind_rows()
# Reorder columns
bay_summary_SME_memscore_test_cond <- bay_summary_SME_memscore_test_cond %>%
  select(predictiveness, everything())
# Show table
print(bay_summary_SME_memscore_test_cond)
```
```{r, include=FALSE}
#Simple comparisons for time
sc_memscore_test <- SME_memscore_test %>%
  group_by(predictiveness) %>%
  pairwise_t_test(mem_score ~ condition,
                  paired = FALSE,
                  p.adjust.method = "bonferroni")

#Call the summary
sc_memscore_test

# Define blocks and conditions
preds <- c("non-predictive", "predictive")
conditions <- c("Certain", "Uncertain", "Unexpected Uncertain")
# Store raw ttestBF objects in a nested list: pred -> comparison
bay_sc_memscore_test <- lapply(preds, function(b) {
  df_pred <- memscore_test %>%
    filter(predictiveness == b) %>%
    group_by(pNum, condition) %>%
    summarise(mem_score = mean(mem_score, na.rm = TRUE), .groups = "drop")
  df_pred$condition <- factor(df_pred$condition, levels = conditions)
# Generate all pairwise comparisons
  cond_pairs <- combn(conditions, 2, simplify = FALSE)
# Compute ttestBF for each pair and store the raw object
  pair_results <- lapply(cond_pairs, function(pair) {
    data1 <- df_pred %>% filter(condition == pair[1]) %>% pull(mem_score)
    data2 <- df_pred %>% filter(condition == pair[2]) %>% pull(mem_score)
    
    ttestBF(data1, data2)  # return the raw BayesFactor object
  })
# Name the list elements with descriptive pair names
  names(pair_results) <- sapply(cond_pairs, function(x) paste(x, collapse = "_vs_"))
  return(pair_results)
})
# Name outer list by epoch
names(bay_sc_memscore_test) <- paste0("pred_", preds)
# Later, if you want a summary table of BF10 and error, you can do:
bay_summary_sc_memscore_test <- lapply(names(bay_sc_memscore_test), function(b) {
  lapply(names(bay_sc_memscore_test[[b]]), function(pair) {
    bf_obj <- bay_sc_memscore_test[[b]][[pair]]
    bf_info <- extractBF(bf_obj)
    tibble(
      block = b,
      comparison = pair,
      BF10 = as.numeric(bf_info$bf),
      error = as.numeric(bf_info$error)
    )
  }) %>% bind_rows()
}) %>% bind_rows()
bay_summary_sc_memscore_test
```

A mixed model ANOVA showed did not show a significant effect of *group*, `r apa(ANOVA_memscore_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_memscore_test[1])`, but there was a significant main effect of *predictiveness*, `r apa(ANOVA_memscore_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_memscore_test[2])`, and a significant *group x predictiveness* interaction `r apa(ANOVA_memscore_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_memscore_test_gxp[1])`. Simple main effects showed an effect of  predictiveness in group Certain, *F* (`r sme_memscore_test_pred[1,3]`, `r sme_memscore_test_pred[1,4]`) = `r sme_memscore_test_pred[1,5]`, *p* < 0.001, *η~p~^2^* = `r sme_memscore_test_pred[1, 8]`, `r report_BF_and_error(bay_SME_memscore_test_pred[['Certain']])`, and on group Unexpected Uncertain, *F* (`r sme_memscore_test_pred[3,3]`, `r sme_memscore_test_pred[3,4]`) = `r sme_memscore_test_pred[3,5]`, *p* = `r sme_memscore_test_pred[3, 9]`, *η~p~^2^* = `r sme_memscore_test_pred[3, 8]`, `r report_BF_and_error(bay_SME_memscore_test_pred[['Unexpected Uncertain']])`,  but not on the Uncertain group, *F* < 1. It is worth noting that there was a simple main effect of *group* for predictive cues, *F* (`r sme_memscore_test_condition[2,3]`, `r sme_memscore_test_condition[2,4]`) = `r sme_memscore_test_condition[2,5]`, *p* = `r sme_memscore_test_condition[2, 8]`, *η~p~^2^* = `r sme_memscore_test_condition[2, 8]`, `r report_BF_and_error(bay_SME_memscore_test_cond[['predictive']])`and for the non-predictive cues, *F* (`r sme_memscore_test_condition[1,3]`, `r sme_memscore_test_condition[1,4]`) = `r sme_memscore_test_condition[1,5]`, *p* = `r sme_memscore_test_condition[1, 8]`, *η~p~^2^* = `r sme_memscore_test_condition[1, 8]`, `r report_BF_and_error(bay_SME_memscore_test_cond[['non-predictive']])`. Bonferroni corrected comparisons showed that, for non-predictive cues, there was a significant difference between groups Certain and Unexpected Uncertain, *p* = `r sc_memscore_test[2, 9]`, `r report_BF_and_error(bay_sc_memscore_test[[1]][[2]])`, and for the predictive cues, there was significant differences between Certain and Uncertain groups,  *p* = `r sc_memscore_test[4, 9]`, `r report_BF_and_error(bay_sc_memscore_test$pred_predictive$Certain_vs_Uncertain)`, and for Unexpected Uncertain and Uncertain groups, *p* = `r sc_memscore_test[6, 9]`, `r report_BF_and_error(bay_sc_memscore_test[[2]][[2]])`. No other pairwise comparison was significant, *p* $\ge$ `r sc_memscore_test[1, 8]`, `r report_BF_and_error(bay_sc_memscore_test$pred_predictive$Certain_vs_Uncertain)`. 

## Discussion
  


# General discussion

Two experiments explored the processing of cues under different
conditions of certainty and uncertainty during a contingency learning
task. In Experiment 1, participants were trained in either a “certain”
condition, in which there were cues that were perfectly predictive of
the outcomes, or in an "uncertain" condition, in which there was a
probabilistic relationship between the predictive cues and the outcomes.
We predicted that uncertainty would increase the memory for cues, in
line with the previously established increased levels of overt attention
to cues trained under uncertain conditions [e.g.,
@beesleyUncertaintyPredictivenessDetermine2015;
@easdaleOnsetUncertaintyFacilitates2019;
@torrents-rodasEffectPredictionError2023;
@walkerRoleUncertaintyAttentional2019;
@walkerProtectionUncertaintyExploration2022]. However, this was not the
case: overall, there was no effect of uncertainty on memory scores, with
evidence to support a conclusion of equivalent levels of memory for cues
(overall) in the Certain and Uncertain conditions. Memory scores for the
predictive cues were higher than those for the non-predictive cues, , a
finding that is consistent with the higher levels of overt attention
paid to predictive cues in the learned predictiveness design [e.g.,
@lepelleyOvertAttentionPredictiveness2011]. However, this result was
restricted to group Certain: there was no observed difference in
recognition memory between predictive and non-predictive cues in group
Uncertain.

Experiment 2 explored the differences in recognition memory for
conditions of “expected” and “unexpected” uncertainty. Unexpected
uncertainty is defined here as a procedure in which participants were
initially trained on certain contingencies and then given further
training, for a shorter period, with uncertain contingencies. This
condition was compared to the certain condition (as trained in
Experiment 1). This period of unexpected uncertainty in the task had an
important effect on cue-processing and memory performance: participants
in this unexpected uncertainty condition showed better memory than those
in the certain condition. We included an additional condition in
Experiment 2 that received just the first phase of certain training,
equivalent in length to that experienced in the unexpected uncertainty
condition. This condition allowed us to determine that it was indeed the
period of unexpected uncertainty that led to *increases* in
cue-processing, rather than further training of the certain
contingencies (in the standard certain condition) leading to a decrease
in cue-processing. Memory was better in the case of the unexpected
uncertainty condition compared to this short-certain condition, and
therefore a short period of unexpected uncertainty appears to enhance
the memory for, and thus processing of, the cues.

It is worth noting that, in both experiments reported here, we observed
an effect of predictive validity on recognition memory, when, for the
Certain conditions: group Certain in Experiment 1 and groups Certain
Long and Certain Short in Experiment 2 showed better memory for
predictive than non-predictive cues. To the best of our knowledge only
one other study has investigated the relationship between the predictive
validity of cues and recognition memory. Griffiths and Mitchell
[-@griffithsSelectiveAttentionHuman2008] used a recognition memory test
as a measure of cue processing and in Experiment 4 of their study used a
learned predictiveness design adapted from Le Pelley and McLaren
[-@lepelleyLearnedAssociabilityAssociative2003]. However, Griffiths and
Mitchell [-@griffithsSelectiveAttentionHuman2008] did not observe
differences in the recognition performance for predictive and
non-predictive cues. There are two main procedural differences between
the present study and Experiment 4 of Griffiths and Mitchell
[-@griffithsSelectiveAttentionHuman2008]. First, their experimental task
involved learning about predictive and non-predictive categories of
stimuli, with each exemplar of a category being presented only once
during training, whereas in our task, the same set of four stimuli was
repeatedly presented during training. Second, their learned
predictiveness task consisted of two phases: the first phase was similar
to the training phase of the present study, but this was followed by a
second phase in which all the categories were predictive of new
outcomes. The memory test followed this second phase. The addition of
this second phase is perhaps the most likely reason for the differences
in the findings: the cue-processing bias that is established in phase 1
of the design is likely to be considerably attenuated by the second
phase in which all cues were established as valid predictors of the
outcomes.

Attentional theories of associative learning have long recognised the
role that uncertainty plays in determining the allocation of processing
resources to stimuli in the environment. According to the Pearce and
Hall [-@pearceModelPavlovianLearning1980] model, the effective salience
of a stimulus is determined by the magnitude of the absolute prediction
error that stimulus has received on the previous trial. That is, if a
stimulus was followed by an unexpected outcome, then the associability
of the stimulus on the next trial should be high; if an expected outcome
was received, then the associability should be low. Pearce and Hall
described this as an active attentional process that aids the animal in
discovering information about stimuli for which it is unsure about the
consequences. In the original model, the attention, and therefore the
associability of a stimulus, is determined by the prediction error on
the previous trial, while in the revised model of Pearce, Kaye, and Hall
[-@pearcePredictiveAccuracyStimulus1982], this was determined by a
longer history of reinforcement (the length of which was controlled by a
parameter in the model). In either case, the model makes the prediction
that uncertainty – periods in which there is prediction error in the
reinforcement schedule – will result in high levels of attention to a
stimulus. The results of Experiment 1 are therefore not compatible with
the principles of the Pearce-Hall model
[@pearceModelPavlovianLearning1980;
@pearcePredictiveAccuracyStimulus1982], since the uncertain condition
did not show evidence of greater levels of cue-processing than the
certain condition, despite the substantial level of prediction error
that was experienced in the former. Since we know from previous work
that this same procedure results in higher levels of overt attention to
cues [@beesleyUncertaintyPredictivenessDetermine2015;
@easdaleOnsetUncertaintyFacilitates2019;
@walkerProtectionUncertaintyExploration2022;
@walkerRoleUncertaintyAttentional2019], the results of Experiment 1
provide insights into the complex relationships between overt attention,
active stimulus processing, and associative learning. The data from
Experiment 1 suggest that the circumstances that favour high levels of
overt attention do not necessarily translate directly into active and
enhanced processing of the stimuli. Thus, the data are consistent with
the failures to observe more rapid learning about new associations under
such conditions of uncertainty
[@beesleyUncertaintyPredictivenessDetermine2015;
@easdaleOnsetUncertaintyFacilitates2019;
@torrents-rodasEffectPredictionError2023]. Taken alone, the data from
Experiment 1 suggest that the hitherto assumption that eye-gaze dwell
time can be used as a proxy measure of stimulus associability, is on
shaky ground.

The data from Experiment 2, however, suggest that uncertainty can, under
some circumstances, lead to enhancements in stimulus processing. In this
procedure, after a period of training in which the task contingencies
were deterministic, there was a sudden change to probabilistic
(uncertain) contingencies. Under these conditions, evidence for enhanced
stimulus processing was obtained, with better memory for the cue stimuli
overall, compared to conditions in which participants were only trained
with certain contingencies. In light of the findings of Experiment 1,
these findings from Experiment 2 provide strong evidence for the
distinction made by Easdale et al.
[-@easdaleOnsetUncertaintyFacilitates2019] between expected and
unexpected uncertainty. After periods of prolonged exposure to stable
levels of uncertainty, participants appear to develop a tolerance for
this uncertainty and their stimulus processing decreases. In contrast, a
sudden onset of uncertain contingencies, following exposure to entirely
certain contingencies, boosts stimulus processing. Together, these data
suggest a non-linear relationship between contingency exposure and
stimulus processing under uncertainty. It is likely that, at the outset
of exposure to an uncertain contingency, stimulus processing will be
high, and this level of processing will decline over the course of
experience with a stable (but uncertain) contingency. In contrast, the
sudden experience of a large prediction error, following certain
contingencies, mar return the level of stimulus processing to something
approaching a maximal level. Continual training with uncertain
contingencies would be expected to see a decline in stimulus processing;
it is likely that the configuration of our procedure in Experiment 2 was
able to capture this high level of stimulus processing prior to an
expected decline in the level of stimulus processing with continued
uncertainty (i.e., a transition to a state of expected uncertainty).

In view of the analysis provided in the context of the Pearce and Hall
model, it is clear that the conditions of expected uncertainty present a
major challenge to attentional theories of associative learning. The
data from the current studies and others using similar designs
[@beesleyUncertaintyPredictivenessDetermine2015;
@easdaleOnsetUncertaintyFacilitates2019;
@walkerProtectionUncertaintyExploration2022;
@walkerRoleUncertaintyAttentional2019] illustrate that the cognitive
system is sensitive not just to the absolute level of prediction error,
but how long that prediction error has been experienced for, and how
stable that pattern of uncertainty has been. Indeed, the manner in which
the uncertainty is experienced in the task, not just the overall level
of uncertainty, can affect the pattern of choices and attention. One
line of examination for future experimental and theoretical work would
be to explore whether expected uncertainty reflects a localised
cue-specific parameter or is better reflected as a global property of
the learning system, akin to a reflection of “vigilance” on the task.

The analysis of the memory scores in Experiment 2 found a simple main
effect of group for non-predictive cues, but not for predictive cues.
Thus, there is a suggestion that the elevated levels of recognition
memory seen for the unexpected uncertain condition in Experiment 2 was
primarily driven by increases in cue-processing for the non-predictive
cues. These findings suggest that unexpected uncertainty returns
participants to an exploratory mode of processing. According to the
uncertainty principle, the increased levels of attention to cues
associated with uncertainty operates as a mechanism to discover new
cue-outcome relationships. This process has been referred to as
“exploratory attention” [e.g.,
@beesleyUncertaintyPredictivenessDetermine2015;
@easdaleOnsetUncertaintyFacilitates2019] or “attention for learning”
[e.g., Hall & Rodríguez, -@hallAttentionPerceiveLearn2019a]. Thus, it is
an attempt to resolve the prediction errors that are experienced by
learning new and valid signals for the outcomes. In Experiment 2, it is
possible that the non-predictive cues have greater “capacity” for
learning, or perhaps offer a more plausible signal, given an association
already exists between the predictive cues and the outcomes from the
earlier certain phase. This exploratory process will be short lived in
this procedure, since the uncertainty that is experienced cannot be
resolved by learning new associations. Indeed, in the procedure of
Easdale et al. [-@easdaleOnsetUncertaintyFacilitates2019], participants
could resolve the uncertainty in a second stage by learning about the
meaningful associations pertaining to the previously non-predictive
cues. In this situation increased attentional processing of the
non-predictive cues was observed.

This characterisation of the effect, as one driven primarily by changes
in processing for non-predictive cues, is also consistent with the
*theory protection* account proposed by Spicer and colleagues
[-@spicerTheoryProtectionAssociative2020;
-@spicerTheoryProtectionHumans2022]. According to this account,
participants tend to protect existing associations as much as possible.
When prediction errors are experienced during learning, rather than
adjust those associations that result in the largest error, the learning
system will instead direct resources to learning about cues that are not
already strongly associated with outcomes. In line with this proposed
property of human learning, it is possible that, in Experiment 3,
participants in group Uncertain directed their attention more towards
non-predictive cues than predictive cues at the onset of the unexpected
uncertainty, since the predictive cues had already been established as
reliable predictors of the outcomes in the first stage of training (in
which the relationship between the predictive cues and the outcomes was
perfect). Further studies will be necessary to confirm if theory
protection offers an accurate account of the manner in which attentional
resources are allocated during uncertainty.

The current results provide a stark warning about the interpretation of
eye-gaze as a proxy for attentional processing. While we have not
collected eye-gaze data in the current tasks (since they were conducted
online), we have substantial evidence that uncertain conditions of the
form used in the current experiments lead to higher levels of both
absolute dwell times on cues, and a higher proportion of the response
time spent on cues [@beesleyUncertaintyPredictivenessDetermine2015;
@easdaleOnsetUncertaintyFacilitates2019;
@walkerRoleUncertaintyAttentional2019;
@walkerProtectionUncertaintyExploration2022]. The finding that higher
overt attention does not lead to faster learning
[@beesleyUncertaintyPredictivenessDetermine2015;
@easdaleOnsetUncertaintyFacilitates2019;
@torrents-rodasEffectPredictionError2023] and, in the current studies,
that it doesn’t lead to greater cue-processing, is problematic for the
use of eye-data in constraining attentional models of associative
learning. Of course, this concern of the apparent disconnect between
overt attention and active stimulus engagement stretches beyond the
field of human learning. As has been noted in other literatures, most
notably in the case of inattentional blindness [e.g.,
@simonsGorillasOurMidst1999]; looking does not always reflect an active
and engaged attentional process.

In conclusion, the studies reported here revealed that unexpected-, but
not expected- uncertainty, leads to an enhancement in the processing of
cues during associative learning, highlighting the importance of the
distinction between expected and unexpected uncertainty. Future work
will be needed to understand the implications of this distinction on
associability and subsequent learning, as well as on eye-gaze.
Furthermore, attentional models of associative learning will need to
accommodate and refine our understanding of expected and unexpected
uncertainty in order to adequately map out the relationships between
patterns of reinforcement, stimulus processing, and learning.

{{< pagebreak >}}

# References

::: {#refs}
:::

{{< pagebreak >}}

# Appendix I

The two sets of images from which the cues and foils displayed in the
study were randomly selected can be seen in @fig-cues_and_foils.

```{r @fig-cues_and_foils, echo=FALSE}
#| label: fig-cues_and_foils
#| fig-cap: Cues and foils used in Experiments 1 and 2.
#| apa-twocolumn: true
#apa-note: "Panel A displays the cues that can be selected for the training phase. Panel B displays the set of foils that could be selected in the memory test."
#| out-width: 100%
#| fig-pos: h
knitr::include_graphics("images/cues_foils.png")
```

The two images used as outcomes in these experiments can be seen in
@fig-outcomes.

```{r fig-outcomes, echo=FALSE}
#| label: fig-outcomes
#| fig-cap: Outcomes used in all experiments.
#| apa-twocolumn: true
#| out-width: 100%
#| fig-pos: h
knitr::include_graphics("images/outcomes.png")
```

{{< pagebreak >}}

# Appendix II

## Experiment 1

```{r, include=FALSE}
UNM07_test <- UNM07_test %>%
  mutate(trial_type = case_when((target == 1 & distractor == 2) | (target == 2 & distractor == 1) | (target == 3 & distractor == 4) | (target == 4 & distractor == 3) ~ "P-Con" ,
                                (target == 5 & distractor == 6) | (target == 6 & distractor == 5) |  (target == 7 & distractor == 8) | (target == 8 & distractor == 7)~ "NP-Con",
                                (target == 1 & (distractor == 5 | distractor == 6)) | (target == 2 & (distractor == 5 | distractor == 6)) | (target == 3 & (distractor == 7 | distractor == 8)) | (target == 4 & (distractor == 7 | distractor == 8)) ~ "P-Incon",
                                  (target == 5 & (distractor == 1 | distractor == 2)) | (target == 6 & (distractor == 1 | distractor == 2)) | (target == 7 & (distractor == 3 | distractor == 4)) | (target == 8 & (distractor == 3 | distractor == 4)) ~  "NP-Incon"),
         #add a congruence variable
         congruence = case_when ((trial_type == "P-Con") | (trial_type == "NP-Con") ~ "congruent",
                                 (trial_type == "P-Incon") | (trial_type == "NP-Incon") ~ "incongruent"))
```

```{r, include=FALSE}
#Calculate the mean accuracy and standard error for each block, including the groups
MA_test <- UNM07_test %>%
  group_by(condition, trial_type) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

@fig-acctestExp1_cong shows the accuracy results from the recognition
memory test of Experiment 1. In group Certain, accuracy for
non-predictive cues was lower than for the predictive cues in the
Certain group. This difference was bigger for the incongruent trials
than the congruent ones. However, this differences were not evident in
group Uncertain, in which, accuracy was similar in all trial types. The
overall accuracy was similar in both groups.

```{r, echo = FALSE, message=FALSE}
#| label: fig-acctestExp1_cong
#| fig-cap: Accuracy on the test phase of Experiment 1.
#| apa-note: "Mean accuracy (±SEM) during the test phase of Experiment 1, for groups trained with certain and uncertain contingencies."
#| fig-height: 4
ggplot(data = MA_test, mapping = aes(x = factor(condition, level=c('Uncertain', 'Certain')), y = mean_acc, fill = trial_type)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of test") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#7B3294", "#C2A5CF", "#008837", "#A6DBA0"))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA accuracy
acc_UNM07_test <- UNM07_test %>%
  group_by (pNum, condition, predictiveness, congruence) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_UNM07_test$predictiveness <- factor(acc_UNM07_test$predictiveness)
acc_UNM07_test$condition <- factor(acc_UNM07_test$condition)
acc_UNM07_test$pNum <- factor(acc_UNM07_test$pNum)
acc_UNM07_test$congruence<- factor(acc_UNM07_test$congruence)
ANOVA_acc_UNM07_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness*congruence), data = acc_UNM07_test)
print(ANOVA_acc_UNM07_test)

bay_ANOVA_acc_UNM07_test <- anovaBF(formula = acc ~ condition*predictiveness*congruence + pNum,
        data = data.frame(acc_UNM07_test),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_acc_UNM07_test)

bay_ANOVA_acc_UNM07_test_gxp <- bay_ANOVA_acc_UNM07_test[4]/bay_ANOVA_acc_UNM07_test[3]
print(bay_ANOVA_acc_UNM07_test_gxp)
bay_ANOVA_acc_UNM07_test_pxc <- bay_ANOVA_acc_UNM07_test[13]/bay_ANOVA_acc_UNM07_test[7]
print(bay_ANOVA_acc_UNM07_test_pxc)
bay_ANOVA_acc_UNM07_test_sxc <- bay_ANOVA_acc_UNM07_test[10]/bay_ANOVA_acc_UNM07_test[6]
print(bay_ANOVA_acc_UNM07_test_sxc)
bay_ANOVA_acc_UNM07_test_gxpxc <- bay_ANOVA_acc_UNM07_test[18]/bay_ANOVA_acc_UNM07_test[17]
print(bay_ANOVA_acc_UNM07_test_gxpxc)
```

A mixed model ANOVA including the between-subjects factor *group*
(Certain vs Uncertain) and the within-subjects factors *predictiveness*
(predictive vs non-predictive) and *congruence* (congruent vs
incongruent) was performed. This analysis found only a significant
effect of the *group x predictiveness* interaction,
`r apa(ANOVA_acc_UNM07_test, effect = "condition:predictiveness")`,
`r report_BF_and_error(bay_ANOVA_acc_UNM07_test_gxp[1])`; but no other
significant differences, *group*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_acc_UNM07_test[1])`; *predictiveness*:
`r apa(ANOVA_acc_UNM07_test, effect = "predictiveness")`,
`r report_BF_and_error(bay_ANOVA_acc_UNM07_test[2])`; *congruence*: *F*
$\le$ 1, `r report_BF_and_error(bay_ANOVA_acc_UNM07_test[5])`;
*predictiveness x congruence*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_acc_UNM07_test_pxc[1])`; *group x
congruence*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_acc_UNM07_test_sxc[1])`; *group x
predictiveness x congruence*:
`r apa(ANOVA_acc_UNM07_test, effect = "condition:predictiveness:congruence")`,
`r report_BF_and_error(bay_ANOVA_acc_UNM07_test_gxpxc[1])`. Simple main
effects analysis showed a significant effect of *predictiveness* for
group Certain, *F* (`r sme_acc_UNM07_test_pred[1, 3]`,
`r sme_acc_UNM07_test_pred[1, 4]`) = `r sme_acc_UNM07_test_pred[1, 5]`,
*p* = `r sme_acc_UNM07_test_pred[1, 9]`, *η~p~^2^* =
`r sme_acc_UNM07_test_pred[1, 8]`,
`r report_BF_and_error(bay_SME_acc_UNM07_test_certain[1])`, but not for
group Uncertain, *F* $\le$ 1,
`r report_BF_and_error(bay_SME_acc_UNM07_test_uncertain[1])`. These
analyses suggest that group Certain were more accurate at remembering
predictive than non-predictive cues, whereas group Uncertain did not
show this difference.

```{r, include=FALSE}
#plot test mem_score but take out the errors
M_mem_UNM07_test <- UNM07_test %>%
  group_by(trial_type, condition) %>%
  summarise(mean_mem_score = mean(c_mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

@fig-testExp1_cong shows the memory scores for Experiment 1. The memory
scores for non-predictive cues was lower than for the predictive cues in
the Certain group, with similar scores for congruent and incongruent
trials. This difference was not observed in the Uncertain group.
Overall, there was no indication of higher memory scores in the
Uncertain group relative to the Certain group.

```{r, echo = FALSE, message=FALSE}
#| label: fig-testExp1_cong
#| fig-cap: Memory scores during the Test of Experiment 1.
#| apa-note: "Mean memory scores (±SEM) during the Test phase of Experiment 1 for predictive and non-predictive trials in the Certain and Uncertain groups."
#| fig-height: 4
ggplot(data = M_mem_UNM07_test, mapping = aes(x = factor(condition, levels = c("Uncertain", "Certain")), y = mean_mem_score, fill = trial_type)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_y_continuous(name = "Memory Score") +
  scale_fill_discrete(type = c("#7B3294", "#C2A5CF", "#008837", "#A6DBA0"))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA mem_score
mem_UNM07_test <- UNM07_test %>%
  group_by (pNum, condition, predictiveness, congruence) %>%
  summarise(mem = mean(c_mem_score, na.rm = TRUE))
mem_UNM07_test$predictiveness <- factor(mem_UNM07_test$predictiveness)
mem_UNM07_test$condition <- factor(mem_UNM07_test$condition)
mem_UNM07_test$pNum <- factor(mem_UNM07_test$pNum)
mem_UNM07_test$congruence <- factor(mem_UNM07_test$congruence)
ANOVA_mem_UNM07_test <- aov_car(formula = mem ~ condition + Error(pNum*predictiveness*congruence), data = mem_UNM07_test)
print(ANOVA_mem_UNM07_test)

bay_ANOVA_mem_UNM07_test <- anovaBF(formula = mem ~ condition*predictiveness*congruence + pNum,
        data = data.frame(mem_UNM07_test),
        whichRandom = "pNum", 
        iterations = bfit)
print(bay_ANOVA_mem_UNM07_test)
bay_ANOVA_mem_UNM07_test_gxp <- bay_ANOVA_mem_UNM07_test[4]/bay_ANOVA_mem_UNM07_test[3]
print(bay_ANOVA_mem_UNM07_test_gxp)
bay_ANOVA_mem_UNM07_test_pxc <- bay_ANOVA_mem_UNM07_test[13]/bay_ANOVA_mem_UNM07_test[7]
print(bay_ANOVA_mem_UNM07_test_pxc)
bay_ANOVA_mem_UNM07_test_sxc <- bay_ANOVA_mem_UNM07_test[10]/bay_ANOVA_mem_UNM07_test[6]
print(bay_ANOVA_mem_UNM07_test_sxc)
bay_ANOVA_mem_UNM07_test_gxpxc <- bay_ANOVA_mem_UNM07_test[18]/bay_ANOVA_mem_UNM07_test[17]
print(bay_ANOVA_mem_UNM07_test_gxpxc)
```

A mixed model ANOVA found a significant effect of *predictiveness*,
`r apa(ANOVA_mem_UNM07_test, effect = "predictiveness")`,
`r report_BF_and_error(bay_ANOVA_mem_UNM07_test[2])`, and a significant
*group x predictiveness* interaction,
`r apa(ANOVA_mem_UNM07_test, effect = "condition:predictiveness")`,
`r report_BF_and_error(bay_ANOVA_mem_UNM07_test_gxp[1])`. There were no
other significant effects or interactions, *group*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_mem_UNM07_test[1])`; *congruence*: *F*
$\le$ 1, `r report_BF_and_error(bay_ANOVA_mem_UNM07_test[5])`;
*predictiveness x congruence*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_mem_UNM07_test_pxc[1])`; *group x
congruence*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_mem_UNM07_test_sxc[1])`; *group x
predictiveness x congruence*:
`r apa(ANOVA_mem_UNM07_test, effect = "condition:predictiveness:congruence")`,
`r report_BF_and_error(bay_ANOVA_mem_UNM07_test_gxpxc[1])`. Simple main
effects showed a significant effect of *predictiveness* for group
Certain, *F*(`r sme_mem_UNM07_test_pred[1,3]`,
`r sme_mem_UNM07_test_pred[1,4]`) = `r sme_mem_UNM07_test_pred[1,5]`,
*p* = `r sme_mem_UNM07_test_pred[1,9]`, *η~p~^2^* =
`r sme_mem_UNM07_test_pred[1,8]`,
`r report_BF_and_error(bay_SME_UNM07_test_uncertain[1])`, but not for
group Uncertain, *F* $\le$ 1,
`r report_BF_and_error(bay_SME_UNM07_test_certain[1])`. Again, memory
score analysis suggests there was better memory for predictive cues than
for non-predictive cues in group Certain, whereas this difference was
not present in group Uncertain. The lack of a main effect of the group
suggest that overall memory was similar in both groups.

## Experiment 2

```{r, include=FALSE}
UNM08_test <- UNM08_test %>%
  mutate(trial_type = case_when((target == 1 & distractor == 2) | (target == 2 & distractor == 1) | (target == 3 & distractor == 4) | (target == 4 & distractor == 3) ~ "P-Con" ,
                                (target == 5 & distractor == 6) | (target == 6 & distractor == 5) |  (target == 7 & distractor == 8) | (target == 8 & distractor == 7)~ "NP-Con",
                                (target == 1 & (distractor == 5 | distractor == 6)) | (target == 2 & (distractor == 5 | distractor == 6)) | (target == 3 & (distractor == 7 | distractor == 8)) | (target == 4 & (distractor == 7 | distractor == 8)) ~ "P-Incon",
                                  (target == 5 & (distractor == 1 | distractor == 2)) | (target == 6 & (distractor == 1 | distractor == 2)) | (target == 7 & (distractor == 3 | distractor == 4)) | (target == 8 & (distractor == 3 | distractor == 4)) ~  "NP-Incon"),
         #add a congruence variable
         congruence = case_when ((trial_type == "P-Con") | (trial_type == "NP-Con") ~ "congruent",
                                 (trial_type == "P-Incon") | (trial_type == "NP-Incon") ~ "incongruent"))
```

```{r, include=FALSE}
#Calculate the mean accuracy and standard error for trial type, including the groups
MA_UNM08_test <- UNM08_test %>%
  group_by(trial_type, condition) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

@fig-acctestExp2_cong shows the accuracy results from the recognition
memory test of Experiment 2. In group Uncertain, accuracy was similar in
all trial types. In both Certain groups, there was lower accuracy in
non-predictive trials than in predictive trials, being this difference
larger for the incongruent trials than for the congruent trials. The
overall accuracy was higher in group Uncertain compared with the other
two Certain groups.

```{r, echo = FALSE, message=FALSE}
#| label: fig-acctestExp2_cong
#| fig-cap: Accuracy on the test phase of Experiment 2.
#| apa-note: "Mean accuracy (±SEM) during the test phase of Experiment 2, for groups Uncertain, Certain Long and Certain Short."
#| fig-height: 4
ggplot(data = MA_UNM08_test, mapping = aes(x = factor(condition, level=c('Uncertain', 'Certain Short','Certain Long')), y = mean_acc, fill = trial_type)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of test") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#7B3294", "#C2A5CF", "#008837", "#A6DBA0"))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA accuracy
acc_UNM08_test <- UNM08_test %>%
  group_by (pNum, condition, predictiveness, congruence) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_UNM08_test$predictiveness <- factor(acc_UNM08_test$predictiveness)
acc_UNM08_test$condition <- factor(acc_UNM08_test$condition)
acc_UNM08_test$pNum <- factor(acc_UNM08_test$pNum)
acc_UNM08_test$congruence <- factor(acc_UNM08_test$congruence)
ANOVA_acc_UNM08_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness*congruence), data = acc_UNM08_test)
print(ANOVA_acc_UNM08_test)

bay_ANOVA_acc_UNM08_test <- anovaBF(formula = acc ~ condition*predictiveness*congruence + pNum,
        data = data.frame(acc_UNM08_test),
        whichRandom = "pNum")
print(bay_ANOVA_acc_UNM08_test)

bay_ANOVA_acc_UNM08_test_gxp <- bay_ANOVA_acc_UNM08_test[4]/bay_ANOVA_acc_UNM08_test[3]
print(bay_ANOVA_acc_UNM08_test_gxp)
bay_ANOVA_acc_UNM08_test_pxc <- bay_ANOVA_acc_UNM08_test[13]/bay_ANOVA_acc_UNM08_test[7]
print(bay_ANOVA_acc_UNM08_test_pxc)
bay_ANOVA_acc_UNM08_test_sxc <- bay_ANOVA_acc_UNM08_test[10]/bay_ANOVA_acc_UNM08_test[6]
print(bay_ANOVA_acc_UNM08_test_sxc)
bay_ANOVA_acc_UNM08_test_gxpxc <- bay_ANOVA_acc_UNM08_test[18]/bay_ANOVA_acc_UNM08_test[17]
print(bay_ANOVA_acc_UNM08_test_gxpxc)
```

```{r, include = FALSE}
# SME of predictiveness:congruence interaction
SME_acc_UM08_pxc <- UNM08_test %>%
  group_by (pNum, congruence, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
#calculate the simple main effect of predictiveness
SME_acc_UM08_pxc_pred <- SME_acc_UM08_pxc %>%
  group_by(congruence) %>%
  anova_test(acc ~ predictiveness + Error(pNum/predictiveness), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
SME_acc_UM08_pxc_pred #Call the output table

SME_acc_UNM08_test_cong <- filter(UNM08_test, congruence == "congruent") %>%
  group_by(pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
SME_acc_UNM08_test_cong$predictiveness <- factor(SME_acc_UNM08_test_cong$predictiveness)
SME_acc_UNM08_test_cong$pNum <- factor(SME_acc_UNM08_test_cong$pNum)

SME_acc_UNM08_test_incong <- filter(UNM08_test, congruence == "incongruent") %>%
  group_by(pNum, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
SME_acc_UNM08_test_incong$predictiveness <- factor(SME_acc_UNM08_test_incong$predictiveness)
SME_acc_UNM08_test_incong$pNum <- factor(SME_acc_UNM08_test_incong$pNum)

bay_SME_acc_UNM08_test_cong <- anovaBF(formula = acc ~ predictiveness,
        data = data.frame(SME_acc_UNM08_test_cong),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_SME_acc_UNM08_test_cong)

bay_SME_acc_UNM08_test_incong <- anovaBF(formula = acc ~ predictiveness,
        data = data.frame(SME_acc_UNM08_test_incong),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_SME_acc_UNM08_test_incong)
```

A mixed model ANOVA showed a significant main effect of the *group*,
`r apa(ANOVA_acc_UNM08_test, effect = "condition")`,
`r report_BF_and_error(bay_ANOVA_acc_UNM08_test[1])`, and of the
*predictiveness*:
`r apa(ANOVA_acc_UNM08_test, effect = "predictiveness")`,
`r report_BF_and_error(bay_ANOVA_acc_UNM08_test[2])`, as well as a
significant *predictiveness x congruence* interaction,
`r apa(ANOVA_acc_UNM08_test, effect = "predictiveness:congruence")`,
`r report_BF_and_error(bay_ANOVA_acc_UNM08_test_pxc[1])`. All other
effects and interactions were non significant, *congruence*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_acc_UNM08_test[5])`; *group x
predictiveness* interaction,
`r apa(ANOVA_acc_UNM08_test, effect = "condition:predictiveness")`,
`r report_BF_and_error(bay_ANOVA_acc_UNM08_test_gxp[1])`; *group x
congruence*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_acc_UNM08_test_sxc[1])`; *group x
predictiveness x congruence*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_acc_UNM08_test_gxpxc[1])`. Simple main
effects showed an effect of *predictiveness* for the incongruent trials,
*F* (`r SME_acc_UM08_pxc_pred[2,3]`, `r SME_acc_UM08_pxc_pred[2,4]`) =
`r SME_acc_UM08_pxc_pred[2,5]`, *p* $\le$ 0.001, *η~p~^2^* =
`r SME_acc_UM08_pxc_pred[2,8]`,
`r report_BF_and_error(bay_SME_acc_UNM08_test_incong[1])`, but not for
the congruent trials, *F* (`r SME_acc_UM08_pxc_pred[1,3]`,
`r SME_acc_UM08_pxc_pred[1,4]`) = `r SME_acc_UM08_pxc_pred[1,5]`, *p* =
`r SME_acc_UM08_pxc_pred[1,9]`, *η~p~^2^* =
`r SME_acc_UM08_pxc_pred[1,8]`,
`r report_BF_and_error(bay_SME_acc_UNM08_test_cong[1])`.

```{r, include=FALSE}
#plot test mem_score
M_mem_UNM08_test <- UNM08_test %>%
  group_by(trial_type, condition) %>%
  summarise(mean_mem_score = mean(c_mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

@fig-testExp2_cong shows the memory scores from the recognition memory
test of Experiment 2. In group Uncertain, accuracy was similar in all
trial types. In both Certain groups, there was lower accuracy in
non-predictive trials than in predictive trials, being this difference
similar for the incongruent trials than for the congruent trials. The
overall accuracy was higher in group Uncertain compared with the other
two Certain groups.

```{r, echo = FALSE, message=FALSE}
#| label: fig-testExp2_cong
#| fig-cap: Memory score on the test phase of Experiment 2.
#| apa-note: "Mean memory scores (±SEM) during the test phase of Experiment 2, for groups Uncertain, Certain Long and Certain Short."
#| fig-height: 4
ggplot(data = M_mem_UNM08_test, mapping = aes(x = factor(condition, levels = c("Uncertain", "Certain Long", "Certain Short")), y = mean_mem_score, fill = trial_type)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_y_continuous(name = "Memory Score") +
  scale_fill_discrete(type = c("#7B3294", "#C2A5CF", "#008837", "#A6DBA0"))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA mem_score
mem_UNM08_test <- UNM08_test %>%
  group_by (pNum, condition, predictiveness, congruence) %>%
  summarise(mem = mean(c_mem_score, na.rm = TRUE))
mem_UNM08_test$predictiveness <- factor(mem_UNM08_test$predictiveness)
mem_UNM08_test$condition <- factor(mem_UNM08_test$condition)
mem_UNM08_test$pNum <- factor(mem_UNM08_test$pNum)
mem_UNM08_test$congruence <- factor(mem_UNM08_test$congruence)
ANOVA_mem_UNM08_test <- aov_car(formula = mem ~ condition + Error(pNum*predictiveness*congruence), data = mem_UNM08_test)
print(ANOVA_mem_UNM08_test)
bay_ANOVA_mem_UNM08_test <- anovaBF(formula = mem ~ condition*predictiveness*congruence + pNum,
        data = data.frame(mem_UNM08_test),
        whichRandom = "pNum")
print(bay_ANOVA_mem_UNM08_test)

bay_ANOVA_mem_UNM08_test_gxp <- bay_ANOVA_mem_UNM08_test[4]/bay_ANOVA_mem_UNM08_test[3]
print(bay_ANOVA_mem_UNM08_test_gxp)
bay_ANOVA_mem_UNM08_test_pxc <- bay_ANOVA_mem_UNM08_test[13]/bay_ANOVA_mem_UNM08_test[7]
print(bay_ANOVA_mem_UNM08_test_pxc)
bay_ANOVA_mem_UNM08_test_sxc <- bay_ANOVA_mem_UNM08_test[10]/bay_ANOVA_mem_UNM08_test[6]
print(bay_ANOVA_mem_UNM08_test_sxc)
bay_ANOVA_mem_UNM08_test_gxpxc <- bay_ANOVA_mem_UNM08_test[18]/bay_ANOVA_mem_UNM08_test[17]
print(bay_ANOVA_mem_UNM08_test_gxpxc)
```

```{r, include = FALSE}
# SME of the predictiveness:congruence interaction
SME_mem_UNM08_test <- UNM08_test %>%
  group_by (pNum, congruence, predictiveness) %>%
  summarise(mem = mean(c_mem_score, na.rm = TRUE))
#calculate the simple main effect of predictiveness
sme_mem_UNM08_test_predxc <- SME_mem_UNM08_test %>%
  group_by(congruence) %>%
  anova_test(mem ~ predictiveness + Error(pNum/predictiveness), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_mem_UNM08_test_predxc #Call the output table

SME_UNM08_test_cong <- filter(UNM08_test, congruence == "congruent") %>%
  group_by(pNum, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
SME_UNM08_test_cong$predictiveness <- factor(SME_UNM08_test_cong$predictiveness)
SME_UNM08_test_cong$pNum <- factor(SME_UNM08_test_cong$pNum)

SME_UNM08_test_incong <- filter(UNM08_test, congruence == "incongruent") %>%
  group_by(pNum, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
SME_UNM08_test_incong$predictiveness <- factor(SME_UNM08_test_incong$predictiveness)
SME_UNM08_test_incong$pNum <- factor(SME_UNM08_test_incong$pNum)

bay_SME_UNM08_test_cong <- anovaBF(formula = mem_score ~ predictiveness,
        data = data.frame(SME_UNM08_test_cong),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_SME_UNM08_test_cong)

bay_SME_UNM08_test_incong <- anovaBF(formula = mem_score ~ predictiveness,
        data = data.frame(SME_UNM08_test_incong),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_SME_UNM08_test_incong)
```

A mixed model ANOVA found a significant main effect of the *group*,
`r apa(ANOVA_mem_UNM08_test, effect = "condition")`,
`r report_BF_and_error(bay_ANOVA_mem_UNM08_test[1])`, and of the
*predictiveness*,
`r apa(ANOVA_mem_UNM08_test, effect = "predictiveness")`,
`r report_BF_and_error(bay_ANOVA_mem_UNM08_test[2])`, as well as a
significant *group x predictiveness* interaction,
`r apa(ANOVA_mem_UNM08_test, effect = "condition:predictiveness")`,
`r report_BF_and_error(bay_ANOVA_mem_UNM08_test_gxp[1])` and a
significant *predictiveness x congruence* interaction,
`r apa(ANOVA_mem_UNM08_test, effect = "predictiveness:congruence")`,
`r report_BF_and_error(bay_ANOVA_mem_UNM08_test_pxc[1])`. No other main
effects or interactions were significant, *congruence*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_mem_UNM08_test[5])`; *group x
congruence*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_mem_UNM08_test_sxc[1])`; *group x
predictiveness x congruence*: *F* $\le$ 1,
`r report_BF_and_error(bay_ANOVA_mem_UNM08_test_gxpxc[1])`. Simple main
effects showed a significant effect of *predictiveness* for group
Certain Long, *F* (`r sme_UNM08_test_pred[1, 3]`,
`r sme_UNM08_test_pred[1, 4]`) = `r sme_UNM08_test_pred[1, 5]`, *p* $\le$
0.001, *η~p~^2^* = `r sme_UNM08_test_pred[1, 8]`,
`r report_BF_and_error(bay_SME_UNM08_test_CL[1])`, and for group Certain
Short, *F* (`r sme_UNM08_test_pred[2, 3]`,
`r sme_UNM08_test_pred[2, 4]`) = `r sme_UNM08_test_pred[2, 5]`, *p* $\le$
0.001, *η~p~^2^* = `r sme_UNM08_test_pred[2, 8]`,
`r report_BF_and_error(bay_SME_UNM08_test_CS[1])`, but not for group
Uncertain, *F* (`r sme_UNM08_test_pred[3, 3]`,
`r sme_UNM08_test_pred[3, 4]`) = `r sme_UNM08_test_pred[3, 5]`, *p* =
`r sme_UNM08_test_pred[3, 9]`, *η~p~^2^* =
`r sme_UNM08_test_pred[3, 8]`,
`r report_BF_and_error(bay_SME_UNM08_test_U[1])`. Also, there was a
significant effect of *predictiveness* for both incongruent trials,
*F*(`r sme_mem_UNM08_test_predxc[2,3]`,
`r sme_mem_UNM08_test_predxc[2,4]`) =
`r sme_mem_UNM08_test_predxc[2,5]`, *p* $\le$ 0.001, *η~p~^2^* =
`r sme_mem_UNM08_test_predxc[2,8]`,
`r report_BF_and_error(bay_SME_UNM08_test_incong[1])`, and congruent
trials, *F*(`r sme_mem_UNM08_test_predxc[1,3]`,
`r sme_mem_UNM08_test_predxc[1,4]`) =
`r sme_mem_UNM08_test_predxc[1,5]`, *p* $\le$ 0.001, *η~p~^2^* =
`r sme_mem_UNM08_test_predxc[1,8]`,
`r report_BF_and_error(bay_SME_UNM08_test_cong[1])`.
