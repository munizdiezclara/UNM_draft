---
title: "Untitled"
format: docx
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(afex)
library(BayesFactor)
library(apa)
library(emmeans)
library(rstatix)
library("writexl")

# function to force scientific formatting of numbers (used for large BFs)
changeSciNot <- function(n) {
  output <- format(n, scientific = TRUE, digits = 2) #Transforms the number into scientific notation even if small
  output <- sub("e", "x10^", output) #Replace e with 10^
  output <- sub("\\+0?", "", output) #Remove + symbol and leading zeros on exponent, if > 1
  output <- sub("-0?", "-", output) #Leaves - symbol but removes leading zeros on exponent, if < 1
  output <- paste0(output,"^")
  # output = strsplit(output, "^", fixed = TRUE)
  # output = paste0(output[[1]][1],"^", output[[1]][2], "^")
  output
}
# function to extract and report BFs with error %s
report_BF_and_error <- function(BF_in, sci_not = FALSE, hyp = "alt"){
  
  if (hyp == "alt") {
    BF_notation = "BF~10~ = "
  } else if (hyp == "null") {
    BF_notation = "BF~01~ = "
  }
  
  if (sci_not == TRUE) {
    BF_value = changeSciNot(extractBF(BF_in)$bf) # change to sci notation
  } else {
    BF_value = round(extractBF(BF_in)$bf,2) # otherwise round
  }
  
  paste0(BF_notation, 
         BF_value, 
         " &plusmn; ", 
         round(100*extractBF(BF_in)$error,2), 
         "%")
}
```

# Exp 1

```{r, include=FALSE}
load("../../UNM05_proc_data.RData")
#Clean participants that did not pass check 2 and/or 3
training <- filter(training, !pNum %in% not_passed_pNum)
test1 <- filter(test1, !pNum %in% not_passed_pNum)
test2 <- filter(test2, !pNum %in% not_passed_pNum)
```

## Test1

### Accuracy

```{r, include=FALSE}
#add a group variable
test1 <- test1 %>%
  mutate(group = case_when(session == 1 ~ "High",
                                session == 2 ~ "Medium",
                                session == 3 ~ "Low"))
#Calculate the mean accuracy and standard error for each block, including the groups
MA_test1 <- test1 %>%
  group_by(predictiveness, group) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

```{r, echo=FALSE, message=FALSE}
ggplot(data = MA_test1, mapping = aes(x = factor(group, levels = c("High", "Medium", "Low")), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of test") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean accuracy for each type of cue in test1 phase")
```

```{r, include=FALSE}
#ANOVA accuracy
acc_test1 <- test1 %>%
  group_by (pNum, group, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_test1$predictiveness <- factor(acc_test1$predictiveness)
acc_test1$group <- factor(acc_test1$group)
acc_test1$pNum <- factor(acc_test1$pNum)
ANOVA_acc_test1 <- aov_car(formula = acc ~ group + Error(pNum*predictiveness), data = acc_test1)
print(ANOVA_acc_test1)
```

```{r, include=FALSE}
bay_ANOVA_acc_test1 <- anovaBF(formula = acc ~ group*predictiveness + pNum,
        data = data.frame(acc_test1),
        whichRandom = "pNum")
print(bay_ANOVA_acc_test1)
```

```{r, include=FALSE}
bay_ANOVA_acc_test1_int <- bay_ANOVA_acc_test1[4]/bay_ANOVA_acc_test1[3]
print(bay_ANOVA_acc_test1_int)
```

Except for those that did the very subtle test, all subjects had lower accuracy for the non predictive vs the predictive targets. However, there are no significant effects: *similarity* : `r apa(ANOVA_acc_test1, effect = "group")`, `r report_BF_and_error(bay_ANOVA_acc_test1[1])`; *predictiveness*: `r apa(ANOVA_acc_test1, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_test1[2])`; *similarity x predictiveness*: `r apa(ANOVA_acc_test1, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_test1[1])`.

###Just correct ratings

```{r, include=FALSE}
#plot test mem_score but take out the errors
M_rat_test1 <- filter(test1, acc == 1) %>%
  group_by(predictiveness, group) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

```{r, echo=FALSE, message=FALSE}
ggplot(data = M_rat_test1, mapping = aes(x = factor(group, levels = c("High", "Medium", "Low")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Ratings") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean corrected memory score for each type of cue in test1 phase")
```

```{r, include=FALSE}
#ANOVA mem_score
rat_test1 <- filter(test1, acc == 1) %>%
  group_by (pNum, group, predictiveness) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_test1$predictiveness <- factor(rat_test1$predictiveness)
rat_test1$group <- factor(rat_test1$group)
rat_test1$pNum <- factor(rat_test1$pNum)
ANOVA_rat_test1 <- aov_car(formula = rat ~ group + Error(pNum*predictiveness), data = rat_test1)
print(ANOVA_rat_test1)
```

```{r, include=FALSE}
bay_ANOVA_rat_test1 <- anovaBF(formula = rat ~ group*predictiveness + pNum,
        data = data.frame(rat_test1),
        whichRandom = "pNum")
print(bay_ANOVA_rat_test1)
```

```{r, include=FALSE}
bay_ANOVA_rat_test1_int <- bay_ANOVA_rat_test1[4]/bay_ANOVA_rat_test1[3]
```

In this first test, there was a significant effect of the predictiveness, `r apa(ANOVA_rat_test1, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test1[2])`, but not of the *similarity*, `r apa(ANOVA_rat_test1, effect = "group")`, `r report_BF_and_error(bay_ANOVA_rat_test1[1])`, nor of the *similarity x predictiveness* interaction, `r apa(ANOVA_rat_test1, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test1[1])`.

##Test2 

### Accuracy

```{r, include=FALSE}
#add a group variable
test2 <- test2 %>%
  mutate(group = case_when(session == 1 ~ "High",
                                session == 2 ~ "Medium",
                                session == 3 ~ "Low"), 
         trial_type = case_when((target == 1 & distractor_test2 == 2) | (target == 2 & distractor_test2 == 1) ~ "P-Con" ,
                                (target == 5 & distractor_test2 == 6) | (target == 6 & distractor_test2 == 5) ~ "NP-Con",
                                (target == 1 & (distractor_test2 == 5 | distractor_test2 == 6)) | (target == 2 & (distractor_test2 == 5 | distractor_test2 == 6)) ~ "P-Incon",
                                  (target == 5 & (distractor_test2 == 1 | distractor_test2 == 2)) | (target == 6 & (distractor_test2 == 1 | distractor_test2 == 2)) ~  "NP-Incon"),
         #add a congruence variable
         congruence = case_when ((trial_type == "P-Con") | (trial_type == "NP-Con") ~ "congruent",
                                 (trial_type == "P-Incon") | (trial_type == "NP-Incon") ~ "incongruent"))
test2 <- filter(test2, congruence == "congruent") #filter just the congruent trials
#plot test accuracy
m_acc_test2 <- test2 %>%
  group_by(predictiveness, group) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

```{r, echo = FALSE, message=FALSE}
ggplot(data = m_acc_test2, mapping = aes(x = factor(group, levels = c("High", "Medium", "Low")), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Rating") +
  coord_cartesian(ylim = c(0.5, 1)) +
  labs(fill = "Trial type")+
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean accuracy for each type of cue in test2 phase")
```

```{r, include=FALSE}
#ANOVA accuracy
acc_test2 <- test2 %>%
  group_by (pNum, group, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_test2$predictiveness <- factor(acc_test2$predictiveness)
acc_test2$group <- factor(acc_test2$group)
acc_test2$pNum <- factor(acc_test2$pNum)
ANOVA_acc_test2 <- aov_car(formula = acc ~ group + Error(pNum*predictiveness), data = acc_test2)
print(ANOVA_acc_test2)

bay_ANOVA_acc_test2 <- anovaBF(formula = acc ~ group*predictiveness + pNum,
        data = data.frame(acc_test2),
        whichRandom = "pNum")
print(bay_ANOVA_acc_test2)

bay_ANOVA_acc_test2_sxp <- bay_ANOVA_acc_test2[4]/bay_ANOVA_acc_test2[3]
print(bay_ANOVA_acc_test2_sxp)
```

There are no significant differences due to any effect or interaction in accuracy in the second test, *similarity* : `r apa(ANOVA_acc_test2, effect = "group")`, `r report_BF_and_error(bay_ANOVA_acc_test2[1])`; *predictiveness*: `r apa(ANOVA_acc_test2, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_test2[2])`; \*\*similarity x predictiveness\*: `r apa(ANOVA_acc_test2, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_test2_sxp[1])`.

###Just correct ratings

```{r, include=FALSE}
#plot test accuracy
m_rat_test2 <- filter(test2, acc == 1) %>%
  group_by(trial_type, group) %>%
  summarise(mean_rat = mean(mem_score, na.rm = TRUE), 
            se_rat = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

```{r, echo = FALSE, message=FALSE}
ggplot(data = m_rat_test2, mapping = aes(x = factor(group, levels = c("High", "Medium", "Low")), y = mean_rat, fill = trial_type)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_rat - se_rat, ymax = mean_rat + se_rat), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Rating") +
  labs(fill = "Trial type")+
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean rating for each type of cue in test2 phase")
```

```{r, include=FALSE}
#ANOVA rat
rat_test2 <- filter(test2, acc ==1) %>%
  group_by (pNum, group, predictiveness) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_test2$predictiveness <- factor(rat_test2$predictiveness)
rat_test2$group <- factor(rat_test2$group)
rat_test2$pNum <- factor(rat_test2$pNum)
ANOVA_rat_test2 <- aov_car(formula = rat ~ group + Error(pNum*predictiveness), data = rat_test2)
print(ANOVA_rat_test2)

bay_ANOVA_rat_test2 <- anovaBF(formula = rat ~ group*predictiveness + pNum,
        data = data.frame(rat_test2),
        whichRandom = "pNum")
print(bay_ANOVA_rat_test2)

bay_ANOVA_rat_test2_sxp <- bay_ANOVA_rat_test2[4]/bay_ANOVA_rat_test2[3]
print(bay_ANOVA_rat_test2_sxp)
```

When the confidence ratings for the correct options were analysed, there was a clear effect of predictiveness, *predictiveness*: `r apa(ANOVA_rat_test2, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test2[2])`. The rest of the main effects and interactions were not significant: *similarity* : `r apa(ANOVA_rat_test2, effect = "group")`, `r report_BF_and_error(bay_ANOVA_rat_test2[1])`; *similarity x predictiveness*: `r apa(ANOVA_rat_test2, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test2_sxp[1])`.

#### Group Low

```{r, include = FALSE}
rat_test2_L <- filter(test2, acc == 1 & group == "Low") %>%
  group_by (pNum, predictiveness) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
d <- with(rat_test2_L,
          rat[predictiveness == "non-predictive"] - rat[predictiveness == "predictive"])
shapiro.test(d) #test the normality
t.test_rat_test2_L <- t.test(rat ~ predictiveness, data = rat_test2_L, paired = TRUE) #t.test
print(t.test_rat_test2_L)
#bayesian ANOVA
pred_rat_test2_L <- subset(rat_test2_L,  predictiveness == "predictive", rat, drop = TRUE)
nonpred_rat_test2_L <- subset(rat_test2_L,  predictiveness == "non-predictive", rat, drop = TRUE)
bay_t.test_rat_test2_L <-  ttestBF(pred_rat_test2_L, nonpred_rat_test2_L, paired = TRUE)
print(bay_t.test_rat_test2_L)
```

T test showed no differences in predictiveness in group Low, `r apa(t.test_rat_test2_L)`, `r report_BF_and_error(bay_t.test_rat_test2_L[1])`.

#### Group Medium

```{r, include = FALSE}
rat_test2_M <- filter(test2, acc == 1 & group == "Medium") %>%
  group_by (pNum, predictiveness) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_test2_M <- rat_test2_M %>%
  group_by(pNum) %>%
  filter(n() == 2) %>%
  ungroup()
d <- with(rat_test2_M,
          rat[predictiveness == "non-predictive"] - rat[predictiveness == "predictive"])
shapiro.test(d) #test the normality
t.test_rat_test2_M <- t.test(rat ~ predictiveness, data = rat_test2_M, paired = TRUE) #t.test
print(t.test_rat_test2_M)
#bayesian ANOVA
pred_rat_test2_M <- subset(rat_test2_M,  predictiveness == "predictive", rat, drop = TRUE)
nonpred_rat_test2_M <- subset(rat_test2_M,  predictiveness == "non-predictive", rat, drop = TRUE)
bay_t.test_rat_test2_M <-  ttestBF(pred_rat_test2_M, nonpred_rat_test2_M, paired = TRUE)
print(bay_t.test_rat_test2_M)
```

For group Medium, there was also no effect of *predictiveness*, `r apa(t.test_rat_test2_M)`, `r report_BF_and_error(bay_t.test_rat_test2_M[1])`.

#### Group High

```{r, include = FALSE}
rat_test2_H <- filter(test2, acc == 1 & group == "High") %>%
  group_by (pNum, predictiveness) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
d <- with(rat_test2_H,
          rat[predictiveness == "non-predictive"] - rat[predictiveness == "predictive"])
shapiro.test(d) #test the normality
t.test_rat_test2_H <- t.test(rat ~ predictiveness, data = rat_test2_H, paired = TRUE)
print(t.test_rat_test2_H)
#bayesian ANOVA
pred_rat_test2_H <- subset(rat_test2_H,  predictiveness == "predictive", rat, drop = TRUE)
nonpred_rat_test2_H <- subset(rat_test2_H,  predictiveness == "non-predictive", rat, drop = TRUE)
bay_t.test_rat_test2_H <-  ttestBF(pred_rat_test2_H, nonpred_rat_test2_H, paired = TRUE)
print(bay_t.test_rat_test2_H)
```

For group High there was a significant effect of *predictiveness*, `r apa(t.test_rat_test2_H)`, `r report_BF_and_error(bay_t.test_rat_test2_H[1])`.

# Exp 2

## Accuracy

```{r, include=FALSE}
load("../../UNM07_proc_data.RData")
UNM07_training <- filter(training, !pNum %in% not_passed_pNum$pNum)
UNM07_test <- filter(test, !pNum %in% not_passed_pNum$pNum)

#create the PPR measure
UNM07_training <- UNM07_training %>%
  mutate(prob_response = case_when((cue1 == 1 | cue1 == 3) & response == "o1_image" ~ 1,
                                   (cue1 == 1 | cue1 == 3) & response == "o2_image" ~ 0, 
                                   (cue1 == 2 | cue1 == 4) & response == "o1_image" ~ 0,
                                   (cue1 == 2 | cue1 == 4) & response == "o2_image" ~ 1))
         
UNM07_test <- UNM07_test %>%
  mutate(trial_type = case_when((target == 1 & distractor == 2) | (target == 2 & distractor == 1) | (target == 3 & distractor == 4) | (target == 4 & distractor == 3) ~ "P-Con" ,
                                (target == 5 & distractor == 6) | (target == 6 & distractor == 5) |  (target == 7 & distractor == 8) | (target == 8 & distractor == 7)~ "NP-Con",
                                (target == 1 & (distractor == 5 | distractor == 6)) | (target == 2 & (distractor == 5 | distractor == 6)) | (target == 3 & (distractor == 7 | distractor == 8)) | (target == 4 & (distractor == 7 | distractor == 8)) ~ "P-Incon",
                                  (target == 5 & (distractor == 1 | distractor == 2)) | (target == 6 & (distractor == 1 | distractor == 2)) | (target == 7 & (distractor == 3 | distractor == 4)) | (target == 8 & (distractor == 3 | distractor == 4)) ~  "NP-Incon"),
         #add a congruence variable
         congruence = case_when ((trial_type == "P-Con") | (trial_type == "NP-Con") ~ "congruent",
                                 (trial_type == "P-Incon") | (trial_type == "NP-Incon") ~ "incongruent"))
UNM07_test <- filter(UNM07_test, congruence == "congruent")
```

```{r, include=FALSE}
#Calculate the mean accuracy and standard error for each block, including the groups
MA_test <- UNM07_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

```{r, echo=FALSE, message=FALSE}
ggplot(data = MA_test, mapping = aes(x = factor(condition, level=c('Uncertain', 'Certain')), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of test") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean accuracy for each type of cue in test phase")
```

```{r, include=FALSE}
#ANOVA accuracy
acc_UNM07_test <- UNM07_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_UNM07_test$predictiveness <- factor(acc_UNM07_test$predictiveness)
acc_UNM07_test$condition <- factor(acc_UNM07_test$condition)
acc_UNM07_test$pNum <- factor(acc_UNM07_test$pNum)
ANOVA_acc_UNM07_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness), data = acc_UNM07_test)
print(ANOVA_acc_UNM07_test)

bay_ANOVA_acc_UNM07_test <- anovaBF(formula = acc ~ condition*predictiveness + pNum,
        data = data.frame(acc_UNM07_test),
        whichRandom = "pNum")
print(bay_ANOVA_acc_UNM07_test)

bay_ANOVA_acc_UNM07_test_int <- bay_ANOVA_acc_UNM07_test[4]/bay_ANOVA_acc_UNM07_test[3]
print(bay_ANOVA_acc_UNM07_test_int)
```

There were no significant differences due to the main effect of *group*, `r apa(ANOVA_acc_UNM07_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_acc_UNM07_test[1])`, nor of *predictiveness*: `r apa(ANOVA_acc_UNM07_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM07_test[2])`, but there was a significant *group x predictiveness* interaction, `r apa(ANOVA_acc_UNM07_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM07_test_int[1])`.

## Just correct ratings

```{r, include=FALSE}
#plot test mem_score but take out the errors
M_rat_UNM07_test <- filter(UNM07_test, acc == 1) %>%
  group_by(predictiveness, condition) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

```{r, echo=FALSE, message=FALSE}
ggplot(data = M_rat_UNM07_test, mapping = aes(x = factor(condition, levels = c("Uncertain", "Certain")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Ratings") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean ratings for each type of cue in UNM07_test phase")
```

```{r, include=FALSE}
#ANOVA mem_score
rat_UNM07_test <- filter(UNM07_test, acc == 1) %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_UNM07_test$predictiveness <- factor(rat_UNM07_test$predictiveness)
rat_UNM07_test$condition <- factor(rat_UNM07_test$condition)
rat_UNM07_test$pNum <- factor(rat_UNM07_test$pNum)
ANOVA_rat_UNM07_test <- aov_car(formula = rat ~ condition + Error(pNum*predictiveness), data = rat_UNM07_test)
print(ANOVA_rat_UNM07_test)
```

```{r, include=FALSE}
bay_ANOVA_rat_UNM07_test <- anovaBF(formula = rat ~ condition*predictiveness + pNum,
        data = data.frame(rat_UNM07_test),
        whichRandom = "pNum")
print(bay_ANOVA_rat_UNM07_test)
```

```{r, include=FALSE}
bay_ANOVA_rat_UNM07_test_int <- bay_ANOVA_rat_UNM07_test[4]/bay_ANOVA_rat_UNM07_test[3]
```

There was no significant effect of the *group*, `r apa(ANOVA_rat_UNM07_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_rat_UNM07_test[1])`, nor *predictiveness*, `r apa(ANOVA_rat_UNM07_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_UNM07_test[2])`, neither of the *group x predictiveness* interaction, `r apa(ANOVA_rat_UNM07_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_UNM07_test[1])`. It is worth mentioning that the effect of predictiveness is very close to significance and that the bayesian evidence is anecdotal.

# Exp 3

## Accuracy

```{r, include=FALSE}
load("../../UNM08_proc_data.RData")
UNM08_training <- rbind(stage1, stage2)
UNM08_training <- filter(UNM08_training, !pNum %in% not_passed_pNum$pNum)
UNM08_test <- filter(test, !pNum %in% not_passed_pNum$pNum)

UNM08_test <- UNM08_test %>%
  mutate(trial_type = case_when((target == 1 & distractor == 2) | (target == 2 & distractor == 1) ~ "P-Con" ,
                                (target == 5 & distractor == 6) | (target == 6 & distractor == 5) ~ "NP-Con",
                                (target == 1 & (distractor == 5 | distractor == 6)) | (target == 2 & (distractor == 5 | distractor == 6)) ~ "P-Incon",
                                  (target == 5 & (distractor == 1 | distractor == 2)) | (target == 6 & (distractor == 1 | distractor == 2)) ~  "NP-Incon"),
         #add a congruence variable
         congruence = case_when ((trial_type == "P-Con") | (trial_type == "NP-Con") ~ "congruent",
                                 (trial_type == "P-Incon") | (trial_type == "NP-Incon") ~ "incongruent"))

#create the PPR measure
UNM08_training <- UNM08_training %>%
  mutate(prob_response = case_when((cue1 == 1 | cue1 == 3) & response == "o1_image" ~ 1,
                                   (cue1 == 1 | cue1 == 3) & response == "o2_image" ~ 0, 
                                   (cue1 == 2 | cue1 == 4) & response == "o1_image" ~ 0,
                                   (cue1 == 2 | cue1 == 4) & response == "o2_image" ~ 1))

UNM08_test <- filter(UNM08_test, congruence == "congruent")
#Calculate the mean accuracy and standard error for each block, including the groups
MA_UNM08_test <- UNM08_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

```{r, echo=FALSE, message=FALSE}
ggplot(data = MA_UNM08_test, mapping = aes(x = factor(condition, level=c('Uncertain', 'Certain Short','Certain Long')), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of test") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean accuracy for each type of cue in test phase")
```

```{r, include=FALSE}
#ANOVA accuracy
acc_UNM08_test <- UNM08_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_UNM08_test$predictiveness <- factor(acc_UNM08_test$predictiveness)
acc_UNM08_test$condition <- factor(acc_UNM08_test$condition)
acc_UNM08_test$pNum <- factor(acc_UNM08_test$pNum)
ANOVA_acc_UNM08_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness), data = acc_UNM08_test)
print(ANOVA_acc_UNM08_test)

bay_ANOVA_acc_UNM08_test <- anovaBF(formula = acc ~ condition*predictiveness + pNum,
        data = data.frame(acc_UNM08_test),
        whichRandom = "pNum")
print(bay_ANOVA_acc_UNM08_test)

bay_ANOVA_acc_UNM08_test_int <- bay_ANOVA_acc_UNM08_test[4]/bay_ANOVA_acc_UNM08_test[3]
print(bay_ANOVA_acc_UNM08_test_int)
```

There were no significant differences, *group*: `r apa(ANOVA_acc_UNM08_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_acc_UNM08_test[1])`, *predictiveness*: `r apa(ANOVA_acc_UNM08_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM08_test[2])`, *group x predictiveness*: `r apa(ANOVA_acc_UNM08_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM08_test[1])`.

## Just correct ratings

```{r, include=FALSE}
#plot test mem_score but take out the errors
M_rat_UNM08_test <- filter(UNM08_test, acc == 1) %>%
  group_by(predictiveness, condition) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

```{r, echo=FALSE, message=FALSE}
ggplot(data = M_rat_UNM08_test, mapping = aes(x = factor(condition, levels = c("Uncertain", "Certain Short", "Certain Long")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Ratings") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean ratings for each type of cue in test phase")
```

```{r, include=FALSE}
#ANOVA mem_score
rat_UNM08_test <- filter(UNM08_test, acc == 1) %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_UNM08_test$predictiveness <- factor(rat_UNM08_test$predictiveness)
rat_UNM08_test$condition <- factor(rat_UNM08_test$condition)
rat_UNM08_test$pNum <- factor(rat_UNM08_test$pNum)
ANOVA_rat_UNM08_test <- aov_car(formula = rat ~ condition + Error(pNum*predictiveness), data = rat_UNM08_test)
print(ANOVA_rat_UNM08_test)
```

```{r, include=FALSE}
bay_ANOVA_rat_UNM08_test <- anovaBF(formula = rat ~ condition*predictiveness + pNum,
        data = data.frame(rat_UNM08_test),
        whichRandom = "pNum")
print(bay_ANOVA_rat_UNM08_test)
```

```{r, include=FALSE}
bay_ANOVA_rat_UNM08_test_int <- bay_ANOVA_rat_UNM08_test[4]/bay_ANOVA_rat_UNM08_test[3]
```

There was a significant effect *predictiveness*, `r apa(ANOVA_rat_UNM08_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_UNM08_test[2])`, but not of the *group*, `r apa(ANOVA_rat_UNM08_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_rat_UNM08_test[1])`, nor of the *group x predictiveness* interaction, `r apa(ANOVA_rat_UNM08_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_UNM08_test_int[1])`.
