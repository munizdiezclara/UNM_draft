---
title: "Untitled"
format: docx
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(afex)
library(BayesFactor)
library(apa)
library(emmeans)
library(rstatix)
library("writexl")

# function to force scientific formatting of numbers (used for large BFs)
changeSciNot <- function(n) {
  output <- format(n, scientific = TRUE, digits = 2) #Transforms the number into scientific notation even if small
  output <- sub("e", "x10^", output) #Replace e with 10^
  output <- sub("\\+0?", "", output) #Remove + symbol and leading zeros on exponent, if > 1
  output <- sub("-0?", "-", output) #Leaves - symbol but removes leading zeros on exponent, if < 1
  output <- paste0(output,"^")
  # output = strsplit(output, "^", fixed = TRUE)
  # output = paste0(output[[1]][1],"^", output[[1]][2], "^")
  output
}
# function to extract and report BFs with error %s
report_BF_and_error <- function(BF_in, sci_not = FALSE, hyp = "alt"){
  
  if (hyp == "alt") {
    BF_notation = "BF~10~ = "
  } else if (hyp == "null") {
    BF_notation = "BF~01~ = "
  }
  
  if (sci_not == TRUE) {
    BF_value = changeSciNot(extractBF(BF_in)$bf) # change to sci notation
  } else {
    BF_value = round(extractBF(BF_in)$bf,2) # otherwise round
  }
  
  paste0(BF_notation, 
         BF_value, 
         " &plusmn; ", 
         round(100*extractBF(BF_in)$error,2), 
         "%")
}
```

# Exp 1

```{r, include=FALSE}
load("UNM05_proc_data.RData")
#Clean participants that did not pass check 2 and/or 3
training <- filter(training, !pNum %in% not_passed_pNum)
test1 <- filter(test1, !pNum %in% not_passed_pNum)
test2 <- filter(test2, !pNum %in% not_passed_pNum)
```

## Test1

### Accuracy

```{r, include=FALSE}
#add a group variable
test1 <- test1 %>%
  mutate(group = case_when(session == 1 ~ "High",
                                session == 2 ~ "Medium",
                                session == 3 ~ "Low"))
#Calculate the mean accuracy and standard error for each block, including the groups
MA_test1 <- test1 %>%
  group_by(predictiveness, group) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

```{r, echo=FALSE}
ggplot(data = MA_test1, mapping = aes(x = factor(group, levels = c("High", "Medium", "Low")), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of test") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean accuracy for each type of cue in test1 phase")
```

```{r, include=FALSE}
#ANOVA accuracy
acc_test1 <- test1 %>%
  group_by (pNum, group, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_test1$predictiveness <- factor(acc_test1$predictiveness)
acc_test1$group <- factor(acc_test1$group)
acc_test1$pNum <- factor(acc_test1$pNum)
ANOVA_acc_test1 <- aov_car(formula = acc ~ group + Error(pNum*predictiveness), data = acc_test1)
print(ANOVA_acc_test1)
```

```{r, include=FALSE}
bay_ANOVA_acc_test1 <- anovaBF(formula = acc ~ group*predictiveness + pNum,
        data = data.frame(acc_test1),
        whichRandom = "pNum")
print(bay_ANOVA_acc_test1)
```

```{r, include=FALSE}
bay_ANOVA_acc_test1_int <- bay_ANOVA_acc_test1[4]/bay_ANOVA_acc_test1[3]
print(bay_ANOVA_acc_test1_int)
```

Except for those that did the very subtle test, all subjects had lower accuracy for the non predictive vs the predictive targets. However, there are no significant effects: *similarity* : `r apa(ANOVA_acc_test1, effect = "group")`, `r report_BF_and_error(bay_ANOVA_acc_test1[1])`; *predictiveness*: `r apa(ANOVA_acc_test1, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_test1[2])`; *similarity x predictiveness*: `r apa(ANOVA_acc_test1, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_test1[1])`.

###Just correct ratings

```{r, include=FALSE}
#plot test mem_score but take out the errors
M_rat_test1 <- filter(test1, acc == 1) %>%
  group_by(predictiveness, group) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

```{r, echo=FALSE}
ggplot(data = M_rat_test1, mapping = aes(x = factor(group, levels = c("High", "Medium", "Low")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Ratings") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean corrected memory score for each type of cue in test1 phase")
```

```{r, include=FALSE}
#ANOVA mem_score
rat_test1 <- filter(test1, acc == 1) %>%
  group_by (pNum, group, predictiveness) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_test1$predictiveness <- factor(rat_test1$predictiveness)
rat_test1$group <- factor(rat_test1$group)
rat_test1$pNum <- factor(rat_test1$pNum)
ANOVA_rat_test1 <- aov_car(formula = rat ~ group + Error(pNum*predictiveness), data = rat_test1)
print(ANOVA_rat_test1)
```

```{r, include=FALSE}
bay_ANOVA_rat_test1 <- anovaBF(formula = rat ~ group*predictiveness + pNum,
        data = data.frame(rat_test1),
        whichRandom = "pNum")
print(bay_ANOVA_rat_test1)
```

```{r, include=FALSE}
bay_ANOVA_rat_test1_int <- bay_ANOVA_rat_test1[4]/bay_ANOVA_rat_test1[3]
```

In this first test, there was a significant effect of the predictiveness, `r apa(ANOVA_rat_test1, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test1[2])`, but not of the *similarity*, `r apa(ANOVA_rat_test1, effect = "group")`, `r report_BF_and_error(bay_ANOVA_rat_test1[1])`, nor of the *similarity x predictiveness* interaction, `r apa(ANOVA_rat_test1, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test1[1])`.

##Test2 
### Accuracy

```{r, include=FALSE}
#add a group variable
test2 <- test2 %>%
  mutate(group = case_when(session == 1 ~ "High",
                                session == 2 ~ "Medium",
                                session == 3 ~ "Low"), 
         trial_type = case_when((target == 1 & distractor_test2 == 2) | (target == 2 & distractor_test2 == 1) ~ "P-Con" ,
                                (target == 5 & distractor_test2 == 6) | (target == 6 & distractor_test2 == 5) ~ "NP-Con",
                                (target == 1 & (distractor_test2 == 5 | distractor_test2 == 6)) | (target == 2 & (distractor_test2 == 5 | distractor_test2 == 6)) ~ "P-Incon",
                                  (target == 5 & (distractor_test2 == 1 | distractor_test2 == 2)) | (target == 6 & (distractor_test2 == 1 | distractor_test2 == 2)) ~  "NP-Incon"),
         #add a congruence variable
         congruence = case_when ((trial_type == "P-Con") | (trial_type == "NP-Con") ~ "congruent",
                                 (trial_type == "P-Incon") | (trial_type == "NP-Incon") ~ "incongruent"))
#plot test accuracy
m_acc_test2 <- test2 %>%
  group_by(trial_type, group) %>%
  summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

```{r, echo = FALSE, warning=FALSE}
ggplot(data = m_acc_test2, mapping = aes(x = factor(group, levels = c("High", "Medium", "Low")), y = mean_acc, fill = trial_type)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Rating") +
  coord_cartesian(ylim = c(0.5, 1)) +
  labs(fill = "Trial type")+
  scale_fill_discrete(type = c("#7B3294", "#C2A5CF", "#008837", "#A6DBA0"))+
  labs(title = "Mean accuracy for each type of cue in test2 phase")
```

```{r, include=FALSE}
#ANOVA accuracy
acc_test2 <- test2 %>%
  group_by (pNum, group, predictiveness, congruence) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_test2$predictiveness <- factor(acc_test2$predictiveness)
acc_test2$group <- factor(acc_test2$group)
acc_test2$pNum <- factor(acc_test2$pNum)
acc_test2$congruence <- factor(acc_test2$congruence)
ANOVA_acc_test2 <- aov_car(formula = acc ~ group + Error(pNum*predictiveness*congruence), data = acc_test2)
print(ANOVA_acc_test2)

bay_ANOVA_acc_test2 <- anovaBF(formula = acc ~ group*predictiveness*congruence + pNum,
        data = data.frame(acc_test2),
        whichRandom = "pNum")
print(bay_ANOVA_acc_test2)

bay_ANOVA_acc_test2_sxp <- bay_ANOVA_acc_test2[4]/bay_ANOVA_acc_test2[3]
print(bay_ANOVA_acc_test2_sxp)
bay_ANOVA_acc_test2_pxc <- bay_ANOVA_acc_test2[13]/bay_ANOVA_acc_test2[7]
print(bay_ANOVA_acc_test2_pxc)
bay_ANOVA_acc_test2_sxc <- bay_ANOVA_acc_test2[10]/bay_ANOVA_acc_test2[6]
print(bay_ANOVA_acc_test2_sxc)
bay_ANOVA_acc_test2_sxpxc <- bay_ANOVA_acc_test2[18]/bay_ANOVA_acc_test2[17]
print(bay_ANOVA_acc_test2_sxpxc)
```

There are no significant differences due to any effect or interaction in accuracy in the second test, Except for those that did the very subtle test, all subjects had lower accuracy for the non predictive vs the predictive targets. However, there are no significant effects: *similarity* : `r apa(ANOVA_acc_test2, effect = "group")`, `r report_BF_and_error(bay_ANOVA_acc_test2[1])`; *predictiveness*: `r apa(ANOVA_acc_test2, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_test2[2])`; *congruence*: `r apa(ANOVA_acc_test2, effect ="congruence")`, `r report_BF_and_error(bay_ANOVA_acc_test2[5])`; *similarity x predictiveness*: `r apa(ANOVA_acc_test2, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_test2_sxp[1])`; *predictiveness x congruence*: `r apa(ANOVA_acc_test2, effect = "predictiveness:congruence")`, `r report_BF_and_error(bay_ANOVA_acc_test2_pxc[1])`; *similarity x congruence*: `r apa(ANOVA_acc_test2, effect = "group:congruence")`, `r report_BF_and_error(bay_ANOVA_acc_test2_sxc[1])`; *similarity x predictiveness x congruence*: `r apa(ANOVA_acc_test2, effect = "group:predictiveness:congruence")`, `r report_BF_and_error(bay_ANOVA_acc_test2_sxpxc[1])`.

###Just correct ratings

```{r, include=FALSE}
#plot test accuracy
m_rat_test2 <- filter(test2, acc == 1) %>%
  group_by(trial_type, group) %>%
  summarise(mean_rat = mean(mem_score, na.rm = TRUE), 
            se_rat = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```

```{r, echo = FALSE, warning=FALSE}
ggplot(data = m_rat_test2, mapping = aes(x = factor(group, levels = c("High", "Medium", "Low")), y = mean_rat, fill = trial_type)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_rat - se_rat, ymax = mean_rat + se_rat), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Rating") +
  labs(fill = "Trial type")+
  scale_fill_discrete(type = c("#7B3294", "#C2A5CF", "#008837", "#A6DBA0"))+
  labs(title = "Mean rating for each type of cue in test2 phase")
```

```{r, include=FALSE}
#ANOVA rat
rat_test2 <- filter(test2, acc ==1) %>%
  group_by (pNum, group, predictiveness, congruence) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_test2$predictiveness <- factor(rat_test2$predictiveness)
rat_test2$group <- factor(rat_test2$group)
rat_test2$pNum <- factor(rat_test2$pNum)
rat_test2$congruence <- factor(rat_test2$congruence)
ANOVA_rat_test2 <- aov_car(formula = rat ~ group + Error(pNum*predictiveness*congruence), data = rat_test2)
print(ANOVA_rat_test2)

bay_ANOVA_rat_test2 <- anovaBF(formula = rat ~ group*predictiveness*congruence + pNum,
        data = data.frame(rat_test2),
        whichRandom = "pNum")
print(bay_ANOVA_rat_test2)

bay_ANOVA_rat_test2_sxp <- bay_ANOVA_rat_test2[4]/bay_ANOVA_rat_test2[3]
print(bay_ANOVA_rat_test2_sxp)
bay_ANOVA_rat_test2_pxc <- bay_ANOVA_rat_test2[13]/bay_ANOVA_rat_test2[7]
print(bay_ANOVA_rat_test2_pxc)
bay_ANOVA_rat_test2_sxc <- bay_ANOVA_rat_test2[10]/bay_ANOVA_rat_test2[6]
print(bay_ANOVA_rat_test2_sxc)
bay_ANOVA_rat_test2_sxpxc <- bay_ANOVA_rat_test2[18]/bay_ANOVA_rat_test2[17]
print(bay_ANOVA_rat_test2_sxpxc)
```

When the confidence ratings for the correct options were analysed, there was a clear effect of predictiveness, *predictiveness*: `r apa(ANOVA_rat_test2, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test2[2])`. The rest of the main effects and interactions were not significant: *similarity* : `r apa(ANOVA_rat_test2, effect = "group")`, `r report_BF_and_error(bay_ANOVA_rat_test2[1])`; ; *congruence*: `r apa(ANOVA_rat_test2, effect ="congruence")`, `r report_BF_and_error(bay_ANOVA_rat_test2[5])`; *similarity x predictiveness*: `r apa(ANOVA_rat_test2, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test2_sxp[1])`; *predictiveness x congruence*: `r apa(ANOVA_rat_test2, effect = "predictiveness:congruence")`, `r report_BF_and_error(bay_ANOVA_rat_test2_pxc[1])`; *similarity x congruence*: `r apa(ANOVA_rat_test2, effect = "group:congruence")`, `r report_BF_and_error(bay_ANOVA_rat_test2_sxc[1])`; *similarity x predictiveness x congruence*: `r apa(ANOVA_rat_test2, effect = "group:predictiveness:congruence")`, `r report_BF_and_error(bay_ANOVA_rat_test2_sxpxc[1])`

#### Group Low

```{r, include = FALSE}
rat_test2_L <- filter(test2, acc == 1 & group == "Low") %>%
  group_by (pNum, predictiveness, congruence) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_test2_L$pNum <- factor(rat_test2_L$pNum)
rat_test2_L$predictiveness <- factor(rat_test2_L$predictiveness)
rat_test2_L$congruence <- factor(rat_test2_L$congruence)
# ANOVA with two within factors
ANOVA_rat_test2_L <- aov_car(rat ~ predictiveness*congruence + Error(pNum/predictiveness*congruence), data=rat_test2_L)
print(ANOVA_rat_test2_L)
#bayesian ANOVA
bay_ANOVA_rat_test2_L <- anovaBF(formula = rat ~ congruence*predictiveness + pNum,
                                    data = data.frame(rat_test2_L),
                                    whichRandom = "pNum")
print(bay_ANOVA_rat_test2_L)
bay_ANOVA_rat_test2_L_int <- bay_ANOVA_rat_test2_L[4]/bay_ANOVA_rat_test2_L[3]
print(bay_ANOVA_rat_test2_L_int)
```

No significant effects or interactions were found in group Low similarity: *predictiveness*, `r apa(ANOVA_rat_test2_L, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test2_L[1])`, nor of *congruence*, `r apa(ANOVA_rat_test2_L, effect = "congruence")`, `r report_BF_and_error(bay_ANOVA_rat_test2_L[2])` and no interaction, `r apa(ANOVA_rat_test2_L, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test2_L_int[1])`.

#### Group Medium

```{r, include = FALSE}
rat_test2_M <- filter(test2, acc == 1 & group == "Medium") %>%
  group_by (pNum, predictiveness, congruence) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_test2_M$pNum <- factor(rat_test2_M$pNum)
rat_test2_M$predictiveness <- factor(rat_test2_M$predictiveness)
rat_test2_M$congruence <- factor(rat_test2_M$congruence)
# ANOVA with two within factors
ANOVA_rat_test2_M <- aov_car(rat ~ predictiveness*congruence + Error(pNum/predictiveness*congruence), data=rat_test2_M)
print(ANOVA_rat_test2_M)
#bayesian ANOVA
bay_ANOVA_rat_test2_M <- anovaBF(formula = rat ~ congruence*predictiveness + pNum,
                                   data = data.frame(rat_test2_M),
                                   whichRandom = "pNum")
print(bay_ANOVA_rat_test2_M)
bay_ANOVA_rat_test2_M_int <- bay_ANOVA_rat_test2_M[4]/bay_ANOVA_rat_test2_M[3]
print(bay_ANOVA_rat_test2_M_int)
```

For group Medium, there was also no effect of *predictiveness*: `r apa(ANOVA_rat_test2_M, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test2_M[1])`, *congruence*: `r apa(ANOVA_rat_test2_M, effect = "congruence")`, `r report_BF_and_error(bay_ANOVA_rat_test2_M[2])`, and no interaction *group x predictiveness*: `r apa(ANOVA_rat_test2_M, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test2_M_int[1])`).

#### Group High

```{r, include = FALSE}
rat_test2_H <- filter(test2, acc == 1 & group == "High") %>%
  group_by (pNum, predictiveness, congruence) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_test2_H$pNum <- factor(rat_test2_H$pNum)
rat_test2_H$predictiveness <- factor(rat_test2_H$predictiveness)
rat_test2_H$congruence <- factor(rat_test2_H$congruence)
# ANOVA with two within factors
ANOVA_rat_test2_H <- aov_car(rat ~ predictiveness*congruence + Error(pNum/predictiveness*congruence), data=rat_test2_H)
print(ANOVA_rat_test2_H)
#bayesian ANOVA
bay_ANOVA_rat_test2_H <- anovaBF(formula = rat ~ congruence*predictiveness + pNum,
                                    data = data.frame(rat_test2_H),
                                    whichRandom = "pNum")
print(bay_ANOVA_rat_test2_H)
bay_ANOVA_rat_test2_H_int <- bay_ANOVA_rat_test2_H[4]/bay_ANOVA_rat_test2_H[3]
print(bay_ANOVA_rat_test2_H_int)
```

```{r, include = FALSE}
# SME
#calculate the simple main effect of congruence
sme_rat_test2_H_congruence <- rat_test2_H %>%
  group_by(predictiveness) %>%
  anova_test(rat ~ congruence + Error(pNum/congruence), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_rat_test2_H_congruence #Call the output table
#calculate the simple main effect of predictiveness
sme_rat_test2_H_pred <- rat_test2_H %>%
  group_by(congruence) %>%
  anova_test(rat ~ predictiveness + Error(pNum/predictiveness), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_rat_test2_H_pred #Call the output table
```

For group High there was a significant effect of *predictiveness*, `r apa(ANOVA_rat_test2_H, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_test2_H[1])`, and an effect of the *predictiveness x congruence*, `r apa(ANOVA_rat_test2_H, effect = "predictiveness:congruence")`, `r report_BF_and_error(bay_ANOVA_rat_test2_H_int[1])`, but no main effect of *congruence*, `r apa(ANOVA_rat_test2_H, effect = "congruence")`, `r report_BF_and_error(bay_ANOVA_rat_test2_H[2])`. The main effect of predictiveness was significant in both the congruent, *F* (1, 28) = 16.961, *p* \< .001, $\\eta^2_p$ = .377, and incongruent trials, *F* (1, 28) = 9.490, *p* = .008, $\\eta^2_p$ = .247. However, there was an effect of congruence in predictive trials, *F* (1, 28) = 7.457, *p* = .022, $\\eta^2_p$ = .21, but not on the non-predictive trials, *F* (1, 28) = 0.654, *p* = .852, $\\eta^2_p$ = .023.

# Exp 2

## Accuracy

```{r, include=FALSE}
load("UNM07_proc_data.RData")
UNM07_training <- filter(training, !pNum %in% not_passed_pNum$pNum)
UNM07_test <- filter(test, !pNum %in% not_passed_pNum$pNum)

#create the PPR measure
UNM07_training <- UNM07_training %>%
  mutate(prob_response = case_when((cue1 == 1 | cue1 == 3) & response == "o1_image" ~ 1,
                                   (cue1 == 1 | cue1 == 3) & response == "o2_image" ~ 0, 
                                   (cue1 == 2 | cue1 == 4) & response == "o1_image" ~ 0,
                                   (cue1 == 2 | cue1 == 4) & response == "o2_image" ~ 1))

#detect participants that had an PPR lower than 0.6 in the final block
UNM07_block8 <- filter(UNM07_training, block == 8) %>%
  group_by(pNum, condition) %>%
  summarise (mean_response = mean(prob_response, na.rm = TRUE))
UNM07_low_acc <- filter(UNM07_block8, mean_response < 0.6) 
#create a frequencies table for each condition
frequencies_UNM07_low_acc <- UNM07_low_acc %>%
  group_by(condition) %>%
  summarise(participant_count = n_distinct(pNum)) %>%
  ungroup()
#check which one is the highest frequency in that table
max_UNM07_low_acc <- max(frequencies_UNM07_low_acc$participant_count)
# Select participants with the lowest accuracy in each condition, as many as the maximum frequency in the frequency table
UNM07_low_acc_total <- UNM07_block8 %>%
  group_by(condition) %>%
  slice_min(order_by = mean_response, n = max_UNM07_low_acc, with_ties = FALSE) %>%
  ungroup()
#get just the participant numbers
UNM07_low_acc <- UNM07_low_acc_total$pNum
#exclude those participants
UNM07_training <- filter(UNM07_training, !pNum %in% UNM07_low_acc_total$pNum)
UNM07_test <- filter(UNM07_test, !pNum %in% UNM07_low_acc_total$pNum)
```

```{r, include=FALSE}
#Calculate the mean accuracy and standard error for each block, including the groups
MA_test <- UNM07_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```

```{r, echo=FALSE}
ggplot(data = MA_test, mapping = aes(x = factor(condition, level=c('Uncertain', 'Certain')), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of test") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean accuracy for each type of cue in test phase")
```

```{r, include=FALSE}
#ANOVA accuracy
acc_UNM07_test <- UNM07_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_UNM07_test$predictiveness <- factor(acc_UNM07_test$predictiveness)
acc_UNM07_test$condition <- factor(acc_UNM07_test$condition)
acc_UNM07_test$pNum <- factor(acc_UNM07_test$pNum)
ANOVA_acc_UNM07_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness), data = acc_UNM07_test)
print(ANOVA_acc_UNM07_test)

bay_ANOVA_acc_UNM07_test <- anovaBF(formula = acc ~ condition*predictiveness + pNum,
        data = data.frame(acc_UNM07_test),
        whichRandom = "pNum")
print(bay_ANOVA_acc_UNM07_test)

bay_ANOVA_acc_UNM07_test_int <- bay_ANOVA_acc_UNM07_test[4]/bay_ANOVA_acc_UNM07_test[3]
print(bay_ANOVA_acc_UNM07_test_int)
```
```{r, include = FALSE}
# SME
#calculate the simple main effect of condition
sme_acc_UNM07_test_group <- acc_UNM07_test %>%
  group_by(predictiveness) %>%
  anova_test(acc ~ condition, effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_acc_UNM07_test_group #Call the output table

#calculate the simple main effect of predictiveness
sme_acc_UNM07_test_pred <- acc_UNM07_test %>%
  group_by(condition) %>%
  anova_test(acc ~ predictiveness + Error(pNum/predictiveness), effect.size = "pes") %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
sme_acc_UNM07_test_pred #Call the output table
```

There were no differences due to the main effect of *group*, `r apa(ANOVA_acc_UNM07_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_acc_UNM07_test[1])`. However, there was a significant main effect of *predictiveness*, `r apa(ANOVA_acc_UNM07_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM07_test[2])` and of the interaction *group x predictiveness*, `r apa(ANOVA_acc_UNM07_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM07_test_int[1])`. The simple main effects analysis found a significant effect of predictiveness in the Certain group, *F* (1, 48) = 7.810, *p* = .014, $\\eta^2_p$ = .14, but not on the Uncertain group, *F* (1, 48) = .459, *p* = 1, $\\eta^2_p$ = .009.

## Just correct ratings

```{r, include=FALSE}
#plot test mem_score but take out the errors
M_rat_UNM07_test <- filter(UNM07_test, acc == 1) %>%
  group_by(predictiveness, condition) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```
```{r, echo=FALSE}
ggplot(data = M_rat_UNM07_test, mapping = aes(x = factor(condition, levels = c("Uncertain", "Certain")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Ratings") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean ratings for each type of cue in UNM07_test phase")
```
```{r, include=FALSE}
#ANOVA mem_score
rat_UNM07_test <- filter(UNM07_test, acc == 1) %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_UNM07_test$predictiveness <- factor(rat_UNM07_test$predictiveness)
rat_UNM07_test$condition <- factor(rat_UNM07_test$condition)
rat_UNM07_test$pNum <- factor(rat_UNM07_test$pNum)
ANOVA_rat_UNM07_test <- aov_car(formula = rat ~ condition + Error(pNum*predictiveness), data = rat_UNM07_test)
print(ANOVA_rat_UNM07_test)
```
```{r, include=FALSE}
bay_ANOVA_rat_UNM07_test <- anovaBF(formula = rat ~ condition*predictiveness + pNum,
        data = data.frame(rat_UNM07_test),
        whichRandom = "pNum")
print(bay_ANOVA_rat_UNM07_test)
```
```{r, include=FALSE}
bay_ANOVA_rat_UNM07_test_int <- bay_ANOVA_rat_UNM07_test[4]/bay_ANOVA_rat_UNM07_test[3]
```

In this first test, there was no significant effect of the *group*, `r apa(ANOVA_rat_UNM07_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_rat_UNM07_test[1])`, the *predictiveness*, `r apa(ANOVA_rat_UNM07_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_UNM07_test[2])`, nor of the *group x predictiveness* interaction, `r apa(ANOVA_rat_UNM07_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_UNM07_test[1])`.

# Exp 3

## Accuracy

```{r, include=FALSE}
load("UNM08_proc_data.RData")
UNM08_training <- rbind(stage1, stage2)
UNM08_training <- filter(UNM08_training, !pNum %in% not_passed_pNum$pNum)
UNM08_test <- filter(test, !pNum %in% not_passed_pNum$pNum)


#create the PPR measure
UNM08_training <- UNM08_training %>%
  mutate(prob_response = case_when((cue1 == 1 | cue1 == 3) & response == "o1_image" ~ 1,
                                   (cue1 == 1 | cue1 == 3) & response == "o2_image" ~ 0, 
                                   (cue1 == 2 | cue1 == 4) & response == "o1_image" ~ 0,
                                   (cue1 == 2 | cue1 == 4) & response == "o2_image" ~ 1))

#detect and clean participants that had an PPR lower than 0.6 in the final block or not passed the test comprehension check
UNM08_block6 <- filter(UNM08_training, block == 6) %>%
  group_by(pNum, condition) %>%
  summarise (mean_response = mean(prob_response, na.rm = TRUE))
UNM08_low_acc <- filter(UNM08_block6, mean_response < 0.6) 
#create a frequencies table for each condition
frequencies_UNM08_low_acc <- UNM08_low_acc %>%
  group_by(condition) %>%
  summarise(participant_count = n_distinct(pNum)) %>%
  ungroup()
#check which one is the highest frequency in that table
max_UNM08_low_acc <- max(frequencies_UNM08_low_acc$participant_count)
# Select participants with the lowest accuracy in each condition, as many as the maximum frequency in the frequency table
UNM08_low_acc_total <- UNM08_block6 %>%
  group_by(condition) %>%
  slice_min(order_by = mean_response, n = max_UNM08_low_acc, with_ties = FALSE) %>%
  ungroup()
#create a vecto with the exluced pNum
UNM08_low_acc <- UNM08_low_acc_total$pNum
#exclude from dataframe
UNM08_training <- filter(UNM08_training, !pNum %in% UNM08_low_acc_total$pNum)
UNM08_test <- filter(UNM08_test, !pNum %in% UNM08_low_acc_total$pNum)
```
```{r, include=FALSE}
#Calculate the mean accuracy and standard error for each block, including the groups
MA_UNM08_test <- UNM08_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_acc = mean(acc, na.rm = TRUE), 
            se_acc = sd(acc, na.rm = TRUE)/sqrt(length(acc)))
```
```{r, echo=FALSE}
ggplot(data = MA_UNM08_test, mapping = aes(x = factor(condition, level=c('Uncertain', 'Certain Short','Certain Long')), y = mean_acc, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(y= mean_acc, ymin = mean_acc - se_acc, ymax = mean_acc + se_acc), width = .2, position = position_dodge(0.9)) +
  coord_cartesian(ylim = c(0, 1))+
  scale_x_discrete (name = "Type of test") +
  scale_y_continuous(name = "Accuracy") +
  coord_cartesian(ylim = c(0.5, 1)) +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean accuracy for each type of cue in test phase")
```
```{r, include=FALSE}
#ANOVA accuracy
acc_UNM08_test <- UNM08_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(acc = mean(acc, na.rm = TRUE))
acc_UNM08_test$predictiveness <- factor(acc_UNM08_test$predictiveness)
acc_UNM08_test$condition <- factor(acc_UNM08_test$condition)
acc_UNM08_test$pNum <- factor(acc_UNM08_test$pNum)
ANOVA_acc_UNM08_test <- aov_car(formula = acc ~ condition + Error(pNum*predictiveness), data = acc_UNM08_test)
print(ANOVA_acc_UNM08_test)

bay_ANOVA_acc_UNM08_test <- anovaBF(formula = acc ~ condition*predictiveness + pNum,
        data = data.frame(acc_UNM08_test),
        whichRandom = "pNum")
print(bay_ANOVA_acc_UNM08_test)

bay_ANOVA_acc_UNM08_test_int <- bay_ANOVA_acc_UNM08_test[4]/bay_ANOVA_acc_UNM08_test[3]
print(bay_ANOVA_acc_UNM08_test_int)
```
```{r, include=FALSE}
# Pairwise comparisons for the main effect of condition
acc_UNM08_test_interaction <- emmeans(ANOVA_acc_UNM08_test, ~condition)
contrast(acc_UNM08_test_interaction, adjust = "bon", "trt.vs.ctrl", ref = c(1,2))

acc_UNM08_test_certs <- subset(acc_UNM08_test, (condition == "Certain Long") | (condition == "Certain Short"), acc, drop = TRUE)
acc_UNM08_test_uncert <- subset(acc_UNM08_test, condition == "Uncertain", acc, drop = TRUE)
bay_t.test_acc_UNM08_int_uncer_vs_certs <-  ttestBF(acc_UNM08_test_certs, acc_UNM08_test_uncert)
print(bay_t.test_acc_UNM08_int_uncer_vs_certs)

pairs(acc_UNM08_test_interaction, adjust = "bon")

acc_UNM08_test_cert <- subset(acc_UNM08_test, condition == "Certain Long", acc, drop = TRUE)
acc_UNM08_test_cert_s <- subset(acc_UNM08_test, condition == "Certain Short", acc, drop = TRUE)
bay_t.test_acc_UNM08_test_certs <-  ttestBF(acc_UNM08_test_cert, acc_UNM08_test_cert_s)
print(bay_t.test_acc_UNM08_test_certs)
```

There was a significant effect of *predictiveness*, `r apa(ANOVA_acc_UNM08_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM08_test[2])`, and a significant main effect of *group*, `r apa(ANOVA_acc_UNM08_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_acc_UNM08_test[1])`, but not of the *group x predictiveness* interaction, `r apa(ANOVA_acc_UNM08_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_acc_UNM08_test[1])`. Bonferroni corrected post-hoc comparisons on the factor of group showed that group Uncertain differed significantly from the average of the Certain groups, *t*(87) = 2.689, *p* = .009, `r report_BF_and_error(bay_t.test_acc_UNM08_int_uncer_vs_certs)`, but memory scores for the two Certain groups did not differ the one from the other, *t*(87) = 0.267, *p* = 1, `r report_BF_and_error(bay_t.test_acc_UNM08_test_certs)`, with the Bayesian evidence suggesting that memory performance was the same in these two groups.

## Just correct ratings

```{r, include=FALSE}
#plot test mem_score but take out the errors
M_rat_UNM08_test <- filter(UNM08_test, acc == 1) %>%
  group_by(predictiveness, condition) %>%
  summarise(mean_mem_score = mean(mem_score, na.rm = TRUE), 
            se_mem_score = sd(mem_score, na.rm = TRUE)/sqrt(length(mem_score)))
```
```{r, echo=FALSE}
ggplot(data = M_rat_UNM08_test, mapping = aes(x = factor(condition, levels = c("Uncertain", "Certain Short", "Certain Long")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Ratings") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  labs(title = "Mean ratings for each type of cue in test phase")
```

```{r, include=FALSE}
#ANOVA mem_score
rat_UNM08_test <- filter(UNM08_test, acc == 1) %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(rat = mean(mem_score, na.rm = TRUE))
rat_UNM08_test$predictiveness <- factor(rat_UNM08_test$predictiveness)
rat_UNM08_test$condition <- factor(rat_UNM08_test$condition)
rat_UNM08_test$pNum <- factor(rat_UNM08_test$pNum)
ANOVA_rat_UNM08_test <- aov_car(formula = rat ~ condition + Error(pNum*predictiveness), data = rat_UNM08_test)
print(ANOVA_rat_UNM08_test)
```
```{r, include=FALSE}
bay_ANOVA_rat_UNM08_test <- anovaBF(formula = rat ~ condition*predictiveness + pNum,
        data = data.frame(rat_UNM08_test),
        whichRandom = "pNum")
print(bay_ANOVA_rat_UNM08_test)
```
```{r, include=FALSE}
bay_ANOVA_rat_UNM08_test_int <- bay_ANOVA_rat_UNM08_test[4]/bay_ANOVA_rat_UNM08_test[3]
```

There was a significant effect *predictiveness*, `r apa(ANOVA_rat_UNM08_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_UNM08_test[2])`, but no significant effect of the *group*, `r apa(ANOVA_rat_UNM08_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_rat_UNM08_test[1])`, nor of the *group x predictiveness* interaction, `r apa(ANOVA_rat_UNM08_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_rat_UNM08_test[1])`.
