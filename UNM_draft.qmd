---
title: "Effects of expected and unexpected uncertainty on cue processing"
# If blank, the running header is the title in upper case.
shorttitle: "Effects of uncertainty on cue processing"
# Set names and affiliations.
# It is nice to specify everyone's orcid, if possible.
# There can be only one corresponding author, but declaring one is optional.
author:
  - name: Clara Muñiz-Diez
    corresponding: true
    orcid: 0000-0001-5192-0462
    email: c.muniz-diez@lancaster.ac.uk
    # Roles are optional. 
    # Select from the CRediT: Contributor Roles Taxonomy https://credit.niso.org/
    # conceptualization, data curation, formal Analysis, funding acquisition, investigation, 
    # methodology, project administration, resources, software, supervision, validation, 
    # visualization, writing, editing
    affiliations:
      - id: id1
        name: "Lancaster University"
        department: Department of Psychology
        city: Lancaster
        region: UK
  - name: Sandra Lagator
    orcid: 0000-0001-6060-2941
    affiliations: 
      - id: id2
        name: "The University of Nottingham"
        department: School of Psychology
        city: Nottingham
        region: UK
  - name: Mark Haselgrove
    orcid: 0000-0001-8981-1181
    affiliations:
      - ref: id2
  - name: Tom Beesley
    orcid: 0000-0003-2836-2743
    # List city and region/state for unaffiliated authors
    affiliations:
      - ref: id1
author-note:
  status-changes: 
    # Example: [Author name] is now at [affiliation].
    affiliation-change: ~
    # Example: [Author name] is deceased.
    deceased: ~
  # Disclosures condensed to one paragraph, but you can start a field with two line breaks to break them up: \n\nNew Paragraph
  disclosures:
    # Example: This study was registered at X (Identifier Y).
    study-registration: ~
    # Acknowledge and cite data/materials to be shared.
    data-sharing: "The programs of the experiments presented here, the data and the full code for the writing this manuscript are freely available on www.github.com/munizdiezclara/UNM_draft."
    # Example: This article is based on data published in [Reference].
    # Example: This article is based on the dissertation completed by [citation].  
    related-report: ~
    # Example: [Author name] has been a paid consultant for Corporation X, which funded this study.
    conflict-of-interest: ~
    # Example: This study was supported by Grant [Grant Number] from [Funding Source].
<<<<<<< HEAD
    financial-support: "This work was supported by the ESRC grant Known unknowns
=======
    financial-support: "This study was supported by the ESRC grant Known unknowns
>>>>>>> 9fe9910d3e42d4f79cdb32899cf0ad7272afb920
and unknown unknowns (ES/W013215/1)." 
    # Example: The authors are grateful to [Person] for [Reason].
    gratitude: ~
    # Example. Because the authors are equal contributors, order of authorship was determined by a fair coin toss.
    authorship-agreements: ~
abstract: "Learning influences the overt attention that is paid to stimuli in two main ways: first, stimuli which are reliable predictors of an outcome are paid more attention than unreliable stimuli; and second, stimuli associated with uncertain outcomes capture more attention than stimuli associated with certain outcomes. Past studies have shown that these two phenomena can be demonstrated within the same experiment, but strikingly, the increase in attention due to uncertainty does not necessarily translate into subsequent better learning. We investigate this paradox by examining stimulus processing in three experiments that included predictive and non-predictive cues, trained under different conditions of uncertainty. In Experiment 1, we established a recognition memory test that could detect differences in stimulus processing from a previous learning task. In Experiment 2, this test revealed that recognition  memory was similar after learning with certain and uncertain stimulus-outcome contingencies. In Experiment 3, uncertain contingencies were introduced after a period of learning with certain contingencies. During the subsequent memory test, this training resulted in better memory than training with certain contingencies throughout the learning phase. These results suggest the importance of drawing a distinction between expected and unexpected uncertainty on stimulus processing. The implications of these results for attentional models of learning are discussed."
# Put as many keywords at you like, separated by commmas (e.g., [reliability, validity, generalizability])
keywords: [Associative Learning, Attention, Uncertainty, Predictiveness, Cue processing]
# If true, tables and figures are mingled with the text instead of listed at the end of the document.
floatsintext: true
# Numbered lines (.pdf and .docx only)
numbered-lines: false
# File with references
bibliography: references.bib
# Suppress title page
suppress-title-page: false
# Masks references that appear in the masked-citations list
mask: false
masked-citations:
  - schneider2012cattell
  - schneider2015intelligence
# Language options. See https://quarto.org/docs/authoring/language.html
lang: en
language:
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
format:
  apaquarto-docx: default
  apaquarto-html: default
  apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: man
---

```{r}
#| label: setup
#| include: false
library(conflicted)
library(tidyverse)
library(flextable)
library(ftExtra)
library(officer)
library(knitr)
library(papaja)
library(afex)
library(BayesFactor)
library(apa)
library(emmeans)
library(dplyr)
library(rstatix)
options(digits = 3)
bfit = 500#1000

conflicts_prefer(dplyr::filter, .quiet = TRUE)
conflicts_prefer(flextable::separate_header, .quiet = TRUE)
conflicted::conflicts_prefer(papaja::theme_apa)

# function to force scientific formatting of numbers (used for large BFs)
changeSciNot <- function(n) {
  output <- format(n, scientific = TRUE, digits = 2) #Transforms the number into scientific notation even if small
  output <- sub("e", "x10^", output) #Replace e with 10^
  output <- sub("\\+0?", "", output) #Remove + symbol and leading zeros on exponent, if > 1
  output <- sub("-0?", "-", output) #Leaves - symbol but removes leading zeros on exponent, if < 1
  output <- paste0(output,"^")
  # output = strsplit(output, "^", fixed = TRUE)
  # output = paste0(output[[1]][1],"^", output[[1]][2], "^")
  output
}
# function to extract and report BFs with error %s
report_BF_and_error <- function(BF_in, sci_not = FALSE, hyp = "alt"){
  
  if (hyp == "alt") {
    BF_notation = "BF~10~ = "
  } else if (hyp == "null") {
    BF_notation = "BF~01~ = "
  }
  
  if (sci_not == TRUE) {
    BF_value = changeSciNot(extractBF(BF_in)$bf) # change to sci notation
  } else {
    BF_value = round(extractBF(BF_in)$bf,2) # otherwise round
  }
  
  paste0(BF_notation, 
         BF_value, 
         " &plusmn; ", 
         round(100*extractBF(BF_in)$error,2), 
         "%")
}
```

A central feature of the cognition of humans and other animals is the ability to learn the predictive relationships between events in the world in order to anticipate future goals and modify behaviour accordingly. However, not all events that co-occur have predictive validity, that is, not all events that happen at the same time are related to each other; furthermore, because of capacity limitations, processing all stimulus pairings would neither be functional nor possible. The cognitive system must therefore select events that are task relevant to focus resources and refine the allocation of attention. The attention that is paid to a stimulus plays an important role in models of associative learning. Take for example the influential Rescorla & Wagner [-@rescorlaTheoryPavlovianConditioning1972] model, in which, the parameter $\alpha$ refers to the salience of a cue. The inherent salience of a cue (e.g., how loud a sound is; how bright a light is) will determine how much it captures attention, and thus how successful learning will be with it. However, the attention paid to a stimulus is not just inherent – it can also be determined by its associative history [for a review, see @lepelleyAttentionAssociativeLearning2016]; that is, from what has been previously learned about that stimulus. For example, at busy pedestrian crossings/crosswalks it is common for a sound to be played when it is safe to cross. Through experience, this sound will readily come to capture attention due to its predictive nature, while other salient but non-predictive stimuli (e.g., an advertising billboard) will not.

There are two main ways in which learning can shape attention. The first is often referred to as the *predictiveness principle*, according to which attention increases to stimuli that reliably signal the occurrence of an outcome [e.g., @mackintoshTheoryAttentionVariations1975; see also: @kruschkeUnifiedModelAttention2001; -@kruschkeAttentionLearning2003; @lepelleyRoleAssociativeHistory2004]. It is thought that this mode of attention is advantageous as it allows for the *exploitation* of reliable knowledge and permits animals to be prepared to make responses to stimuli with known consequences. A common method for studying this process in the lab is with the “learned predictiveness” design [e.g., @lepelleyLearnedAssociabilityAssociative2003]. In this contingency learning procedure, cues are trained as differentially predictive of particular outcomes (e.g., in the allergist task, participants are instructed that various foods are differentially predictive of one kind of allergic reaction or another on an imaginary patient). Typically, compounds of cues are presented, where one cue is perfectly predictive of an outcome, while the other is non-predictive. Take for example, a training phase in which compounds AX and AY are followed by outcome 1, whereas compounds BX and BY are followed by outcome 2. Here, cues A and B are predictive of outcomes 1 and 2 respectively, but cues X and Y are non-predictive, being paired with outcomes 1 and 2 equally often. Once this training is complete and participants show good levels of learning, a second phase is introduced, in which the cues are trained with new outcomes, and both cues are now perfectly predictive of these outcomes (e.g., AX-O3; BY-O4). The central finding of this procedure is that, in the second phase, participants learn more about the cues that were predictive in the first phase than about the cues that were previously non-predictive. The interpretation of this result [e.g., @lepelleyLearnedAssociabilityAssociative2003] is that, in the first phase, the cues that were established as predictive undergo an increase in their ability to attract attention, which enhances learning in the second phase. Consistent with this interpretation is the observation that (a) cues which have been established as predictive attract longer eye-gaze durations than non-predictive cues and (b) this overt measure of differential attention correlates with the bias established to learning [@lepelleyOvertAttentionPredictiveness2011]. The learned predictiveness effect has been extensively replicated and reproduced in studies of learning [for a review, see: @lepelleyAttentionAssociativeLearning2016].

The second way in which learning can modify the attention paid to stimuli is the *uncertainty principle* [e.g., @pearceModelPavlovianLearning1980; see also: @lepelleyModelingAttentionAssociative2012; @schmajukLatentInhibitionNeural1996] which states that more attention will be paid to cues which have an uncertain outcome. It is thought that this mode of attention is useful as it allows for the exploration of cues, whose predictive validity is uncertain, in order to discover relationships between these events. Griffiths et al. [-@griffithsNegativeTransferHuman2011] showed an instance of this principle using the “negative transfer” procedure. In this study, participants experienced a stimulus followed by a small-magnitude outcome (a food predicted a minor allergic reaction) in the first phase, and the same stimulus followed by a larger-magnitude outcome (a food predicted a critical allergic reaction) in the second phase. The study had two groups, with the only difference between the groups being that one group received a small number of presentations of the stimulus in the absence of the outcome between the first and second phases of the experiment in order to introduce uncertainty into the learnt contingency. The critical result was that the group that received “no outcome” trials learned more quickly about the large outcome in the second phase, compared to the group who did not have these no outcome trials. This result suggests that this brief period of uncertainty enhanced the attention paid to the cues facilitating subsequent learning[^1].

[^1]: It should be noted that this result has proved difficult to reproduce (see Le Pelley et al., 2016).

At face value, these two principles seem to be incompatible or contradictory: the predictiveness principle states we focus resources on cues that we know about, whilst the uncertainty principle states that we focus resources on cues we are less sure about. However, it is quite possible that both principles operate and describe changes in the allocation of attention, depending on the experienced contingencies. In fact, a number of hybrid models of learning and attention have tried to reconcile the evidence in favour of both principles [e.g., @esberReconcilingInfluencePredictiveness2011; @lepelleyRoleAssociativeHistory2004; @pearceTwoTheoriesAttention2010], and some of them propose that the predictiveness and the uncertainty principles may have different functions [e.g., @kerstenTwoCompetingAttentional1998; @lepelleyRoleAssociativeHistory2004]. The predictiveness principle leads to a prioritization of information in situations in which outcome events are reliable. However, when outcome events are less stable, it is less advantageous to invest cognitive resources in exploiting what is known. Under these circumstances it might be more advantageous to explore other sources of information, in order to attempt to reduce the uncertainty in the environment. For example, a teacher can easily identify students that might need extra help, solely focusing on their grades on the exams. However, there might be students who pass those exams, but show other, less reliable signs of a need for extra help, such as poor class engagement or absenteeism. For this reason, if the teacher wants to better understand the needs of their students, they might explore new signals in order to reduce uncertainty.

There is a growing body of evidence that points towards both principles operating in parallel in human contingency learning tasks [@koenigRewardDrawsEye2017; @luquePredictionUncertaintyAssociative2017; @torrents-rodasEvidenceTwoAttentional2021]. Beesley et al. [-@beesleyUncertaintyPredictivenessDetermine2015], for example, adapted the learned predictiveness design of Le Pelley and McLaren [-@lepelleyLearnedAssociabilityAssociative2003] to manipulate both predictiveness and uncertainty within the same procedure. In this study, each compound of cues had either a certain contingency with the outcome (i.e., it was consistently followed by the same outcome), or an uncertain contingency (i.e., it was probabilistically related to the outcomes, with one outcome occurring on 70% of trials, and the other on 30% of trials). Measuring participants eye gaze, this study showed that on “uncertain trials”, all cues (both predictive and non-predictive) received more attention than the cues did on “certain trials”. However, a predictiveness effect was also evident, with higher attention to predictive than to non-predictive cues, although this effect was only evident for cues. Thus, this study showed that attention is both determined by the uncertainty principle, since there was higher attention when uncertainty was high, as well as by the predictiveness principle, since within each compound, attention was devoted more to predictive over non-predictive cues.

A study by Easdale et al. [-@easdaleOnsetUncertaintyFacilitates2019] examined how these differences in uncertainty affected the rate of learning about cues. Participants experienced the same certain and uncertain contingencies as in Beesley et al. [-@beesleyUncertaintyPredictivenessDetermine2015] in a first phase, before receiving a second phase in which there were new contingencies to learn that would resolve the uncertainty entirely. Contrary to the expectations of models of the uncertainty principle [e.g., @pearceModelPavlovianLearning1980] it was found that participants who initially experienced the uncertain contingencies learnt about these new contingencies more *slowly* than those participants who first learnt about certain contingencies. Easdale et al. argued that this result provides evidence to support a distinction between “expected uncertainty” and “unexpected uncertainty”: in the former, participants may learn to anticipate variation in the outcome, which then leads to slower acquisition of the new contingencies [see also, @behrensLearningValueInformation2007].

Torrent-Rodas et al. [-@torrents-rodasEffectPredictionError2023] also examined the impact of uncertainty on overt attention and new learning using a within-subjects design. In a first phase they found that non-predictive cues that were associated with maximal prediction error (e.g., cue X during XZ-O1 and XZ-O2 training) received higher levels of attention compared to predictive cues (e.g., cues A and B during AZ-O1 and BZ-O2 training). However, when the participants were given new contingencies to learn in a second stage, discriminations between compounds that relied on previously non-predictive cues were not learnt at a faster rate than those that relied on previously predictive cues. Thus, like the results of Easdale et al., the data from Torrents-Rodas et al. suggest that expected uncertainty drives higher levels of overt attention to cues, but this does not translate into more rapid learning. One of the reasons why this finding of slower learning under conditions of expected uncertainty is surprising, is that participants in this condition showed higher attention to the cues in the first stage. Thus, the data from Easdale et al. and Torrents-Rodas et al. represent a paradoxical set of results for theories of associative learning to explain, since participants were overtly attending to cues more in the uncertain condition, yet this did not translate into faster learning about these cues. All attentional theories of associative learning predict that the attention paid to a stimulus is directly related to the rate at which learning occurs for that stimulus. This raises the question of what the high levels of overt attention to uncertain cues represent in the results of Beesley et al. [-@beesleyUncertaintyPredictivenessDetermine2015], Easdale et al. [-@easdaleOnsetUncertaintyFacilitates2019], and Torrents-Rodas et al. [-@torrents-rodasEffectPredictionError2023]. It is this process that is currently poorly understood and is the focus of the current study.

A critical question arises from this paradox: do uncertain conditions result in an increase in cognitive processing? Pearce and Hall [-@pearceModelPavlovianLearning1980] thought so, suggesting that stimuli which are part of an unfamiliar (i.e. unlearned) task undergo more controlled processing (Pearce and Hall, 1980, p 549) and only transition to more automatic processing once the task is familiar. Consequently, we might expect that uncertainty should not only increase just the associative learning pertaining to these stimuli (i.e., “learning rate”), but also the memory of the stimulus representation itself [@chunInteractionsAttentionMemory2007]. For example, Otten et al. [-@ottenBrainActivityEvent2006] have shown that neural activity associated with a cue to semantically process an upcoming stimulus predicts the successful later retrieval of that stimulus at test; deeper processing tasks encourage richer encoding.

In the current study, three experiments were conducted with the design employed by Beesley et al. [-@beesleyUncertaintyPredictivenessDetermine2015], in which the predictiveness and the uncertainty of the cues were manipulated. Based on the notion that greater stimulus processing is associated with superior memory recall [@craikLevelsProcessingFramework1972; @craikDepthProcessingRetention1975; @fletcherFunctionalRolesPrefrontal1998]. Stimulus processing was measured by means of a recognition memory test for the cues at the end of the task. Experiment 1 developed a suitably sensitive recognition memory test that could adequately detect variations in recognition memory for predictive and non-predictive stimuli within a learned predictiveness design. Experiment 2 assessed the memory for predictive and non-predictive cues trained either under certain or expected uncertain cues, and Experiment 3 contrasted the effect of expected and unexpected uncertainty.

# Experiment 1

The aim of Experiment 1 was to develop a recognition memory test that was sensitive to detecting differences in the predictive validity of cues. All participants undertook the same training with the first phase of a “learned predictiveness” design (see Table 1). In this design, cues A and B are perfectly predictive of the outcomes that follow them, while cues X and Y are non-predictive. This learning phase was followed by two recognition memory tests. In both tests, two images were presented side-by-side, with one image being a cue presented in the training phase and the other image being a variation of one of the cues (hereafter, the “foil”). In Test 1, the foil had the same shape as the target but there was variation in its colours. In Test 2, the foil was a variation of one of the other cues. There were two different types of test trial in Test 2: congruent trials, in which the target and the foil were of the same predictiveness, and incongruent trials, in which the target and foil differed in their predictiveness. There were three between-subject groups, which differed in terms of the similarity that the foils had to the cues: High, Medium and Low similarity (see @fig-foil_example). The design of Experiment 1 can be seen in @tbl-exp1. Note that during the training stage cues A, B, X and Y were all presented equally frequently and equivalently recently to the recognition memory tests. Other things being equal, then, their memory traces should be equivalent. Thus, if recognition scores differ between these stimuli, it would indicate a difference in the level of processing that the cues underwent during training.

::: {#tbl-exp1 apa-note="Uppercase letters A, B, X, and Y represent the cues presented during training. O1 and O2 represent the outcomes presented in training. Lowercase letters *a*, *b*, *x*, and *y* represent the foils that are similar to their respective (uppercase letter) cues presented in the training phase." apa-twocolumn="true"}
+-------------+-------------+--------------------------+----------------------------+
| Training    | Test 1      | Test2 - congruent trials | Test2 - incongruent trials |
+:===========:+:===========:+:========================:+:==========================:+
| AX-O1       | A vs *a*    | A vs *b*                 | A vs *x* or *y*            |
+-------------+-------------+--------------------------+----------------------------+
| AY-O1       | B vs *b*    | B vs *a*                 | B vs *x* or *y*            |
+-------------+-------------+--------------------------+----------------------------+
| BX-O2       | X vs *x*    | X vs *y*                 | X vs *a* or *b*            |
+-------------+-------------+--------------------------+----------------------------+
| BY-O2       | Y vs *y*    | Y vs *x*                 | Y vs *a* or *b*            |
+-------------+-------------+--------------------------+----------------------------+

Design of Experiment 1
:::

## Methods

### Transparency and openness statement

In this study we detail the processes for identifying any data to be excluded, any data exclusions and all measures in the study. Statistical analyses were conducted using RStudio [-@positteamRStudio2024], with R version 4.3.3 [@rcoreteamLanguageEnvironmentStatistical2023]. All experiments were built with the open-source software PsychoPy [v. 2022.2.4, @peircePsychoPy2ExperimentsBehavior2019], and all experiments were run on Pavlovia. Participants were recruited through Prolific. The design and analysis of the experiments were based on previously published manuscripts but were not preregistered. Materials and data are freely available at: www.github.com/munizdiezclara/UNM_draft. All the experiments reported in this paper received ethical approval by the Ethics Committee at the School of Psychology, Lancaster University, UK.

### Participants

```{r, include=FALSE}
#load the data
load("UNM05_proc_data.RData")
UNM05_demographics <- demographics
UNM05_training <- training
UNM05_test1 <- test1
UNM05_test2 <- test2
UNM05_not_passed <- not_passed_pNum
```

`r nrow(UNM05_demographics)` participants were recruited through Prolific. The sample consisted of `r length(which(UNM05_demographics$gender == "female"))` women, `r length(which(UNM05_demographics$gender == "male"))` men and one non-binary person, with `r n_distinct(UNM05_demographics$Nationality)` different nationalities. The mean age was `r format(mean(UNM05_demographics$age, na.rm = TRUE), digits = 3)` for the `r nrow(UNM05_demographics) - sum(is.na(UNM05_demographics$age))` participants that reported their age, being (range `r min(UNM05_demographics$age, na.rm = TRUE)` - `r max(UNM05_demographics$age, na.rm = TRUE)`. Pre-screening of participants in Prolific ensured that they had normal or corrected to normal vision, fluency in English language, and had not participated in previous studies from our lab. Participants were rewarded with £2.70 for their participation in the study. Participants were randomly allocated to each of the three groups, according to the foil-to-cue similarity during the memory tests (High, Medium and Low). Six participants were excluded, as they failed at least one of the comprehension checks before the memory tests, resulting on `r length(which(UNM05_demographics$session == 1)) - length(which(UNM05_not_passed$session == 1))` in group High, `r length(which(UNM05_demographics$session == 2)) - length(which(UNM05_not_passed$session == 2))` in group Medium, and `r length(which(UNM05_demographics$session == 3)) - length(which(UNM05_not_passed$session == 3))` in group Low. Post-hoc calculations using G\*Power 3.1 [@faulStatisticalPowerAnalyses2007] revealed that the resulting sample size of `r nrow(UNM05_demographics) - nrow(UNM05_not_passed)` had a power of .99 to detect an effect size of η~p~^2^ = .06 that was observed for the predictiveness main effect reported in @fig-test2Exp1.

### Apparatus and stimuli

Participants were presented with a task built in PsychoPy [v. 2022.2.4, @peircePsychoPy2ExperimentsBehavior2019] and hosted in Pavlovia. The task was designed so it could only be run on a computer, but not on mobile devices. The screen background colour was grey (RGB: 128, 128, 128) and all stimuli and instructions were presented against this background. The four cues presented to each participant (A, B, X and Y) were randomly selected from a set of eight images, representing imaginary chemical compounds made of three red circles and three blue circles connected with black lines. Each cue was 945 x 945 pixels, automatically re-scaled to 0.4 x 0.4 of the window height. The outcomes (O1 and O2) were two images displaying a mutant creature, black with yellow details. Each was 332 x 664 pixels, automatically re-scaled to 0.16 x 0.2 of the window height.

Examples of the cue images are shown in @fig-foil_example. The foils used in the tests were colour-modifications of the original cues: for the High similarity group, the colours of one red and one blue circle were switched (four remained unchanged); for the Medium similarity group, the colours of two red and two blue circles were switched (two remained unchanged); and for the Low similarity group, the colours of all circles were switched. All of the images used in the experiment are presented in Appendix I.

```{r fig-foil_example}
#| fig-cap: Example of the modifications in the original cue image to create the foils.
#| apa-twocolumn: true
#| apa-note: "The top row shows an example cue used in the training phase, with three potential foils (depending on condition) shown in the bottom row. High - two of the circles have swapped colours; Medium - four circles have swapped colours; Low - all six circles have swapped colours."
#| out-width: 100%
#| fig-pos: h
knitr::include_graphics("images/foil_example.png")
```

### Design

The experiment used a mixed design, with cue-predictiveness (P and NP) manipulated within-subjects and the similarity between targets and foils manipulated between-subjects. The training phase consisted of four blocks, with each block consisting of 20 trials. As shown in @tbl-exp1, there were four trial types (compound cues), with cues A and B predictive of outcome 1 and 2, respectively. Cues X and Y were paired equally often with outcomes 1 and 2 and were therefore non-predictive. Each compound cue was presented 5 times per block. The position of the cues and the outcomes (right-left), as well as the order of presentation of the trials, was fully randomized within each block.

Test 1 consisted of two presentations of each cue (8 trials in total). On each trial the target cue was presented with its corresponding foil. The left-right display of the target and the foil was counterbalanced, in such a way that each target appeared once on the left and once on the right.

Test 2 consisted of six presentations of each of the four cues (24 trials in total). Each cue was presented twice with each of the three foils shown in @tbl-exp1. For example, cue A was presented with the foil corresponding to cue B, the foil corresponding to cue X, and the foil corresponding to cue Y.

### Procedure

Participants were presented with the study information and responded to a series of questions to give informed consent. If participants gave consent, they proceeded to the training phase instructions. These described the initial learning task, in which they would see two images of fictitious chemicals that would be mixed to produce a mutant creature. Participants were told that their task was to predict which mutant will result from each combination of chemicals. They were also instructed to use the feedback provided after their decision to make their future choices more accurate.

After reading the instructions, participants were presented with a comprehension check, in which the instructions were summarised, and they were asked to select the answer that best described what they had to do in the task. The experiment ended if participants failed this comprehension check twice. If participants passed the comprehension check, they proceeded to the training phase.

All the trials in the training phase started with a 0.5 second blank screen. After that, two cues were presented in the top part and the two outcomes in the bottom part. The coordinates (x/y PsychoPy height units) of the centre of the four stimuli images were: left cue, -0.3 x 0.2; right cue, 0.3 x 0.2 left outcome, -0.125 x -0.2; right outcome, 0.125 x -0.2. All stimuli were rescaled according to the height of the monitor. The participants had to select one of the outcomes by clicking on them, which was indicated by a yellow frame surrounding the selected outcome. Feedback was provided after 0.5 seconds: the correct outcome was surrounded by a green frame. If the correct outcome was selected by participants, the message “CORRECT!” was displayed in the centre of the screen in green (RGB: 0, 255, 0), otherwise the message “INCORRECT!” appeared in red (RGB: 255, 0, 0). After two seconds, the next trial started. If participants failed to select an outcome within 10 seconds of the trial starting, all images disappeared from the screen and the message “TIMEOUT - TOO SLOW” was presented in the centre of the screen in red, and the next trial started after one second. An example of a training trial can be seen in @fig-trainexample.

```{r @fig-trainexample}
#| label: fig-trainexample
#| fig-cap: An example of a Training Trial in Experiment 1.
#| apa-twocolumn: true
#| apa-note: "The timings represent the duration of each display. "
#| out-width: 100%
#| fig-pos: h
knitr::include_graphics("images/trainexample.png")
```

Once participants had completed the training phase, the instructions for Test 1 were displayed, telling participants that they were about to see two similar chemicals on each trial, and only one of them had appeared in the previous task. They had to select that chemical and then rate their confidence on a scale from 1 to 10 (see below). No feedback on response accuracy was provided. After the instructions, a comprehension check was included, presented only once and participants continued to the test, irrespective of the response given.

All test trials started with a 0.5 second blank screen. After that, the images of a target (a cue presented in the training phase), and a foil were presented in the top half of the screen. These images had the same size and position as in the training phase. Participants had to click on the image they thought they had seen in the previous phase, after which a rating scale was displayed for them to give a confidence rating. Above this rating scale, the question *How confident are you of your response?* was displayed. The rating scale had 10 points, with the labels *I am guessing* on the left end (point 1), and *I am certain* on the right end (point 10), and a red dot in the middle. Participants had to click on the rating scale to move the red dot to give their confidence rating. After this, a button with the word *CONTINUE* appeared. All responses in the test phase had no time limit and participants could advance to the next test trial at their own pace.

Once participants had completed Test 1, the instructions for Test 2 were displayed. They were very similar to Test 1 instructions, except it stated that the two chemicals would not be similar, but that it was still the case that only one of them had been presented in the first task. Another comprehension check was included before the start of Test 2. The test trials procedure is shown in @fig-testexample.

```{r @fig-testexample}
#| label: fig-testexample
#| fig-cap: An example of a Test Trial in Experiment 1.
#| apa-twocolumn: true
#| apa-note: "The timings represent the duration of each display. "
#| out-width: 100%
#| fig-pos: h
knitr::include_graphics("images/testexample.png")
```

## Results

In all statistical tests, we adopt a significance level of .05. Greenhouse–Geisser corrected degrees of freedom were used where Mauchly’s test indicated that the assumption of sphericity was violated.

Participants that failed either the comprehension check before Test 1 or Test 2 were excluded from these analyses. Six participants were excluded based on this criterion (three from group High, and three from group Low).

```{r, include = FALSE}
#add a group variable
UNM05_training <- UNM05_training %>%
  mutate(group = case_when(session == 1 ~ "High",
                                session == 2 ~ "Medium",
                                session == 3 ~ "Low"))
#Clean participants that did not pass check 2 and/or 3
UNM05_training <- filter(UNM05_training, !pNum %in% UNM05_not_passed$pNum)
#Calculate the mean accuracy and standard error for each block, including the groups
UNM05_MA_training <- UNM05_training %>%
  group_by(block, group) %>%
  summarise(mean_accuracy = mean(correct_answer, na.rm = TRUE), 
            se_accuracy = sd(correct_answer, na.rm = TRUE)/sqrt(length(correct_answer)))
```

The dependent variable during training was the proportion of responses on a block in which participants selected the correct outcome. In @fig-trainingExp1 the mean accuracy for each similarity group is displayed across the four blocks of training. All groups showed a similar increase in responding across blocks, reaching an approximate accuracy of 0.85 in the final block of the experiment.

```{r, echo = FALSE, warning=FALSE}
#| label: fig-trainingExp1
#| fig-cap: Accuracy during the training phase of Experiment 1.
#| apa-note: "Proportion of accurate responses (±SEM) on the training phase of Experiment 1, plotted against the four blocks of trials, for each similarity group."
#| fig-height: 4
ggplot(UNM05_MA_training, mapping = aes(x = block, y = mean_accuracy, group = factor(group, levels = c("High", "Medium", "Low")))) +
  geom_point(mapping = aes(shape = factor(group, levels = c("High", "Medium", "Low")), color = factor(group, levels = c("High", "Medium", "Low"))), size = 2.5) +
  geom_line(mapping = aes(color = factor(group, levels = c("High", "Medium", "Low")))) +
  geom_errorbar(aes(x= block, y = mean_accuracy, ymin = mean_accuracy-se_accuracy, ymax = mean_accuracy+se_accuracy), colour = "black", width=.1)+
  scale_x_continuous(name = "Block") + 
  scale_color_discrete(type = c("#AF8DC3", "#FEB24C", "#7FBF7B"))+
  labs(shape = "Similarity", color = "Similarity") +
  scale_y_continuous(name = "Accuracy", limits = c(NA, 1))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA
UNM05_acc <- UNM05_training %>%
  group_by (pNum, block, group) %>%
  summarise(mean_response = mean(correct_answer, na.rm = TRUE))
UNM05_acc$block <- factor(UNM05_acc$block)
UNM05_acc$pNum <- factor(UNM05_acc$pNum)
UNM05_acc$group <- factor(UNM05_acc$group)
ANOVA_UNM05_acc <- aov_car(formula = mean_response ~ group + Error(pNum/block), data = UNM05_acc)
print(ANOVA_UNM05_acc)
#Bayesian Anova
bay_ANOVA_UNM05_acc <- anovaBF(formula = mean_response ~ group + block + pNum,
        data = data.frame(UNM05_acc),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM05_acc)
bay_ANOVA_UNM05_acc_int <- bay_ANOVA_UNM05_acc[4]/bay_ANOVA_UNM05_acc[3]
print(bay_ANOVA_UNM05_acc_int)
```

This was confirmed by a mixed model Analysis of Variance (ANOVA) with the within-subjects factor *block* (1-4), and the between-subjects factor *similarity* (High, Medium, and Low). Degrees of freedom were corrected by Greenhouse-Geisser when necessary. This analysis found a significant main effect of the *block*, with extreme Bayesian evidence for the alternative hypothesis, `r apa(ANOVA_UNM05_acc, effect = "block")`, `r report_BF_and_error(bay_ANOVA_UNM05_acc[1], sci_not = TRUE)`. There was no main effect of *similarity*, `r apa(ANOVA_UNM05_acc, effect = "group")`, `r report_BF_and_error(bay_ANOVA_UNM05_acc[2])` nor an interaction effect, `r apa(ANOVA_UNM05_acc, effect = "group:block")`, `r report_BF_and_error(bay_ANOVA_UNM05_acc_int[1])`, with both showing moderate Bayesian evidence for the null hypothesis. These results indicate that the training was equally effective for the three similarity groups, all of them increasing their performance as the phase progressed.

```{r, include = FALSE}
#add a group variable
UNM05_test1 <- UNM05_test1 %>%
  mutate(group = case_when(session == 1 ~ "High",
                                session == 2 ~ "Medium",
                                session == 3 ~ "Low"))
#create the memory_score
UNM05_test1 <- UNM05_test1 %>%
mutate (c_mem_score = case_when(acc == 0 ~ 0, acc == 1 ~ mem_score))
#Clean participants that did not pass check 2 and/or 3
UNM05_test1 <- filter(UNM05_test1, !pNum %in% UNM05_not_passed$pNum)
#factorize the session, which is the factor that contains the group
UNM05_test1$session <- as.factor(UNM05_test1$session)
#Calculate the mean accuracy and standard error for each block, including the groups
UNM05_MS_test1 <- UNM05_test1 %>%
  group_by(predictiveness, group) %>%
    summarise(mean_mem_score = mean(c_mem_score, na.rm = TRUE), 
            se_mem_score = sd(c_mem_score, na.rm = TRUE)/sqrt(length(c_mem_score)))
```

```{r, include = FALSE}
UNM05_test2 <- UNM05_test2 %>%
  #add a group variable
  mutate(group = case_when(session == 1 ~ "High",
                                session == 2 ~ "Medium",
                                session == 3 ~ "Low"))
#Clean participants that did not pass check 2 and/or 3
UNM05_test2 <- filter(UNM05_test2, !pNum %in% UNM05_not_passed$pNum)
```

```{r, include=FALSE}
UNM05_memscore_test1 <- UNM05_test1 %>%
  group_by (pNum, group, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE),
            acc = mean(acc, na.rm = TRUE))
UNM05_cor_test1 <- cor.test(UNM05_memscore_test1$acc, UNM05_memscore_test1$mem_score)
print(UNM05_cor_test1)
UNM05_BFcor_test1 <- correlationBF(UNM05_memscore_test1$acc, UNM05_memscore_test1$mem_score)
print(UNM05_BFcor_test1)
```

```{r, include=FALSE}
UNM05_memscore_test2 <- UNM05_test2 %>%
  group_by (pNum, group, predictiveness) %>%
  summarise(mem_score = mean(mem_score, na.rm = TRUE),
            acc = mean(acc, na.rm = TRUE))
UNM05_test2_cor <- cor.test(UNM05_memscore_test2$acc, UNM05_memscore_test2$mem_score)
print(UNM05_test2_cor)
UNM05_BFcor_test2 <- correlationBF(UNM05_memscore_test2$acc, UNM05_memscore_test2$mem_score)
print(UNM05_BFcor_test2)
```

For the test data, we calculated a “memory score” for each trial by multiplying by 1 the confidence rating given by participants on the trials in which the target was selected (correct responses), and by 0 on the trials in which the foil was selected (incorrect responses). These two measures, accuracy and confidence rating, were significantly correlated in both memory tests: `r apa(UNM05_cor_test1)`, `r report_BF_and_error(UNM05_BFcor_test1, sci_not = TRUE)` in Test 1, and `r apa(UNM05_test2_cor)`, `r report_BF_and_error(UNM05_BFcor_test2, sci_not = TRUE)` in Test 2. Mean memory scores for Test 1 are displayed in @fig-test1Exp1. In groups Low and Medium similarity, memory score was higher for the predictive than the non-predictive cue, whereas this tendency was inverted in the High group. Also, groups Low and Medium showed higher memory score than group Low.

```{r, echo = FALSE, warning=FALSE}
#| label: fig-test1Exp1
#| fig-cap: Memory scores during Test 1 of Experiment 1.
#| apa-note: "Mean memory scores (±SEM) on Test 1 of Experiment 1 for predictive and non-predictive trials in the High, Medium and Low similarity groups."
#| fig-height: 4
ggplot(UNM05_MS_test1, mapping = aes(x = factor(group, levels = c("High", "Medium", "Low")), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  scale_y_continuous(name = "Memory score")+
  #scale_fill_grey(start = 0.33) +
  theme_apa()
```

```{r, include=FALSE}
#ANOVA mem_score
UNM05_memscore_test1 <- UNM05_test1 %>%
  group_by (pNum, group, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
UNM05_memscore_test1$predictiveness <- factor(UNM05_memscore_test1$predictiveness)
UNM05_memscore_test1$group <- factor(UNM05_memscore_test1$group)
UNM05_memscore_test1$pNum <- factor(UNM05_memscore_test1$pNum)
ANOVA_UNM05_test1 <- aov_car(formula = mem_score ~ group + Error(pNum*predictiveness), data = UNM05_memscore_test1)
print(ANOVA_UNM05_test1)
bay_ANOVA_UNM05_test1 <- anovaBF(formula = mem_score ~ group*predictiveness + pNum,
        data = data.frame(UNM05_memscore_test1),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM05_test1)
bay_ANOVA_UNM05_test1_int <- bay_ANOVA_UNM05_test1[4]/bay_ANOVA_UNM05_test1[3]
print(bay_ANOVA_UNM05_test1_int)
```

These differences, however, were very small, and a mixed model ANOVA of the memory scores revealed none of the main effects or the interaction were significant: *similarity*: `r apa(ANOVA_UNM05_test1, effect = "group")`, `r report_BF_and_error(bay_ANOVA_UNM05_test1[1])`; *predictiveness*: `r apa(ANOVA_UNM05_test1, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM05_test1[2])`; *similarity x predictiveness*: `r apa(ANOVA_UNM05_test1, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM05_test1[1])`. Of note, there was moderate evidence for a null-effect of predictiveness, suggesting that Test 1 failed to reveal any differences in cue-processing between predictive and non-predictive cues.

```{r, include = FALSE}
UNM05_test2 <- UNM05_test2 %>%
  #add a group variable
  mutate(         #add a trial_type variable (PredictivenessxCongruence)
         trial_type = case_when((target == 1 & distractor_test2 == 2) | (target == 2 & distractor_test2 == 1) ~ "P-Con" ,
                                (target == 5 & distractor_test2 == 6) | (target == 6 & distractor_test2 == 5) ~ "NP-Con",
                                (target == 1 & (distractor_test2 == 5 | distractor_test2 == 6)) | (target == 2 & (distractor_test2 == 5 | distractor_test2 == 6)) ~ "P-Incon",
                                  (target == 5 & (distractor_test2 == 1 | distractor_test2 == 2)) | (target == 6 & (distractor_test2 == 1 | distractor_test2 == 2)) ~  "NP-Incon"),
         #add a congruence variable
         congruence = case_when ((trial_type == "P-Con") | (trial_type == "NP-Con") ~ "congruent",
                                 (trial_type == "P-Incon") | (trial_type == "NP-Incon") ~ "incongruent"),
         #create a memory_score
         c_mem_score = case_when(acc == 0 ~ 0, acc == 1 ~ mem_score))
#Calculate the mean accuracy and standard error for each block, including the groups
UNM05_MS_test2 <- UNM05_test2 %>%
  group_by(trial_type, group) %>%
    summarise(mean_mem_score = mean(c_mem_score, na.rm = TRUE), 
            se_mem_score = sd(c_mem_score, na.rm = TRUE)/sqrt(length(c_mem_score)))
```

@fig-test2Exp1 shows the data from Test 2. Overall, mean memory scores were lower for the non-predictive than for the predictive cues. This difference was more pronounced in the High group than in the other groups. However, there were no apparent differences between the congruent trials (in which target and foil were either both predictive or both non-predictive) and incongruent trials (in which the target and foil differed in their predictiveness).

```{r, echo = FALSE, warning=FALSE}
#| label: fig-test2Exp1
#| fig-cap: Memory scores during Test 2 of Experiment 1.
#| apa-note: "Mean memory scores (±SEM) on Test 2 of Experiment 1 in the High, Medium and Low similarity groups. The four bars per group represent the four types of trials: trials in which a non-predictive target was presented with a non-predictive foil (NP-Con), trials in which a non-predictive target was presented with a predictive foil (NP-Incon), trials in which a predictive target was presented with a non-predictive foil (P-Incon), and trials in which a predictive target was presented with a predictive foil (P-Con)."
#| fig-height: 4
ggplot(UNM05_MS_test2, mapping = aes(x = factor(group, levels = c("High", "Medium", "Low")), y = mean_mem_score, fill = trial_type)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Similarity") +
  scale_y_continuous(name = "Memory score")+
  labs(fill = "Trial type")+
  scale_fill_discrete(type = c("#7B3294", "#C2A5CF", "#008837", "#A6DBA0"))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA mem_score
UNM05_memscore_test2 <- UNM05_test2 %>%
  group_by (pNum, group, predictiveness, congruence) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
UNM05_memscore_test2$predictiveness <- factor(UNM05_memscore_test2$predictiveness)
UNM05_memscore_test2$congruence <- factor(UNM05_memscore_test2$congruence)
UNM05_memscore_test2$group <- factor(UNM05_memscore_test2$group)
UNM05_memscore_test2$pNum <- factor(UNM05_memscore_test2$pNum)
ANOVA_UNM05_test2 <- aov_car(formula = mem_score ~ group + Error(pNum*predictiveness*congruence), data = UNM05_memscore_test2)
print(ANOVA_UNM05_test2)
bay_ANOVA_UNM05_test2  <- anovaBF(formula = mem_score ~ group + predictiveness + congruence + pNum,
        data = data.frame(UNM05_memscore_test2),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM05_test2)
bay_ANOVA_UNM05_test2_sxp <- bay_ANOVA_UNM05_test2[4]/bay_ANOVA_UNM05_test2[3]
print(bay_ANOVA_UNM05_test2_sxp)
bay_ANOVA_UNM05_test2_pxc <- bay_ANOVA_UNM05_test2[13]/bay_ANOVA_UNM05_test2[7]
print(bay_ANOVA_UNM05_test2_pxc)
bay_ANOVA_UNM05_test2_sxc <- bay_ANOVA_UNM05_test2[10]/bay_ANOVA_UNM05_test2[6]
print(bay_ANOVA_UNM05_test2_sxc)
bay_ANOVA_UNM05_test2_sxpxc <- bay_ANOVA_UNM05_test2[18]/bay_ANOVA_UNM05_test2[17]
print(bay_ANOVA_UNM05_test2_sxpxc)
```

These data were analysed using a mixed ANOVA of memory scores with the factors of *predictiveness* (predictive vs. non-predictive target cue), *congruence* (target and foil same predictiveness or not), and *similarity* (High, Medium, Low). This analysis found that the only significant differences were due to the main effect of *predictiveness*, with strong Bayesian evidence for the alternative hypothesis, `r apa(ANOVA_UNM05_test2, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2[2])`. No other significant effects were found, and the Bayesian evidence was moderate for the null hypothesis for all of them: *similarity*: `r apa(ANOVA_UNM05_test2, effect = "group")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2[1])`; *congruence*: `r apa(ANOVA_UNM05_test2, effect = "congruence")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2[5])`; *similarity x predictiveness*: `r apa(ANOVA_UNM05_test2, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_sxp[1])`; *predictiveness x congruence*: `r apa(ANOVA_UNM05_test2, effect = "predictiveness:congruence")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_pxc[1])`; *similarity x congruence*: `r apa(ANOVA_UNM05_test2, effect = "group:congruence")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_sxc[1])`; *similarity x predictiveness x congruence*: `r apa(ANOVA_UNM05_test2, effect = "group:predictiveness:congruence")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_sxpxc[1])`. The results indicate that, in Test 2, participants showed better memory for predictive than non-predictive cues, but there were no differences due to the congruence of the test trial, nor the similarity of the foils to the training stimuli.

```{r, include = FALSE}
UNM05_test2_L <- filter(UNM05_test2, group == "Low") %>%
  group_by (pNum, predictiveness, congruence) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
UNM05_test2_L$pNum <- factor(UNM05_test2_L$pNum)
UNM05_test2_L$predictiveness <- factor(UNM05_test2_L$predictiveness)
UNM05_test2_L$congruence <- factor(UNM05_test2_L$congruence)
# ANOVA with two within factors
ANOVA_UNM05_test2_L <- aov_car(mem_score ~ predictiveness*congruence + Error(pNum/predictiveness*congruence), data=UNM05_test2_L)
print(ANOVA_UNM05_test2_L)
#bayesian ANOVA
bay_ANOVA_UNM05_test2_L <- anovaBF(formula = mem_score ~ congruence*predictiveness + pNum,
                                    data = data.frame(UNM05_test2_L),
                                    whichRandom = "pNum",
                                    iterations = bfit)
print(bay_ANOVA_UNM05_test2_L)
bay_ANOVA_UNM05_test2_L_int <- bay_ANOVA_UNM05_test2_L[4]/bay_ANOVA_UNM05_test2_L[3]
print(bay_ANOVA_UNM05_test2_L_int)
```

```{r, include = FALSE}
UNM05_test2_M <- filter(UNM05_test2, group == "Medium") %>%
  group_by (pNum, predictiveness, congruence) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
UNM05_test2_M$pNum <- factor(UNM05_test2_M$pNum)
UNM05_test2_M$predictiveness <- factor(UNM05_test2_M$predictiveness)
UNM05_test2_M$congruence <- factor(UNM05_test2_M$congruence)
# ANOVA with two within factors
ANOVA_UNM05_test2_M <- aov_car(mem_score ~ predictiveness*congruence + Error(pNum/predictiveness*congruence), data=UNM05_test2_M)
print(ANOVA_UNM05_test2_M)
#bayesian ANOVA
bay_ANOVA_UNM05_test2_M <- anovaBF(formula = mem_score ~ congruence*predictiveness + pNum,
                                   data = data.frame(UNM05_test2_M),
                                   whichRandom = "pNum",
                                   iterations = bfit)
print(bay_ANOVA_UNM05_test2_M)
bay_ANOVA_UNM05_test2_M_int <- bay_ANOVA_UNM05_test2_M[4]/bay_ANOVA_UNM05_test2_M[3]
print(bay_ANOVA_UNM05_test2_M_int)
```

```{r, include = FALSE}
UNM05_test2_H <- filter(UNM05_test2, group == "High") %>%
  group_by (pNum, predictiveness, congruence) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
UNM05_test2_H$pNum <- factor(UNM05_test2_H$pNum)
UNM05_test2_H$predictiveness <- factor(UNM05_test2_H$predictiveness)
UNM05_test2_H$congruence <- factor(UNM05_test2_H$congruence)
# ANOVA with two within factors
ANOVA_UNM05_test2_H <- aov_car(mem_score ~ predictiveness*congruence + Error(pNum/predictiveness*congruence), data=UNM05_test2_H)
print(ANOVA_UNM05_test2_H)
#bayesian ANOVA
bay_ANOVA_UNM05_test2_H <- anovaBF(formula = mem_score ~ congruence*predictiveness + pNum,
                                    data = data.frame(UNM05_test2_H),
                                    whichRandom = "pNum",
                                    iterations = bfit)
print(bay_ANOVA_UNM05_test2_H)
bay_ANOVA_UNM05_test2_H_int <- bay_ANOVA_UNM05_test2_H[4]/bay_ANOVA_UNM05_test2_H[3]
print(bay_ANOVA_UNM05_test2_H_int)
```

Despite there being no meaningful effects of similarity, and since the aim of this experiment was to determine the most sensitive way of testing participants’ memory, exploratory analyses were conducted on the data from each similarity group separately. For each, we conducted a within-subjects ANOVA with the factors *predictiveness* (predictive vs non-predictive) and *congruence* (congruent vs incongruent). In group Low, there was no main effect of *predictiveness*, `r apa(ANOVA_UNM05_test2_L, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_L[1])`, nor of *congruence*, `r apa(ANOVA_UNM05_test2_L, effect = "congruence")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_L[2])` and no interaction, `r apa(ANOVA_UNM05_test2_L, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_L_int[1])`. For group Medium, there was also no effect of *predictiveness*: `r apa(ANOVA_UNM05_test2_M, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_M[1])`, *congruence*: `r apa(ANOVA_UNM05_test2_M, effect = "congruence")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_M[2])`, and no interaction *group x predictiveness*: `r apa(ANOVA_UNM05_test2_M, effect = "group:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_M_int[1])`). However, for group High there was a significant effect of *predictiveness*, `r apa(ANOVA_UNM05_test2_H, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_H[1])`, but no effect of *congruence*, `r apa(ANOVA_UNM05_test2_H, effect = "congruence")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_H[2])`, and no interaction, `r apa(ANOVA_UNM05_test2_H, effect = "predictiveness:congruence")`, `r report_BF_and_error(bay_ANOVA_UNM05_test2_H_int[1])`. These results indicate that the only group that had a better memory for predictive stimuli over non-predictive stimuli was the High similarity group, irrespective of the congruence between foil and target in test.

## Discussion

Experiment 1 aimed to find an effective method for examining memory cues which had been established as either predictive or non-predictive of an outcome during a previous learning task. Two different tests were used, both involving the presentation of a target (a previously trained cue) and a foil (a novel cue), which had the colours of some its elements swapped. In Test 1, the foil was the same shape as the target while in Test 2, the foil was derived from a different cue from the target. Three groups of participants received different levels of foil-to-cue similarity. Test 1 revealed no differences in memory due to predictiveness nor due to the foil-to-cue similarity. Test 2 showed better memory for predictive than non-predictive cues, and, although there was no significant effect of, nor any interaction with, the factor of the similarity, further analysis showed that only the High similarity group (in which the colours changed in two out of the six circles) showed an effect of cue-predictiveness on recognition memory.

To the best of our knowledge only one other study has investigated the relationship between the predictive validity of cues and recognition memory. Griffiths and Mitchell [-@griffithsSelectiveAttentionHuman2008] also used a recognition memory test as a measure of cue processing and in Experiment 4 of their study used a learned predictiveness design adapted from Le Pelley and McLaren [-@lepelleyLearnedAssociabilityAssociative2003], and similar in nature to the design of Experiment 1. However, Griffiths and Mitchell [-@griffithsSelectiveAttentionHuman2008] did not observe differences in the recognition performance for predictive and non-predictive cues. There are two main procedural differences between the current Experiment 1 and Experiment 4 of Griffiths and Mitchell [-@griffithsSelectiveAttentionHuman2008]. First, their experimental task involved learning about predictive and non-predictive categories of stimuli, with each exemplar within each category being presented only once during training, whereas in our task, the same set of four stimuli was repeatedly presented during training. Second, their learned predictiveness task consisted of two phases: the first phase was similar to the training phase of the present study, but this was followed by a second phase in which all the categories were predictive of new outcomes. The memory test followed this second phase. The addition of this second phase is certainly the most likely reason for the differences in the findings: the established cue-processing bias that is established in phase 1 of the design is likely to be considerably attenuated by the second phase in which all cues were established as valid predictors of the outcomes.

Somewhat surprisingly there was no effect of foil-to-cue congruence in the data, with the predictiveness effect being observed both when the pairs of stimuli were congruent in their predictiveness (i.e., P-Con; NP-Con) and incongruent (i.e., P-Incon; NP-Incon). This is surprising, because there are seemingly different sources of memory that might contribute to these two trial types. Take for example the two trial types where the NP cue is the target stimulus. On NP-Con trials, the participant must rely on their memory of only NP cues to make a response, while on “NP-Incon” trials, the participant should be able to use their (presumably better) memory about P cues to detect the P foil. In principle, this should lead to better performance on the NP-Incon trials compared to NP-Con trials: this was clearly not the case. The critical factor in determining memory scores is whether the “P cue” was the target or the foil on the given trial: “P-Incon” trials are performed better than “NP-Incon” trials. It is possible that there is strong generalisation from the P cue to the presented P foil, and this is mistakenly identified as the target stimulus (especially so in the case of the High similarity condition). What is clear is that the only variable that affected memory scores was the predictiveness of the target stimulus, and so Test 2 represents a useful measure of recognition memory for stimuli in the task.

# Experiment 2

The purpose of Experiment 2 was to examine differences in recognition memory in a learned predictiveness procedure under certain and uncertain cue-outcome contingency conditions. Two groups were trained, one with a perfect contingency between the predictive cues and their paired outcome (Group Certain) - replicating the training from Experiment 1 - and one with a contingency of 0.8 between the predictive cues and their paired outcome (Group Uncertain). After this training, we tested the memory for cues in both groups using the memory test procedure that was most successful in Experiment 1, namely Test 2 (where targets were paired with foils from different cues). We also used the “High” similarity stimuli from Experiment 1, since it was only using these stimuli that a difference in memory performance between predictive and non-predictive stimuli was established.

The design of Experiment 2 is shown in @tbl-exp2. Previous experiments have established that for uncertain contingencies, participants spend longer attending to (looking at) all cues compared to attention to cues in certain contingencies [@beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019; @walkerProtectionUncertaintyExploration2022]. Experiment 2 therefore aimed to test whether uncertain contingencies, where it is well established that there is a high level of attention to cues, result in an improvement in the processing of these stimuli. As such, we predicted that memory would be better, overall, for the cues in group Uncertain compared to group Certain. Finally, on the basis of the results of Experiment 1, we anticipate seeing superior memory scores for the predictive than the non-predictive cues in group Certain.

::: {#tbl-exp2 apa-note="Uppercase letters A, B, X, and Y represent the cues presented during training. O1 and O2 represent the outcomes presented in training. Lowercase letters a, b, x, and y represent the foils that are similar to the (corresponding upper-case letter) cues presented in the training phase. The numbers before the trials define the proportion of trials of that type that were presented." apa-twocolumn="true"}
+---------------+---------------------------+------------------+
| Group         | Training                  | Test             |
+===============+:=========================:+:================:+
| Certain       | AX - O1                   | A vs *b*/*x*/*y* |
|               |                           |                  |
|               | AY - O1                   | B vs *a*/*x*/*y* |
|               |                           |                  |
|               | BX - O2                   | X vs *a*/*b*/*y* |
|               |                           |                  |
|               | BY - O2                   | Y vs *a*/*b*/*x* |
+---------------+---------------------------+------------------+
| Uncertain     | 0.8 AX - O1 / 0.2 AX - O2 | A vs *b*/*x*/*y* |
|               |                           |                  |
|               | 0.8 AY - O1 / 0.2 AY - O2 | B vs *a*/*x*/*y* |
|               |                           |                  |
|               | 0.8 BX - O2 / 0.2 BX - O1 | X vs *a*/*b*/*y* |
|               |                           |                  |
|               | 0.8 BY - O2 / 0.2 BY - O1 | Y vs *a*/*b*/*x* |
+---------------+---------------------------+------------------+

Design of Experiment 2
:::

## Methods

### Participants

```{r, include=FALSE}
#load the data
load("UNM07_proc_data.RData")
UNM07_demographics <- demographics
UNM07_training <- training
UNM07_test <- test
UNM07_not_passed <- not_passed_pNum
```

```{r, include = FALSE}
#create the PPR measure
UNM07_training <- UNM07_training %>%
  mutate(prob_response = case_when((cue1 == 1 | cue1 == 3) & response == "o1_image" ~ 1,
                                   (cue1 == 1 | cue1 == 3) & response == "o2_image" ~ 0, 
                                   (cue1 == 2 | cue1 == 4) & response == "o1_image" ~ 0,
                                   (cue1 == 2 | cue1 == 4) & response == "o2_image" ~ 1))

#detect and clean participants that had an PPR lower than 0.6 in the final block or not passed the test comprehension check
UNM07_block8 <- filter(UNM07_training, block == 8) %>%
  group_by(pNum, condition) %>%
  summarise (mean_response = mean(prob_response, na.rm = TRUE))
UNM07_low_acc_total <- filter(UNM07_block8, mean_response < 0.6) 
UNM07_low_acc <- UNM07_low_acc_total$pNum
UNM07_training <- filter(UNM07_training, !pNum %in% UNM07_not_passed$pNum & !pNum %in% UNM07_low_acc_total$pNum)
UNM07_test <- filter(UNM07_test, !pNum %in% UNM07_not_passed$pNum & !pNum %in% UNM07_low_acc_total$pNum)
```

`r nrow(UNM07_demographics)` participants were recruited through Prolific. The sample consisted of `r length(which(UNM07_demographics$gender == "female"))` women, `r length(which(UNM07_demographics$gender == "male"))` men and one non-binary person, with `r n_distinct(UNM07_demographics$Nationality)` different nationalities. The mean age was `r format(mean(UNM07_demographics$age, na.rm = TRUE), digits = 3)` calculated for the `r nrow(UNM07_demographics) - sum(is.na(UNM07_demographics$age))` participants that reported their age (range `r min(UNM07_demographics$age, na.rm = TRUE)` - `r max(UNM07_demographics$age, na.rm = TRUE)`). Pre-screening of participants in Prolific ensured that they had normal or corrected to normal vision, fluency in English language, and had not participated in previous studies from our lab. Participants were rewarded with £2.70 for their participation in the study. Participants were randomly allocated to either the Certain or Uncertain condition. Four participants were excluded due to failing the comprehension check before the test (three in group Certain and one in group Uncertain). Following a criterion similar to Le Pelley & McLaren [-@lepelleyLearnedAssociabilityAssociative2003; @lepelleyLearnedPredictivenessInfluences2013], an additional criterion was added in this experiment due to the poor performance of some participants in the training phase, such that those that had an accuracy lower than 0.6 in the final block of the training phase were removed from the analyses. This led to the exclusion of `r nrow(UNM07_low_acc_total)` participants, seven in group Certain and `r length(which(UNM07_low_acc_total$condition == "Uncertain"))` in group Uncertain. This resulted in a final sample of `r nrow(UNM07_test)/24` participants, `r length(which(UNM07_test$condition == "Certain"))/24` in group Certain and `r length(which(UNM07_test$condition == "Uncertain"))/24` in group Uncertain[^2]. Post-hoc calculations using G\*Power 3.1 [@faulStatisticalPowerAnalyses2007] revealed that this sample size had a power of .99 to detect an effect size of η~p~^2^ = .06 that was observed for the *group x predictiveness* interaction reported in @fig-testExp2.

[^2]: The critical interaction effect reported in the results section is significant without the application of this exclusion criterion.

### Apparatus and stimuli

The materials used for Experiment 2 were the same as in Experiment 1, with the stimuli taken from the High similarity group.

### Design

The design of Experiment 2 is shown in @tbl-exp2. The Certain group employed the same design as in Experiment 1: cues A and B were perfectly predictive of the outcomes they were paired with, while cues X and Y were non-predictive. For the Uncertain group, cues A and B were the best available predictors on each trial but had a 0.8 contingency with the predicted outcome. To implement this contingency, each block of trials contained 5 presentations of each compound, where for four of these trials one outcome was “correct” (e.g., AX-O1) and for one of these trials the alternative outcome was “correct” (e.g., AX-O2). Cues X and Y were non-predictive in the two groups. The training phase of this experiment consisted of eight blocks, resulting in a total of 160 trials. Training was followed by a test identical to Test 2 of Experiment 1.

### Procedure

The procedure for both the training and test phases was identical to those described in Experiment 1.

## Results

For the training phase, it was necessary to calculate response performance in a different manner in this experiment, since participants in the uncertain condition received trials in which the “correct” outcome was switched on 20% of the trials. Thus, even if participants in group Uncertain were to always select the most probable outcome (O1 when A is present and O2 when B is present), it would result in an accuracy score of 80%. Thus, we calculated the proportion of probable responses (PPR): for the Uncertain group, on each trial, the score was 0 when participants chose the less probable outcome (i.e., O2 for A and O1 for B) and 1 when they chose the most probable outcome (i.e., O1 for A and O2 for B). For the certain condition, this equates to a standard accuracy score.

```{r, include = FALSE}
#Calculate the mean PPR and standard error for each block, including the groups
UNM07_MA_training <- UNM07_training %>%
  group_by(block, condition) %>%
  summarise(mean_accuracy = mean(prob_response, na.rm = TRUE), 
            se_accuracy = sd(prob_response, na.rm = TRUE)/sqrt(length(prob_response)))
```

@fig-trainingExp2 shows the mean PPR across blocks for each group. Participants in the Certain group showed higher PPR through training than the Uncertain group, reaching a PPR of 0.92 on block 8. The Uncertain group showed a slower increase in their PPR that reached 0.77 in block 8.

```{r, echo = FALSE, warning=FALSE}
#| label: fig-trainingExp2
#| fig-cap: PPR on the training phase of Experiment 2.
#| apa-note: "Mean proportion of probable responses (±SEM) during the training phase of Experiment 2, for groups trained with certain and uncertain contingencies."
#| fig-height: 4
ggplot(UNM07_MA_training, mapping = aes(x = block, y = mean_accuracy, group = condition, color = condition)) +
  geom_point(mapping = aes(shape = condition), size = 2.5) +
  geom_line() +
  geom_errorbar(aes(x= block, y = mean_accuracy, ymin = mean_accuracy-se_accuracy, ymax = mean_accuracy+se_accuracy), colour = "black", width=.1)+
  scale_x_continuous(name = "Block") + 
  labs(shape = "Group", colour = "Group") +
  scale_color_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6, 7, 8))+
  scale_y_continuous(name = "PPR", limits = c(NA, 1))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA
UNM07_acc <- UNM07_training %>%
  group_by (pNum, block, condition) %>%
  summarise(mean_response = mean(prob_response, na.rm = TRUE))
UNM07_acc$block <- factor(UNM07_acc$block)
UNM07_acc$pNum <- factor(UNM07_acc$pNum)
UNM07_acc$condition <- factor(UNM07_acc$condition)
ANOVA_UNM07_acc <- aov_car(formula = mean_response ~ condition + Error(pNum/block), data = UNM07_acc)
print(ANOVA_UNM07_acc)
#Bayesian Anova
bay_ANOVA_UNM07_acc <- anovaBF(formula = mean_response ~ condition + block + pNum,
        data = data.frame(UNM07_acc),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM07_acc)
bay_ANOVA_UNM07_acc_int <- bay_ANOVA_UNM07_acc[4]/bay_ANOVA_UNM07_acc[3]
print(bay_ANOVA_UNM07_acc_int)
```

A mixed model ANOVA of individual PPR scores found significant both main effects, of *group*, `r apa(ANOVA_UNM07_acc, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM07_acc[2])`, and of *block*, `r apa(ANOVA_UNM07_acc, effect = "block")`, `r report_BF_and_error(bay_ANOVA_UNM07_acc[1], sci_not = TRUE)`. There was no interaction effect between these factors, `r apa(ANOVA_UNM07_acc, effect = "condition:block")`, with the Bayesian analysis providing strong evidence for the null hypothesis, `r report_BF_and_error(bay_ANOVA_UNM07_acc_int[1])`. These results indicate that the training increased the PPR for both groups, as the effect of block was significant, with the Certain group showing a consistently higher PPR than Uncertain group.

```{r, include = FALSE}
#create the memory_score
UNM07_test <- UNM07_test %>%
  mutate (c_mem_score = case_when(acc == 0 ~ 0, acc == 1 ~ mem_score))
#Calculate the mean PPR and standard error for each block, including the groups
UNM07_MS_test <- UNM07_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_mem_score = mean(c_mem_score, na.rm = TRUE), 
            se_mem_score = sd(c_mem_score, na.rm = TRUE)/sqrt(length(c_mem_score)))
```

@fig-testExp2 shows the results from the recognition memory test. memory scores were calculated in the same manner as for Experiment 1. The memory for non-predictive cues was lower than for the predictive cues in the Certain group. This difference was notably attenuated in in the Uncertain group, and there was no indication of higher memory scores in the uncertain group relative to the Certain group.

```{r, echo = FALSE, warning=FALSE}
#| label: fig-testExp2
#| fig-cap: Memory scores during the Test of Experiment 2.
#| apa-note: "Mean memory scores (±SEM) during the Test phase of Experiment 2 for predictive and non-predictive trials in the Certain and Uncertain groups."
#| fig-height: 4
ggplot(UNM07_MS_test, mapping = aes(x = factor(condition, level=c('Uncertain', 'Certain')), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  scale_y_continuous(name = "Memory score")+
  #scale_fill_grey(start = 0.33) +
  theme_apa()
```

```{r, include=FALSE}
#ANOVA mem_score
UNM07_memscore_test <- UNM07_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
UNM07_memscore_test$predictiveness <- factor(UNM07_memscore_test$predictiveness)
UNM07_memscore_test$condition <- factor(UNM07_memscore_test$condition)
UNM07_memscore_test$pNum <- factor(UNM07_memscore_test$pNum)
ANOVA_UNM07_test <- aov_car(formula = mem_score ~ condition + Error(pNum*predictiveness), data = UNM07_memscore_test)
print(ANOVA_UNM07_test)
bay_ANOVA_UNM07_test <- anovaBF(formula = mem_score ~ condition*predictiveness + pNum,
        data = data.frame(UNM07_memscore_test),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM07_test)
bay_ANOVA_UNM07_test_int <- bay_ANOVA_UNM07_test[4]/bay_ANOVA_UNM07_test[3]
print(bay_ANOVA_UNM07_test_int)
```

```{r, include = FALSE}
# Pairwise comparisons between group levels
#interaction analysis
UNM07_memscore_test_interaction_group <- emmeans(ANOVA_UNM07_test, ~ predictiveness|condition)
pairs(UNM07_memscore_test_interaction_group, adjust = "bon")

certain_pred_UNM07_test <- subset(UNM07_test, (predictiveness == "predictive") & (condition == "Certain"), mem_score, drop = TRUE)
certain_nonpred_UNM07_test <- subset(UNM07_test, (predictiveness == "non-predictive") & (condition == "Certain"), mem_score, drop = TRUE)
bay_t.test_UNM07_int_certain <-  ttestBF(certain_pred_UNM07_test, certain_nonpred_UNM07_test, paired = TRUE)
  
uncertain_pred_UNM07_test <- subset(UNM07_test, (predictiveness == "predictive") & (condition == "Uncertain"), mem_score, drop = TRUE)
uncertain_nonpred_UNM07_test <- subset(UNM07_test, (predictiveness == "non-predictive") & (condition == "Uncertain"), mem_score, drop = TRUE)
bay_t.test_UNM07_int_uncertain <-  ttestBF(uncertain_pred_UNM07_test, uncertain_nonpred_UNM07_test, paired = TRUE)

UNM07_memscore_test_interaction_pred <- emmeans(ANOVA_UNM07_test, ~ condition|predictiveness)
pairs(UNM07_memscore_test_interaction_pred, adjust = "bon")
```

A mixed model ANOVA of memory scores revealed a significant main effect of *predictiveness*, with moderate Bayesian evidence, `r apa(ANOVA_UNM07_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM07_test[2])` and a significant *group x predictiveness* interaction, with anecdotal Bayesian evidence, `r apa(ANOVA_UNM07_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM07_test_int[1])`. However, the *group* effect was not significant and the evidence for the null hypothesis was moderate, `r apa(ANOVA_UNM07_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM07_test[1])`. Bonferroni-corrected pairwise comparisons showed that there was a significant difference between predictive and non-predictive cues in the Certain group, *t*(73) = 3.45, *p* = .001, `r report_BF_and_error(bay_t.test_UNM07_int_certain[1])`, but not in the Uncertain group, *t*(73) = 0.28, *p* = .781, `r report_BF_and_error(bay_t.test_UNM07_int_uncertain[1])`. This indicated that the training with certain contingency produces a difference in memory for the cues depending on their predictiveness, but that difference does not emerge during uncertain training.

## Discussion

Experiment 2 aimed to examine the effect of uncertainty on recognition memory for predictive and non-predictive cues. The Uncertain group of participants were exposed to a probabilistic relationship between the predictive cues and their respective outcomes, while the Certain group received deterministic relationships. We hypothesised that the previously observed effect of uncertainty on overt attention (e.g., Beesley et al., 2015) would lead to better memory for cues in that condition. However, in a final recognition memory test, the two groups showed a similar overall level of recognition memory for the cues. There was an effect of cue-predictiveness in group Certain with better recognition memory for the predictive than the non-predictive cues (replicating the results of Experiment 1). However, there was no effect of cue-predictiveness in group Uncertain, with evidence to suggest memory for predictive and non-predictive cues was equivalent. This latter result is consistent with previous studies [@beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019] that have shown that attention (in those cases measured by eye-gaze dwell times), under certain training, decreased for non-predictive cues but not for predictive cues. That decrease in attention could be responsible for the worse memory performance for the non-predictive cues, compared with the predictive cues, in the certain group.

A central distinction made in Easdale et al. [-@easdaleOnsetUncertaintyFacilitates2019] was that between *expected-* and *unexpected-uncertainty*. In those experiments, participants who experienced a sustained period with uncertain compounds (as is the case in our “uncertain group” in Experiment 2) learnt more slowly about new contingencies, compared to a group that received a sudden and unexpected change in the contingencies. Thus, it may be the case that the current uncertain condition does not promote higher recognition memory, overall, because participants have come to expect a certain level of uncertainty and are no longer engaging in an exploratory mode of cue-processing. Of course, the expected levels of high attention to cues under these uncertain conditions presents a paradox for learning and attention research: why does a high level of attention not translate to better learning and memory for those cues? We return to this point in the general discussion. Nevertheless, this analysis of the findings in terms of expected and unexpected uncertainty suggests that a more acute period of uncertainty may (re)engage a mode of exploratory attentional processing for the cues, which would result in better memory of those cues. Experiment 3 tested this hypothesis.

# Experiment 3

Experiment 3 aimed to examine whether the introduction of uncertainty, following a period of certain training (i.e., unexpected uncertainty), would lead to an increase in cue-processing (better recognition memory). The design of Experiment 3 can be seen in @tbl-exp3. The experiment consisted of three groups. Groups Certain Long and Certain Short received training that was similar to the Certain condition from Experiment 2, experiencing a protracted period of certain contingencies between the cue compounds and the outcomes throughout the training phase, differing only in the amount of training trials experienced by each of them. Group Uncertain first experienced the same certain contingencies experienced by Group Certain Long, before the contingencies were changed to uncertain for a short period before the recognition memory test. Our prediction was that, if the introduction of unexpected uncertainty promotes greater levels of exploratory attention, then we should see better recognition memory performance in this uncertain condition, compared to the certain condition.

::: {#tbl-exp3 apa-note="Uppercase letters A, B, X, and Y represent the cues presented during training. O1 and O2 represent the outcomes presented in training. Lowercase letters a, b, x, and y represent the foils that are similar to the (corresponding upper-case letter) cues presented in the training phase. The numbers before the trials define the proportion of trials of that type that were presented." apa-twocolumn="true"}
+---------------+--------------+---------------------------+------------------+
| Group         | Stage 1      | Stage 2                   | Test             |
+===============+==============+:=========================:+:================:+
| Certain Long  | AX - O1      | AX - O1                   | A vs *b*/*x*/*y* |
|               |              |                           |                  |
|               | AY - O1      | AY - O1                   | B vs *a*/*x*/*y* |
|               |              |                           |                  |
|               | BX - O2      | BX - O2                   | X vs *a*/*b*/*y* |
|               |              |                           |                  |
|               | BY - O2      | BY - O2                   | Y vs *a*/*b*/*x* |
+---------------+--------------+---------------------------+------------------+
| Certain Short | AX - O1      |                           | A vs *b*/*x*/*y* |
|               |              |                           |                  |
|               | AY - O1      |                           | B vs *a*/*x*/*y* |
|               |              |                           |                  |
|               | BX - O2      |                           | X vs *a*/*b*/*y* |
|               |              |                           |                  |
|               | BY - O2      |                           | Y vs *a*/*b*/*x* |
+---------------+--------------+---------------------------+------------------+
| Uncertain     | AX - O1      | 0.8 AX - O1 / 0.2 AX - O2 | A vs *b*/*x*/*y* |
|               |              |                           |                  |
|               | AY - O1      | 0.8 AY - O1 / 0.2 AY - O2 | B vs *a*/*x*/*y* |
|               |              |                           |                  |
|               | BX - O2      | 0.8 BX - O2 / 0.2 BX - O1 | X vs *a*/*b*/*y* |
|               |              |                           |                  |
|               | BY - O2      | 0.8 BY - O2 / 0.2 BY - O1 | Y vs *a*/*b*/*x* |
+---------------+--------------+---------------------------+------------------+

Design of Experiment 3
:::

We also included a third condition, group Certain Short, which received the same certain contingencies as the other two conditions in Stage 1 but did not experience Stage 2; they received a shorter training phase than the other two conditions. If the onset of the uncertainty leads to greater cue-processing, then we should also see better cue-memory in the Uncertain condition compared to the Certain Short condition. The inclusion of this condition is important because longer training with the certain contingencies in the “Certain Long” condition could *decrease* cue processing, which would be an alternative explanation of any difference in cue processing we observe between Group Uncertain and Group Certain Long. If this is the case, we should see equivalent recognition memory in the Certain Short and Uncertain conditions, and poorer recognition memory in the Certain Long condition. Therefore, the addition of this third condition allowed us to make stronger inferences about the causal relationship between the onset of uncertainty and cue-processing.

## Methods

### Participants

```{r, include=FALSE}
#load the data
load("UNM08_proc_data.RData")
UNM08_demographics <- demographics
UNM08_training <- rbind(stage1, stage2)
UNM08_test <- test
UNM08_not_passed <- not_passed_pNum
```

```{r, include = FALSE}
#create the PPR measure
UNM08_training <- UNM08_training %>%
  mutate(prob_response = case_when((cue1 == 1 | cue1 == 3) & response == "o1_image" ~ 1,
                                   (cue1 == 1 | cue1 == 3) & response == "o2_image" ~ 0, 
                                   (cue1 == 2 | cue1 == 4) & response == "o1_image" ~ 0,
                                   (cue1 == 2 | cue1 == 4) & response == "o2_image" ~ 1))

#detect and clean participants that had an PPR lower than 0.6 in the final block or not passed the test comprehension check
UNM08_block6 <- filter(UNM08_training, block == 6) %>%
  group_by(pNum, condition) %>%
  summarise (mean_response = mean(prob_response, na.rm = TRUE))
UNM08_low_acc_total <- filter(UNM08_block6, mean_response < 0.6) 
UNM08_low_acc <- UNM08_low_acc_total$pNum
UNM08_training <- filter(UNM08_training, !pNum %in% UNM08_not_passed$pNum & !pNum %in% UNM08_low_acc_total$pNum)
UNM08_test <- filter(UNM08_test, !pNum %in% UNM08_not_passed$pNum & !pNum %in% UNM08_low_acc_total$pNum)
```

`r nrow(UNM08_demographics)` participants were recruited through Prolific. The mean age of the `r nrow(UNM08_demographics) - sum(is.na(UNM08_demographics$age))` participants that reported their age was `r format(mean(UNM08_demographics$age, na.rm = TRUE), digits = 3)` (range `r min(UNM08_demographics$age, na.rm = TRUE)` - `r max(UNM08_demographics$age, na.rm = TRUE)`), with `r length(which(UNM08_demographics$gender == "female"))` women, `r length(which(UNM08_demographics$gender == "male"))` men, and one non-binary person, and `r n_distinct(UNM08_demographics$Nationality)` different nationalities. Participants were divided into 3 groups in the following fashion: `r nrow(filter(UNM08_demographics, condition == "Certain Long"))` in group Certain Long, `r nrow(filter(UNM08_demographics, condition == "Certain Short"))` in group Certain Short and `r nrow(filter(UNM08_demographics, condition == "Uncertain"))` in group Uncertain. The 34 participants in group Certain Short were included in the experiment after the data collection was completed for the other two groups. The same exclusion criteria were applied in Experiment 3 as in Experiment 2. Six participants were excluded on the basis of failing the comprehension check before the test, all in group Uncertain; `r nrow(UNM08_low_acc_total)` participants were excluded due to a low PPR (\< 0.6) on the last block of Stage 1, two in group Certain Long, four in group Certain Short and `r length(which(UNM08_low_acc_total$condition == "Uncertain"))`in group Uncertain[^3]. We continued to recruit participants after applying our exclusion criteria, to ensure that there were 30 participants in each group. Thus, the results below are for the remaining `r nrow(UNM08_test)/24` participants. Post-hoc calculations using G\*Power 3.1 [@faulStatisticalPowerAnalyses2007] revealed that this sample size had a power of .87 to detect an effect size of η~p~^2^ = .09 that was observed for the group main effect reported in @fig-testExp3.

[^3]: The critical main effect of group reported in the results section is significant without the application of this exclusion criterion.

### Apparatus and stimuli

The materials were the same as in Experiments 1 and 2.

### Design

The experiment used a mixed design (as seen in @tbl-exp3), with three groups: Certain Long, Certain Short, and Uncertain. All groups received six blocks of certain training. Group Certain Long then received a further 4 blocks of certain training; group Uncertain, received a further four blocks of uncertain training (with contingencies of 0.8); and group Certain Short received no further training (they completed six training blocks only). When training was completed, all groups progressed to the memory test, which was identical to the one Experiment 2.

### Procedure

All the details about the procedure were identical to Experiment 2.

## Results

```{r, include = FALSE}
#Calculate the mean PPR and standard error for each block, including the groups and stages
UNM08_MA_training <- UNM08_training %>%
  group_by(block, stage, condition) %>%
  summarise(mean_accuracy = mean(prob_response, na.rm = TRUE), 
            se_accuracy = sd(prob_response, na.rm = TRUE)/sqrt(length(prob_response)))

#add a dummy to display stage 2 for Certain Short
MA_stage2_dummy <- data.frame(stage = c('stage 2', 'stage 2', 'stage 2', 'stage 2'),
                              block = c(7:10),
                              condition = c('Certain Short', 'Certain Short', 'Certain Short', 'Certain Short'),
                              mean_accuracy = c(0.001, 0.002, 0.003, 0.004),
                              se_accuracy = c(0.0001, 0.00020, 0.0003, 0.00004))
UNM08_MA_training <- rbind(UNM08_MA_training, MA_stage2_dummy)
#change stage 1 and stage 2 to Stage1 and Stage 2, and Certain_short to Certain Short
UNM08_MA_training <- UNM08_MA_training %>%
  mutate(stage = case_when(stage == "stage 1" ~ "Stage 1",
                           stage == "stage 2" ~ "Stage 2"))
```

@fig-trainingExp3 shows the mean PPR for each group across the four blocks of training. All participants showed a similar increase in PPR in stage 1, reaching a PPR of around 0.93 on block 6. In Stage 2, group Certain showed a similar PPR to block 6, but the Uncertain group showed a decrease in PPR to a level of around 0.85.

```{r, echo = FALSE, warning=FALSE}
#| label: fig-trainingExp3
#| fig-cap: PPR on the training phase of Experiment 3.
#| apa-note: "Mean proportion of probable responses (±SEM) during the training phase of Experiment 3, plotted against the ten blocks of trials, for each Group."
#| fig-height: 4
ggplot(UNM08_MA_training, mapping = aes(x = block, y = mean_accuracy, group = condition)) +
  geom_point(mapping = aes(shape = condition, color = condition), size = 2.5) +
  geom_line(mapping = aes(color = condition)) +
  geom_errorbar(aes(x= block, y = mean_accuracy, ymin = mean_accuracy-se_accuracy, ymax = mean_accuracy+se_accuracy), colour = "black", width=.1)+
  facet_grid(cols = vars(stage), space = "free_x", scales = "free_x") + 
  scale_x_continuous(name = "Block", breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) + 
  scale_color_discrete(type = c("#AF8DC3", "#FEB24C", "#7FBF7B"))+
  labs(shape = "Group", color = "Group") +
  scale_y_continuous(name = "PPR", limits = c(0.5, 1))+
  theme_apa()
```

```{r, include=FALSE}
#ANOVA
UNM08_acc_stage1 <- filter(UNM08_training, stage == "stage 1") %>%
  group_by (pNum, block, condition) %>%
  summarise(mean_response = mean(prob_response, na.rm = TRUE))
UNM08_acc_stage1$block <- factor(UNM08_acc_stage1$block)
UNM08_acc_stage1$pNum <- factor(UNM08_acc_stage1$pNum)
UNM08_acc_stage1$condition <- factor(UNM08_acc_stage1$condition)
ANOVA_UNM08_acc_stage1 <- aov_car(formula = mean_response ~ condition + Error(pNum/block), data = UNM08_acc_stage1)
print(ANOVA_UNM08_acc_stage1)
#Bayesian Anova
bay_ANOVA_UNM08_acc_stage1 <- anovaBF(formula = mean_response ~ condition + block + pNum,
        data = data.frame(UNM08_acc_stage1),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM08_acc_stage1)
bay_ANOVA_UNM08_acc_stage1_int <- bay_ANOVA_UNM08_acc_stage1[4]/bay_ANOVA_UNM08_acc_stage1[3]
print(bay_ANOVA_UNM08_acc_stage1_int)
```

The Stage 1 data were analysed with a mixed-model ANOVA (with the degrees of freedom corrected by Greenhouse-Geisser when the sphericity assumption was not fulfilled), with the between-subjects factor of *group* (Certain Long, Certain Short, and Uncertain), and the within-subjects factor of *block* (1-6). This revealed a significant effect of block, with extreme Bayesian evidence for the alternative hypothesis, `r apa(ANOVA_UNM08_acc_stage1, effect = "block")`, `r report_BF_and_error(bay_ANOVA_UNM08_acc_stage1[1], sci_not = TRUE)`. There was no effect of *group*, with strong evidence for the null hypothesis, `r apa(ANOVA_UNM08_acc_stage1, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM08_acc_stage1[2])`, and no interaction effect, with extreme evidence for the null hypothesis, `r apa(ANOVA_UNM08_acc_stage1, effect = "condition:block")`, `r report_BF_and_error(bay_ANOVA_UNM08_acc_stage1_int[1])`.

```{r, include=FALSE}
#ANOVA
UNM08_acc <- filter(UNM08_training, condition == "Certain Long" | condition == "Uncertain") %>%
  group_by (pNum, block, condition) %>%
  summarise(mean_response = mean(prob_response, na.rm = TRUE))
UNM08_acc$block <- factor(UNM08_acc$block)
UNM08_acc$pNum <- factor(UNM08_acc$pNum)
UNM08_acc$condition <- factor(UNM08_acc$condition)
ANOVA_UNM08_acc <- aov_car(formula = mean_response ~ condition + Error(pNum/block), data = UNM08_acc)
print(ANOVA_UNM08_acc)
#Bayesian Anova
bay_ANOVA_UNM08_acc <- anovaBF(formula = mean_response ~ condition + block + pNum,
        data = data.frame(UNM08_acc),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM08_acc)
bay_ANOVA_UNM08_acc_int <- bay_ANOVA_UNM08_acc[4]/bay_ANOVA_UNM08_acc[3]
print(bay_ANOVA_UNM08_acc_int)
```

```{r, include=FALSE}
# Pairwise comparisons for the interaction analysis
resp_interaction <- emmeans(ANOVA_UNM08_acc, ~condition|block)
pairs(resp_interaction, adjust = "bon")

UNM08_acc__block1_cert <- subset(UNM08_acc, (condition == "Certain Long") & (block == 1), mean_response, drop = TRUE)
UNM08_acc__block1_uncert <- subset(UNM08_acc, (condition == "Uncertain") & (block == 1), mean_response, drop = TRUE)
bay_t.test_UNM08_int_block1 <-  ttestBF(UNM08_acc__block1_cert, UNM08_acc__block1_uncert)
print(bay_t.test_UNM08_int_block1)

UNM08_acc__block2_cert <- subset(UNM08_acc, (condition == "Certain Long") & (block == 2), mean_response, drop = TRUE)
UNM08_acc__block2_uncert <- subset(UNM08_acc, (condition == "Uncertain") & (block == 2), mean_response, drop = TRUE)
bay_t.test_UNM08_int_block2 <-  ttestBF(UNM08_acc__block2_cert, UNM08_acc__block2_uncert)
print(bay_t.test_UNM08_int_block2)

UNM08_acc__block3_cert <- subset(UNM08_acc, (condition == "Certain Long") & (block == 3), mean_response, drop = TRUE)
UNM08_acc__block3_uncert <- subset(UNM08_acc, (condition == "Uncertain") & (block == 3), mean_response, drop = TRUE)
bay_t.test_UNM08_int_block3 <-  ttestBF(UNM08_acc__block3_cert, UNM08_acc__block3_uncert)
print(bay_t.test_UNM08_int_block3)

UNM08_acc__block4_cert <- subset(UNM08_acc, (condition == "Certain Long") & (block == 4), mean_response, drop = TRUE)
UNM08_acc__block4_uncert <- subset(UNM08_acc, (condition == "Uncertain") & (block == 4), mean_response, drop = TRUE)
bay_t.test_UNM08_int_block4 <-  ttestBF(UNM08_acc__block4_cert, UNM08_acc__block4_uncert)
print(bay_t.test_UNM08_int_block4)

UNM08_acc__block5_cert <- subset(UNM08_acc, (condition == "Certain Long") & (block == 5), mean_response, drop = TRUE)
UNM08_acc__block5_uncert <- subset(UNM08_acc, (condition == "Uncertain") & (block == 5), mean_response, drop = TRUE)
bay_t.test_UNM08_int_block5 <-  ttestBF(UNM08_acc__block5_cert, UNM08_acc__block5_uncert)
print(bay_t.test_UNM08_int_block5)

UNM08_acc__block6_cert <- subset(UNM08_acc, (condition == "Certain Long") & (block == 6), mean_response, drop = TRUE)
UNM08_acc__block6_uncert <- subset(UNM08_acc, (condition == "Uncertain") & (block == 6), mean_response, drop = TRUE)
bay_t.test_UNM08_int_block6 <-  ttestBF(UNM08_acc__block6_cert, UNM08_acc__block6_uncert)
print(bay_t.test_UNM08_int_block6)

UNM08_acc__block7_cert <- subset(UNM08_acc, (condition == "Certain Long") & (block == 7), mean_response, drop = TRUE)
UNM08_acc__block7_uncert <- subset(UNM08_acc, (condition == "Uncertain") & (block == 7), mean_response, drop = TRUE)
bay_t.test_UNM08_int_block7 <-  ttestBF(UNM08_acc__block7_cert, UNM08_acc__block7_uncert)
print(bay_t.test_UNM08_int_block7)

UNM08_acc__block8_cert <- subset(UNM08_acc, (condition == "Certain Long") & (block == 8), mean_response, drop = TRUE)
UNM08_acc__block8_uncert <- subset(UNM08_acc, (condition == "Uncertain") & (block == 8), mean_response, drop = TRUE)
bay_t.test_UNM08_int_block8 <-  ttestBF(UNM08_acc__block8_cert, UNM08_acc__block8_uncert)
print(bay_t.test_UNM08_int_block8)

UNM08_acc__block9_cert <- subset(UNM08_acc, (condition == "Certain Long") & (block == 9), mean_response, drop = TRUE)
UNM08_acc__block9_uncert <- subset(UNM08_acc, (condition == "Uncertain") & (block == 9), mean_response, drop = TRUE)
bay_t.test_UNM08_int_block9 <-  ttestBF(UNM08_acc__block9_cert, UNM08_acc__block9_uncert)
print(bay_t.test_UNM08_int_block9)

UNM08_acc__block10_cert <- subset(UNM08_acc, (condition == "Certain Long") & (block == 10), mean_response, drop = TRUE)
UNM08_acc__block10_uncert <- subset(UNM08_acc, (condition == "Uncertain") & (block == 10), mean_response, drop = TRUE)
bay_t.test_UNM08_int_block10 <-  ttestBF(UNM08_acc__block10_cert, UNM08_acc__block10_uncert)
print(bay_t.test_UNM08_int_block10)
```

The data from Stage 1 and 2 were analysed with a mixed model ANOVA (using the Greenhouse-Geisser correction when needed), with the between-subjects factor of *group* (Certain Long vs Uncertain) and the within-subjects factor of *block* (1-10). There was no effect of *group*, with anecdotal Bayesian evidence for the null hypothesis, `r apa(ANOVA_UNM08_acc, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM08_acc[2])`, but there was a significant effect of *block* with extreme Bayesian evidence for the alternative hypothesis, `r apa(ANOVA_UNM07_acc, effect = "block")`, `r report_BF_and_error(bay_ANOVA_UNM08_acc[1], sci_not = TRUE)`, and a significant group by block interaction, with extreme Bayesian evidence for the alternative hypothesis, `r apa(ANOVA_UNM08_acc, effect = "condition:block")`, `r report_BF_and_error(bay_ANOVA_UNM08_acc_int[1])`. Bonferroni-corrected comparisons showed that the Uncertain group had a lower PPR on the Stage 2 blocks, *t*(58) \> 3.27, *p* \< .002, `r report_BF_and_error(bay_t.test_UNM08_int_block6)`, but no differences in the Stage 1 blocks, *t*(58) \< 1.15, *p* \> .254, `r report_BF_and_error(bay_t.test_UNM08_int_block7)`.

Taken together, these results indicate that the training in Stage 1 increased the PPR for all groups in the same fashion, in Stage 1, while in Stage 2, the Certain Long group showed a consistently higher PPR than the Uncertain group.

```{r, include = FALSE}
#create the memory_score
UNM08_test <- UNM08_test %>%
  mutate (c_mem_score = case_when(acc == 0 ~ 0, acc == 1 ~ mem_score))
#Calculate the mean PPR and standard error for each block, including the groups
UNM08_MS_test <- UNM08_test %>%
  group_by(predictiveness, condition) %>%
    summarise(mean_mem_score = mean(c_mem_score, na.rm = TRUE), 
            se_mem_score = sd(c_mem_score, na.rm = TRUE)/sqrt(length(c_mem_score)))
```

@fig-testExp3 shows the recognition memory data for the three conditions. Memory for non-predictive cues was lower than for predictive cues in all groups, but this difference was notably attenuated in the Uncertain group. Interestingly, the memory for the cues in the Uncertain group was higher, overall, than in the Certain Long and Certain Short groups.

```{r, echo = FALSE, warning=FALSE}
#| label: fig-testExp3
#| fig-cap: Memory scores on the Test of Experiment 3.
#| apa-note: "Mean memory scores (±SEM) during the Test of Experiment 2 for predictive and non-predictive trials across the three groups."
#| fig-height: 4
ggplot(UNM08_MS_test, mapping = aes(x = factor(condition, level=c('Uncertain', 'Certain Long', 'Certain Short')), y = mean_mem_score, fill = predictiveness)) +
  geom_col(position = position_dodge2()) +
  geom_errorbar(aes(ymin = mean_mem_score - se_mem_score, ymax = mean_mem_score + se_mem_score), width=.2, position=position_dodge(0.9)) +
  scale_x_discrete (name = "Group") +
  scale_fill_discrete(type = c("#AF8DC3", "#7FBF7B"))+
  scale_y_continuous(name = "Memory score")+
  #scale_fill_grey(start = 0.33) +
  theme_apa()
```

```{r, include=FALSE}
#ANOVA mem_score
UNM08_memscore_test <- UNM08_test %>%
  group_by (pNum, condition, predictiveness) %>%
  summarise(mem_score = mean(c_mem_score, na.rm = TRUE))
UNM08_memscore_test$predictiveness <- factor(UNM08_memscore_test$predictiveness)
UNM08_memscore_test$condition <- factor(UNM08_memscore_test$condition)
UNM08_memscore_test$pNum <- factor(UNM08_memscore_test$pNum)
ANOVA_UNM08_test <- aov_car(formula = mem_score ~ condition + Error(pNum*predictiveness), data = UNM08_memscore_test)
print(ANOVA_UNM08_test)
bay_ANOVA_UNM08_test <- anovaBF(formula = mem_score ~ condition + predictiveness,
        data = data.frame(UNM08_memscore_test),
        whichRandom = "pNum",
        iterations = bfit)
print(bay_ANOVA_UNM08_test)
bay_ANOVA_UNM08_test_int <- bay_ANOVA_UNM08_test[4]/bay_ANOVA_UNM08_test[3]
print(bay_ANOVA_UNM08_test_int)
# Pairwise comparisons for the interaction analysis
UNM08_test_interaction <- emmeans(ANOVA_UNM08_test, ~condition)
contrast(UNM08_test_interaction, adjust = "bon", "trt.vs.ctrl", ref = c(1,2))

UNM08_test_certs <- subset(UNM08_memscore_test, (condition == "Certain Long") | (condition == "Certain Short"), mem_score, drop = TRUE)
UNM08_test_uncert <- subset(UNM08_memscore_test, condition == "Uncertain", mem_score, drop = TRUE)
bay_t.test_UNM08_int_uncer_vs_certs <-  ttestBF(UNM08_test_certs, UNM08_test_uncert)
print(bay_t.test_UNM08_int_uncer_vs_certs)

pairs(UNM08_test_interaction, adjust = "bon")

UNM08_test_cert <- subset(UNM08_memscore_test, condition == "Certain Long", mem_score, drop = TRUE)
UNM08_test_cert_s <- subset(UNM08_memscore_test, condition == "Certain Short", mem_score, drop = TRUE)
bay_t.test_UNM08_test_certs <-  ttestBF(UNM08_test_cert, UNM08_test_cert_s)
print(bay_t.test_UNM08_test_certs)
```

A mixed model ANOVA, including the between-subjects factor *group* (Certain Long, Certain Short, Uncertain), and the within-subjects factor *predictiveness* (predictive vs non-predictive) showed a significant main effect of the *group*, with moderate evidence for the alternative hypothesis, `r apa(ANOVA_UNM08_test, effect = "condition")`, `r report_BF_and_error(bay_ANOVA_UNM08_test[1])` and a main effect of *predictiveness*, with extreme Bayesian evidence for the alternative hypothesis,, `r apa(ANOVA_UNM08_test, effect = "predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM08_test[2])`. However, the *group x predictiveness* interaction failed to reach significance, showing anecdotal Bayesian evidence for the null hypothesis, `r apa(ANOVA_UNM08_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM08_test_int[1])`. Bonferroni corrected post-hoc comparisons on the factor of group showed that group Uncertain differed significantly from the average of the Certain groups, *t*(87) = 1.18, *p* = .004, `r report_BF_and_error(bay_t.test_UNM08_int_uncer_vs_certs)`, but memory scores for the two Certain groups did not differ the one from the other, *t*(87) = 0.188, *p* = 1, `r report_BF_and_error(bay_t.test_UNM08_test_certs)`, with the Bayesian evidence suggesting that memory performance was the same in these two groups.

These results indicate that memory for predictive cues was better than for non-predictive cues independently of the contingencies that were presented at training. However, those participants that experienced an unexpected period of uncertainty showed generally better memory than the Certain groups.

## Discussion

Experiment 3 examined the effect of unexpected uncertainty on recognition memory. The “Uncertain” group of participants first experienced a period of training with certain contingencies, before receiving a second period with uncertain contingencies. Participants that were exposed to this unexpected uncertainty showed a higher level of recognition memory for the cues than participants that received only certain training. An important difference between Experiment 2 and 3 is that in Experiment 2, the Certain and Uncertain groups had a similar recognition memory for the cues, whereas in the current experiment, the Uncertain group showed better cue-recognition. We interpret this difference to be a consequence of the expectancy of uncertainty: in Experiment 3, but not Experiment 2, uncertainty is suddenly introduced after a sustained period of certain training.

These results suggest that introducing a period of unexpected uncertainty results in enhanced cue processing and are consistent with previous results [@easdaleOnsetUncertaintyFacilitates2019] that showed that unexpected uncertainty enhanced learning. Easdale et al. used a training phase with participants learning about either certain or uncertain contingencies. Participants showed better attention to uncertain cues. However, when those cues were subsequently trained under new contingencies, it was participants in the certain condition that learned about these more rapidly, compared to those participants in the uncertain condition. Easdale et al. suggested that the transition from certain to uncertain contingencies brought about a state of “unexpected uncertainty” which promoted new learning. Experiment 3 shows more directly that a period of unexpected uncertainty leads to superior cue processing and stronger memory representations.

# General discussion

Three experiments explored the processing of cues under different conditions of certainty and uncertainty during a contingency learning task. In Experiment 1 we established a recognition memory task that could reveal differences in the level of processing for the different cues used during the preceding learning task. Participants were first required to learn the relationships between compounds of cues and outcomes in a task in which each compound contained a cue that was perfectly predictive and a cue that was non-predictive of the outcome. In a subsequent two-alternative forced-choice recognition test, each trained cue was presented alongside a variation of another cue and participants were asked to select the stimulus that they had experienced during training. Participants were asked to rate their confidence in these judgments and the combination of this score with their accuracy on each trial created a “memory score”. Memory scores were higher for the predictive than for the non-predictive cues. This result aligns with the well-established attentional bias that is observed for predictive over non-predictive cues in these designs [e.g., @lepelleyOvertAttentionPredictiveness2011].

In Experiment 2, the same “certain” condition was trained as in Experiment 1, in addition to an “uncertain” condition, in which there was a probabilistic relationship between the predictive cues and the outcomes. We predicted that uncertainty would increase the memory scores for cues, in line with the previously established increased levels of overt attention to cues trained under uncertain conditions [e.g., @beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019; @torrents-rodasEffectPredictionError2023; @walkerRoleUncertaintyAttentional2019; @walkerProtectionUncertaintyExploration2022]. However, this was not the case: overall, there was no effect of uncertainty on memory scores, with evidence to support a conclusion of equivalent levels of memory for cues (overall) in the certain and uncertain conditions. In keeping with the results of Experiment 1, memory scores for the predictive cues were higher than those for the non-predictive cues, but this result was restricted to group Certain: there was no observed difference between predictive and non-predictive cues in group Uncertain.

Experiment 3 explored the differences in recognition memory for conditions of “expected” and “unexpected” uncertainty. Unexpected uncertainty is defined here as using a procedure in which participants were initially trained on certain contingencies and then given further training, for a shorter period, with uncertain contingencies. This condition was compared to the certain condition (as trained in Experiments 1 and 2). This period of unexpected uncertainty in the task had a dramatic effect on cue-processing and memory performance: participants in this unexpected uncertainty condition showed better memory than those in the certain condition. We included an additional condition in Experiment 3 that received just the first phase of certain training, equivalent in length to that experienced in the unexpected uncertainty condition. This condition provided a strong test to determine if it was indeed the period of unexpected uncertainty that led to increases in cue-processing, rather than further training of the certain contingencies (in the standard certain condition) leading to a decrease in cue-processing. Memory was better in the case of the unexpected uncertainty condition compared to this short-certain condition, and therefore a short period of unexpected uncertainty appears to enhance the memory for, and thus processing of, the cues.

Attentional theories of associative learning have long recognised the role that uncertainty plays in determining the allocation of processing resources to stimuli in the environment. According to the Pearce and Hall [-@pearceModelPavlovianLearning1980] model, the effective salience of a stimulus is determined by the magnitude of the absolute prediction error that stimulus has received on the previous trial. That is, if a stimulus was followed by an unexpected outcome, then the associability of the stimulus on the next trial should be high; if an expected outcome was received, then the associability should be low. Pearce and Hall described this as an active attentional process that aids the animal in discovering information about stimuli for which it is unsure about the consequences. In the original model, the attention, and therefore the associability of a stimulus, is determined by the prediction error on the previous trial, while in the revised model of Pearce, Kaye, and Hall [-@pearcePredictiveAccuracyStimulus1982] this was determined by a longer history of reinforcement (the length of which was controlled by a parameter in the model). In either case, the model makes the prediction that uncertainty – periods in which there is prediction error in the reinforcement schedule – will result in high levels of attention to a stimulus. The results of Experiment 2 are therefore not compatible with the principles of the Pearce-Hall model [@pearceModelPavlovianLearning1980; @pearcePredictiveAccuracyStimulus1982], since the uncertain condition did not show evidence of greater levels of cue-processing than the certain condition, despite the substantial level of prediction error that was experienced in the former. Since we know from previous work that this same procedure results in higher levels of overt attention to cues [@beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019; @walkerProtectionUncertaintyExploration2022; @walkerRoleUncertaintyAttentional2019], the results of Experiment 2 provide insights into the complex relationships between overt attention, active stimulus processing, and associative learning. The data from Experiment 2 suggest that the circumstances that favour high levels of overt attention do not necessarily translate directly into active and enhanced processing of the stimuli. Thus, the data are consistent with the failures to observe more rapid learning about new associations under such conditions of uncertainty [@beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019; @torrents-rodasEffectPredictionError2023]. Taken alone, the data from Experiment 2 suggest that the hitherto assumption that eye-gaze dwell time can be used as a proxy measure of stimulus associability, is on shaky ground.

The data from Experiment 3, however, suggest that uncertainty can, under some circumstances, lead to enhancements in stimulus processing. In this procedure, after a period of training in which the task contingencies were deterministic, there was a sudden change to uncertain (probabilistic) contingencies. Under these conditions, evidence for enhanced stimulus processing was obtained, with better memory for the cue stimuli overall, compared to conditions in which participants were only trained with certain contingencies. In light of the findings of Experiment 2, these findings from Experiment 3 provide strong evidence for the distinction made by Easdale et al. [-@easdaleOnsetUncertaintyFacilitates2019] between expected and unexpected uncertainty. After periods of prolonged exposure to stable levels of uncertainty, participants appear to develop a tolerance for this uncertainty and their stimulus processing decreases. In contrast, a sudden onset of uncertain contingencies, following exposure to entirely certain contingencies, boosts stimulus processing. Together, these data suggest a non-linear relationship between contingency exposure and stimulus processing under uncertainty. It is likely that, at the outset of exposure to an uncertain contingency, stimulus processing will be high, and this level of processing will decline over the course of experience with the stable (but uncertain) contingency. In contrast, the sudden experience of a prediction error, following certain contingencies, increases the level of stimulus processing to a maximal level. Continual training with uncertain contingencies would be expected to see a decline in stimulus processing; it is likely that the configuration of our procedure in Experiment 3 was able to capture this high level of stimulus processing prior to an expected decline in the level of stimulus processing with continued uncertainty (i.e., a transition to a state of expected uncertainty).

In view of the analysis provided in the context of the Pearce and Hall model, it is clear that the conditions of expected uncertainty present a major challenge to attentional theories of associative learning. The data from the current studies and others using similar designs [@beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019; @walkerProtectionUncertaintyExploration2022; @walkerRoleUncertaintyAttentional2019] illustrates that the cognitive system is sensitive not just to the absolute level of prediction error, but how long that prediction error has been experienced for, and how stable that pattern of uncertainty has been. Indeed, the manner in which the uncertainty is experienced in the task, not just the overall level of uncertainty, can affect the pattern of choices and attention. One line of examination for future experimental and theoretical work would be to explore whether expected uncertainty reflects a localised cue-specific parameter or is better reflected as a global property of the learning system, akin to a reflection of “vigilance” on the task.

```{r, include=FALSE}
options(scipen = 999)
SME_UNM08_memscore_condition <- UNM08_memscore_test %>%
  group_by(predictiveness) %>%
  anova_test(mem_score ~ condition) %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
print(SME_UNM08_memscore_condition)
```

The results of Experiment 3 suggest that unexpected uncertainty returns participants to an exploratory mode of processing. To what extent was this directed more towards predictive or non-predictive cues? The analysis of the memory scores in Experiment 3 found an overall main effect of group and no significant interaction effect between group and cue-predictiveness with the Bayesian ANOVA indicating anecdotal evidence for the null hypothesis, `r apa(ANOVA_UNM08_test, effect = "condition:predictiveness")`, `r report_BF_and_error(bay_ANOVA_UNM08_test_int[1])`. Thus, our analysis of the simple main effects is entirely exploratory, but this found a significant effect of group on the memory scores for non-predictive cues, *F*(2, 87) = 6.75, *p* = .004, but not for the predictive cues, *F* (2, 87) = 1.13, *p* = .328. Thus, there is a suggestion that the elevated levels of recognition memory seen for the unexpected uncertain condition in Experiment 3 was primarily driven by increases in cue-processing for the non-predictive cues.

According to the uncertainty principle, the increased levels of attention to cues associated with uncertainty operates as a mechanism to discover new cue-outcome relationships. This process has been referred to as “exploratory attention” [e.g., @beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019] or “attention for learning” [e.g., Hall & Rodríguez, -@hallAttentionPerceiveLearn2019a]. Thus, it is an attempt to resolve the prediction errors that are experienced by learning new and valid signals for the outcomes. In Experiment 3, it is possible that the non-predictive cues have greater “capacity” for learning, or perhaps offer a more plausible signal, given an association already exists between the predictive cues and the outcomes from the early certain phase. This exploratory process will be short lived in this procedure, since the uncertainty that is experienced cannot be resolved by learning new associations. Indeed, in the procedure of Easdale et al. [-@easdaleOnsetUncertaintyFacilitates2019], participants could resolve the uncertainty in a second stage by learning about the meaningful associations pertaining to the previously non-predictive cues. In this situation increased attentional processing of the non-predictive cues was observed.

This characterisation of the effect as one driven primarily by changes in processing for non-predictive cues is also consistent with the theory protection account proposed by Spicer and colleagues [-@spicerTheoryProtectionAssociative2020; -@spicerTheoryProtectionHumans2022]. According to this account, participants tend to protect existing associations as much as possible. When prediction errors are experienced during learning, rather than adjust those associations that result in the largest error, the learning system will instead direct resources to learning about cues that are not already strongly associated with outcomes. In line with this proposed property of human learning, it is possible that, in Experiment 3, participants in group Uncertain directed their attention more towards non-predictive cues than predictive cues at the onset of the unexpected uncertainty, since the predictive cues had already been established as reliable predictors of the outcomes in the first stage of training (in which the relationship between the predictive cues and the outcomes was perfect). Further studies will be necessary to confirm if theory protection offers an accurate account of the manner in which attentional resources are allocated during uncertainty.

The current results provide a stark warning about the interpretation of eye-gaze as a proxy for attentional processing. While we have not collected eye-gaze in the current tasks (since they were conducted online), we have substantial evidence that uncertain conditions lead to higher levels of both absolute dwell times on cues, and a higher proportion of the response time spent on cues [@beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019; @walkerRoleUncertaintyAttentional2019; @walkerProtectionUncertaintyExploration2022]. The finding that higher overt attention doesn’t lead to faster learning [@beesleyUncertaintyPredictivenessDetermine2015; @easdaleOnsetUncertaintyFacilitates2019; @torrents-rodasEffectPredictionError2023] and, in the current studies, that it doesn’t lead to greater cue-processing, is problematic for the use of eye-data in constraining attentional models of associative learning. Of course, this concern of the apparent disconnect between overt attention and active stimulus engagement stretches beyond the field of human learning. As has been noted in other literatures, most notably in the case of inattentional blindness [e.g., @simonsGorillasOurMidst1999]; looking does not always reflect an active and engaged attentional process.

In conclusion, the studies reported here revealed that unexpected-, but not expected- uncertainty, leads to an enhancement in the processing of cues during associative learning, highlighting the importance of the distinction between expected and unexpected uncertainty. Future work will be needed to understand the implications of this distinction on associability and subsequent learning, as well as on eye-gaze. Furthermore, attentional models of associative learning will need to accommodate and refine our understanding of expected and unexpected uncertainty in order to adequately map out the relationships between patterns of reinforcement, stimulus processing, and learning.

# References

::: {#refs}
:::

# Appendix I

The four sets of images from which the cues and foils displayed in the experiment were randomly selected can be seen in @fig-cues_and_foils.

```{r @fig-cues_and_foils}
#| label: fig-cues_and_foils
#| fig-cap: Cues and foils used in Experiment 1.
#| apa-twocolumn: true
#apa-note: "Panel A displays the cues that can be selected for the training phase. Panel B displays the set of foils that could be selected in the tests for group High similarity. Panel C displays the set of foils that could be selected in the tests for group Medium similarity. Panel D displays the set of foils that could be selected in the tests for group Low similarity."
#| out-width: 100%
#| fig-pos: h
knitr::include_graphics("images/cues_foils.png")
```

The two images used as outcomes in these experiments can be seen in @fig-outcomes.

```{r fig-outcomes}
#| label: fig-outcomes
#| fig-cap: Outcomes used in all experiments.
#| apa-twocolumn: true
#| out-width: 100%
#| fig-pos: h
knitr::include_graphics("images/outcomes.png")
```
